{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d537cbad",
   "metadata": {},
   "source": [
    "# RAD Security: CVE-Aware Analysis Agent\n",
    "\n",
    "## AI Engineer Take-Home Exercise\n",
    "\n",
    "This document outlines the architecture and implementation of an LLM-powered agent designed to analyze security incidents in the context of CVE data. The agent uses semantic search, retrieval-augmented generation, and structured agent tools to provide contextual prioritization of security vulnerabilities.\n",
    "\n",
    "### Key Features\n",
    "\n",
    "- **Semantic Search**: Searches over KEV, NVD, and historical incident records\n",
    "- **Agent Tools**: Uses MCP (Model Context Protocol) for structured tool access\n",
    "- **Contextual Prioritization**: Ranks CVEs based on their relevance to specific incidents\n",
    "- **Historical Learning**: Builds a vector store of past analyses for normalization\n",
    "- **Persistence**: Stores analyses in SQLite for future reference\n",
    "\n",
    "## Why This Architecture\n",
    "\n",
    "The architecture is designed to address several key challenges in security incident analysis:\n",
    "\n",
    "1. **Volume Challenge**: Security teams face thousands of CVEs and alerts daily\n",
    "2. **Context Challenge**: Understanding the relationship between vulnerabilities and incidents requires contextual knowledge\n",
    "3. **Expertise Challenge**: Security expertise is scarce and expensive\n",
    "4. **Consistency Challenge**: Manual analysis leads to inconsistent prioritization\n",
    "\n",
    "Our solution uses LLMs and semantic search to understand incident context, identify relevant CVEs, prioritize them based on impact, and generate human-readable explanations of the analysis.\n",
    "\n",
    "## 1. Setup and Dependencies\n",
    "\n",
    "Let's start by installing the required dependencies and setting up our environment.\n",
    "\n",
    "**Why we do this:** Ensuring all required packages are available creates a reproducible environment. This setup step loads essential libraries for LangChain, LangGraph, OpenAI, FAISS vector storage, and Redis caching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1282aad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: asyncio>=3.4.3 in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from -r requirements.txt (line 1)) (3.4.3)\n",
      "Requirement already satisfied: faiss-cpu>=1.11.0 in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from -r requirements.txt (line 2)) (1.11.0)\n",
      "Requirement already satisfied: fastapi>=0.115.12 in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from -r requirements.txt (line 3)) (0.115.12)\n",
      "Requirement already satisfied: fastmcp>=2.3.3 in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from -r requirements.txt (line 4)) (2.3.3)\n",
      "Requirement already satisfied: httpx>=0.28.1 in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from -r requirements.txt (line 5)) (0.28.1)\n",
      "Requirement already satisfied: ipykernel>=6.29.5 in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from -r requirements.txt (line 6)) (6.29.5)\n",
      "Requirement already satisfied: jupytext>=1.17.1 in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from -r requirements.txt (line 7)) (1.17.1)\n",
      "Requirement already satisfied: langchain>=0.3.25 in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from -r requirements.txt (line 8)) (0.3.25)\n",
      "Requirement already satisfied: langchain-community>=0.3.24 in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from -r requirements.txt (line 9)) (0.3.24)\n",
      "Requirement already satisfied: langchain-mcp-adapters>=0.0.11 in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from -r requirements.txt (line 10)) (0.0.11)\n",
      "Requirement already satisfied: langchain-openai>=0.3.16 in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from -r requirements.txt (line 11)) (0.3.16)\n",
      "Requirement already satisfied: langgraph>=0.4.3 in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from -r requirements.txt (line 12)) (0.4.3)\n",
      "Requirement already satisfied: mcp>=1.8.1 in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from mcp[cli]>=1.8.1->-r requirements.txt (line 13)) (1.8.1)\n",
      "Requirement already satisfied: openai>=1.78.1 in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from -r requirements.txt (line 14)) (1.79.0)\n",
      "Requirement already satisfied: orjson>=3.10.18 in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from -r requirements.txt (line 15)) (3.10.18)\n",
      "Requirement already satisfied: pytest>=8.3.5 in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from -r requirements.txt (line 16)) (8.3.5)\n",
      "Requirement already satisfied: python-dotenv>=1.1.0 in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from -r requirements.txt (line 17)) (1.1.0)\n",
      "Requirement already satisfied: redis>=4.5 in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from -r requirements.txt (line 18)) (6.1.0)\n",
      "Requirement already satisfied: streamlit>=1.45.1 in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from -r requirements.txt (line 19)) (1.45.1)\n",
      "Requirement already satisfied: tabulate>=0.9.0 in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from -r requirements.txt (line 20)) (0.9.0)\n",
      "Requirement already satisfied: tiktoken>=0.9.0 in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from -r requirements.txt (line 21)) (0.9.0)\n",
      "Requirement already satisfied: uvicorn>=0.34.2 in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from -r requirements.txt (line 22)) (0.34.2)\n",
      "Requirement already satisfied: yfinance>=0.2.61 in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from -r requirements.txt (line 23)) (0.2.61)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from faiss-cpu>=1.11.0->-r requirements.txt (line 2)) (2.1.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from faiss-cpu>=1.11.0->-r requirements.txt (line 2)) (24.2)\n",
      "Requirement already satisfied: starlette<0.47.0,>=0.40.0 in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from fastapi>=0.115.12->-r requirements.txt (line 3)) (0.46.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from fastapi>=0.115.12->-r requirements.txt (line 3)) (2.11.3)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from fastapi>=0.115.12->-r requirements.txt (line 3)) (4.12.2)\n",
      "Requirement already satisfied: exceptiongroup>=1.2.2 in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from fastmcp>=2.3.3->-r requirements.txt (line 4)) (1.3.0)\n",
      "Requirement already satisfied: openapi-pydantic>=0.5.1 in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from fastmcp>=2.3.3->-r requirements.txt (line 4)) (0.5.1)\n",
      "Requirement already satisfied: rich>=13.9.4 in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from fastmcp>=2.3.3->-r requirements.txt (line 4)) (13.9.4)\n",
      "Requirement already satisfied: typer>=0.15.2 in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from fastmcp>=2.3.3->-r requirements.txt (line 4)) (0.15.3)\n",
      "Requirement already satisfied: websockets>=14.0 in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from fastmcp>=2.3.3->-r requirements.txt (line 4)) (15.0.1)\n",
      "Requirement already satisfied: anyio in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from httpx>=0.28.1->-r requirements.txt (line 5)) (4.9.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from httpx>=0.28.1->-r requirements.txt (line 5)) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from httpx>=0.28.1->-r requirements.txt (line 5)) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from httpx>=0.28.1->-r requirements.txt (line 5)) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from httpcore==1.*->httpx>=0.28.1->-r requirements.txt (line 5)) (0.16.0)\n",
      "Requirement already satisfied: comm>=0.1.1 in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from ipykernel>=6.29.5->-r requirements.txt (line 6)) (0.2.2)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from ipykernel>=6.29.5->-r requirements.txt (line 6)) (1.8.14)\n",
      "Requirement already satisfied: ipython>=7.23.1 in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from ipykernel>=6.29.5->-r requirements.txt (line 6)) (9.2.0)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from ipykernel>=6.29.5->-r requirements.txt (line 6)) (8.6.3)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from ipykernel>=6.29.5->-r requirements.txt (line 6)) (5.7.2)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from ipykernel>=6.29.5->-r requirements.txt (line 6)) (0.1.7)\n",
      "Requirement already satisfied: nest-asyncio in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from ipykernel>=6.29.5->-r requirements.txt (line 6)) (1.6.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from ipykernel>=6.29.5->-r requirements.txt (line 6)) (7.0.0)\n",
      "Requirement already satisfied: pyzmq>=24 in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from ipykernel>=6.29.5->-r requirements.txt (line 6)) (26.4.0)\n",
      "Requirement already satisfied: tornado>=6.1 in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from ipykernel>=6.29.5->-r requirements.txt (line 6)) (6.4.2)\n",
      "Requirement already satisfied: traitlets>=5.4.0 in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from ipykernel>=6.29.5->-r requirements.txt (line 6)) (5.14.3)\n",
      "Requirement already satisfied: markdown-it-py>=1.0 in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from jupytext>=1.17.1->-r requirements.txt (line 7)) (3.0.0)\n",
      "Requirement already satisfied: mdit-py-plugins in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from jupytext>=1.17.1->-r requirements.txt (line 7)) (0.4.2)\n",
      "Requirement already satisfied: nbformat in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from jupytext>=1.17.1->-r requirements.txt (line 7)) (5.10.4)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from jupytext>=1.17.1->-r requirements.txt (line 7)) (6.0.2)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.58 in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from langchain>=0.3.25->-r requirements.txt (line 8)) (0.3.59)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from langchain>=0.3.25->-r requirements.txt (line 8)) (0.3.8)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from langchain>=0.3.25->-r requirements.txt (line 8)) (0.3.37)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from langchain>=0.3.25->-r requirements.txt (line 8)) (2.0.37)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from langchain>=0.3.25->-r requirements.txt (line 8)) (2.32.3)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from langchain-community>=0.3.24->-r requirements.txt (line 9)) (3.11.11)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from langchain-community>=0.3.24->-r requirements.txt (line 9)) (9.1.2)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from langchain-community>=0.3.24->-r requirements.txt (line 9)) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from langchain-community>=0.3.24->-r requirements.txt (line 9)) (2.9.1)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from langchain-community>=0.3.24->-r requirements.txt (line 9)) (0.4.0)\n",
      "Requirement already satisfied: langgraph-checkpoint<3.0.0,>=2.0.10 in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from langgraph>=0.4.3->-r requirements.txt (line 12)) (2.0.25)\n",
      "Requirement already satisfied: langgraph-prebuilt>=0.1.8 in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from langgraph>=0.4.3->-r requirements.txt (line 12)) (0.1.8)\n",
      "Requirement already satisfied: langgraph-sdk>=0.1.42 in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from langgraph>=0.4.3->-r requirements.txt (line 12)) (0.1.69)\n",
      "Requirement already satisfied: xxhash<4.0.0,>=3.5.0 in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from langgraph>=0.4.3->-r requirements.txt (line 12)) (3.5.0)\n",
      "Requirement already satisfied: python-multipart>=0.0.9 in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from mcp>=1.8.1->mcp[cli]>=1.8.1->-r requirements.txt (line 13)) (0.0.20)\n",
      "Requirement already satisfied: sse-starlette>=1.6.1 in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from mcp>=1.8.1->mcp[cli]>=1.8.1->-r requirements.txt (line 13)) (2.3.5)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from openai>=1.78.1->-r requirements.txt (line 14)) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from openai>=1.78.1->-r requirements.txt (line 14)) (0.9.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from openai>=1.78.1->-r requirements.txt (line 14)) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from openai>=1.78.1->-r requirements.txt (line 14)) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from pytest>=8.3.5->-r requirements.txt (line 16)) (0.4.6)\n",
      "Requirement already satisfied: iniconfig in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from pytest>=8.3.5->-r requirements.txt (line 16)) (2.1.0)\n",
      "Requirement already satisfied: pluggy<2,>=1.5 in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from pytest>=8.3.5->-r requirements.txt (line 16)) (1.6.0)\n",
      "Requirement already satisfied: altair<6,>=4.0 in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from streamlit>=1.45.1->-r requirements.txt (line 19)) (5.5.0)\n",
      "Requirement already satisfied: blinker<2,>=1.5.0 in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from streamlit>=1.45.1->-r requirements.txt (line 19)) (1.9.0)\n",
      "Requirement already satisfied: cachetools<6,>=4.0 in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from streamlit>=1.45.1->-r requirements.txt (line 19)) (5.5.2)\n",
      "Requirement already satisfied: click<9,>=7.0 in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from streamlit>=1.45.1->-r requirements.txt (line 19)) (8.1.8)\n",
      "Requirement already satisfied: pandas<3,>=1.4.0 in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from streamlit>=1.45.1->-r requirements.txt (line 19)) (2.2.3)\n",
      "Requirement already satisfied: pillow<12,>=7.1.0 in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from streamlit>=1.45.1->-r requirements.txt (line 19)) (11.0.0)\n",
      "Requirement already satisfied: protobuf<7,>=3.20 in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from streamlit>=1.45.1->-r requirements.txt (line 19)) (5.29.3)\n",
      "Requirement already satisfied: pyarrow>=7.0 in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from streamlit>=1.45.1->-r requirements.txt (line 19)) (20.0.0)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from streamlit>=1.45.1->-r requirements.txt (line 19)) (0.10.2)\n",
      "Requirement already satisfied: watchdog<7,>=2.1.5 in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from streamlit>=1.45.1->-r requirements.txt (line 19)) (6.0.0)\n",
      "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from streamlit>=1.45.1->-r requirements.txt (line 19)) (3.1.44)\n",
      "Requirement already satisfied: pydeck<1,>=0.8.0b4 in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from streamlit>=1.45.1->-r requirements.txt (line 19)) (0.9.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from tiktoken>=0.9.0->-r requirements.txt (line 21)) (2024.11.6)\n",
      "Requirement already satisfied: multitasking>=0.0.7 in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from yfinance>=0.2.61->-r requirements.txt (line 23)) (0.0.11)\n",
      "Requirement already satisfied: platformdirs>=2.0.0 in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from yfinance>=0.2.61->-r requirements.txt (line 23)) (4.3.7)\n",
      "Requirement already satisfied: pytz>=2022.5 in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from yfinance>=0.2.61->-r requirements.txt (line 23)) (2025.1)\n",
      "Requirement already satisfied: frozendict>=2.3.4 in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from yfinance>=0.2.61->-r requirements.txt (line 23)) (2.4.6)\n",
      "Requirement already satisfied: peewee>=3.16.2 in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from yfinance>=0.2.61->-r requirements.txt (line 23)) (3.18.1)\n",
      "Requirement already satisfied: beautifulsoup4>=4.11.1 in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from yfinance>=0.2.61->-r requirements.txt (line 23)) (4.13.4)\n",
      "Requirement already satisfied: curl_cffi>=0.7 in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from yfinance>=0.2.61->-r requirements.txt (line 23)) (0.11.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.3.24->-r requirements.txt (line 9)) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.3.24->-r requirements.txt (line 9)) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.3.24->-r requirements.txt (line 9)) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.3.24->-r requirements.txt (line 9)) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.3.24->-r requirements.txt (line 9)) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.3.24->-r requirements.txt (line 9)) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.3.24->-r requirements.txt (line 9)) (1.18.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from altair<6,>=4.0->streamlit>=1.45.1->-r requirements.txt (line 19)) (3.1.5)\n",
      "Requirement already satisfied: jsonschema>=3.0 in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from altair<6,>=4.0->streamlit>=1.45.1->-r requirements.txt (line 19)) (4.23.0)\n",
      "Requirement already satisfied: narwhals>=1.14.2 in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from altair<6,>=4.0->streamlit>=1.45.1->-r requirements.txt (line 19)) (1.40.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from beautifulsoup4>=4.11.1->yfinance>=0.2.61->-r requirements.txt (line 23)) (2.7)\n",
      "Requirement already satisfied: cffi>=1.12.0 in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from curl_cffi>=0.7->yfinance>=0.2.61->-r requirements.txt (line 23)) (1.17.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community>=0.3.24->-r requirements.txt (line 9)) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community>=0.3.24->-r requirements.txt (line 9)) (0.9.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit>=1.45.1->-r requirements.txt (line 19)) (4.0.12)\n",
      "Requirement already satisfied: decorator in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from ipython>=7.23.1->ipykernel>=6.29.5->-r requirements.txt (line 6)) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from ipython>=7.23.1->ipykernel>=6.29.5->-r requirements.txt (line 6)) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from ipython>=7.23.1->ipykernel>=6.29.5->-r requirements.txt (line 6)) (0.19.2)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from ipython>=7.23.1->ipykernel>=6.29.5->-r requirements.txt (line 6)) (3.0.51)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from ipython>=7.23.1->ipykernel>=6.29.5->-r requirements.txt (line 6)) (2.19.1)\n",
      "Requirement already satisfied: stack_data in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from ipython>=7.23.1->ipykernel>=6.29.5->-r requirements.txt (line 6)) (0.6.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from jupyter-client>=6.1.12->ipykernel>=6.29.5->-r requirements.txt (line 6)) (2.9.0.post0)\n",
      "Requirement already satisfied: pywin32>=300 in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel>=6.29.5->-r requirements.txt (line 6)) (310)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from langchain-core<1.0.0,>=0.3.58->langchain>=0.3.25->-r requirements.txt (line 8)) (1.33)\n",
      "Requirement already satisfied: ormsgpack<2.0.0,>=1.8.0 in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from langgraph-checkpoint<3.0.0,>=2.0.10->langgraph>=0.4.3->-r requirements.txt (line 12)) (1.9.1)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from langsmith<0.4,>=0.1.17->langchain>=0.3.25->-r requirements.txt (line 8)) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from langsmith<0.4,>=0.1.17->langchain>=0.3.25->-r requirements.txt (line 8)) (0.23.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from markdown-it-py>=1.0->jupytext>=1.17.1->-r requirements.txt (line 7)) (0.1.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from pandas<3,>=1.4.0->streamlit>=1.45.1->-r requirements.txt (line 19)) (2025.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi>=0.115.12->-r requirements.txt (line 3)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi>=0.115.12->-r requirements.txt (line 3)) (2.33.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi>=0.115.12->-r requirements.txt (line 3)) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from requests<3,>=2->langchain>=0.3.25->-r requirements.txt (line 8)) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from requests<3,>=2->langchain>=0.3.25->-r requirements.txt (line 8)) (1.26.20)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from SQLAlchemy<3,>=1.4->langchain>=0.3.25->-r requirements.txt (line 8)) (3.1.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from typer>=0.15.2->fastmcp>=2.3.3->-r requirements.txt (line 4)) (1.5.4)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from nbformat->jupytext>=1.17.1->-r requirements.txt (line 7)) (2.21.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from cffi>=1.12.0->curl_cffi>=0.7->yfinance>=0.2.61->-r requirements.txt (line 23)) (2.22)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit>=1.45.1->-r requirements.txt (line 19)) (5.0.2)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel>=6.29.5->-r requirements.txt (line 6)) (0.8.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from jinja2->altair<6,>=4.0->streamlit>=1.45.1->-r requirements.txt (line 19)) (3.0.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.58->langchain>=0.3.25->-r requirements.txt (line 8)) (3.0.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit>=1.45.1->-r requirements.txt (line 19)) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit>=1.45.1->-r requirements.txt (line 19)) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit>=1.45.1->-r requirements.txt (line 19)) (0.24.0)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=7.23.1->ipykernel>=6.29.5->-r requirements.txt (line 6)) (0.2.13)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->jupyter-client>=6.1.12->ipykernel>=6.29.5->-r requirements.txt (line 6)) (1.17.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community>=0.3.24->-r requirements.txt (line 9)) (1.1.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from stack_data->ipython>=7.23.1->ipykernel>=6.29.5->-r requirements.txt (line 6)) (2.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from stack_data->ipython>=7.23.1->ipykernel>=6.29.5->-r requirements.txt (line 6)) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\dan guilliams\\appdata\\roaming\\python\\python312\\site-packages (from stack_data->ipython>=7.23.1->ipykernel>=6.29.5->-r requirements.txt (line 6)) (0.2.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0 -> 25.1.1\n",
      "[notice] To update, run: C:\\Python312\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Install requirements\n",
    "%pip install -r requirements.txt\n",
    "# Or whatever equivelent you prefer (poetry, uv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe8a849",
   "metadata": {},
   "source": [
    "## 2. Start Redis (for Idempotency Cache)\n",
    "\n",
    "We use Redis for request deduplication and caching. This ensures our system is idempotent and avoids redundant processing.\n",
    "\n",
    "We ensure a unique `request_id` is sent \n",
    "\n",
    "**Why we do this:** Redis provides fast, in-memory caching that helps us:\n",
    "1. Deduplicate analysis requests (idempotency)\n",
    "2. Cache expensive operations like semantic searches\n",
    "3. Reduce API costs and latency by storing LLM responses\n",
    "4. Ensure consistent behavior even with intermittent failures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7843b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "docker: Error response from daemon: Conflict. The container name \"/local-redis\" is already in use by container \"d7532e8e07eda16696991be756f16a983f49b82035ed57a286f0bc5f0aaafb22\". You have to remove (or rename) that container to be able to reuse that name.\n",
      "\n",
      "Run 'docker run --help' for more information\n"
     ]
    }
   ],
   "source": [
    "# Start Docker Service for Redis:\n",
    "!docker run -d --name local-redis -p 6379:6379 redis:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3629950",
   "metadata": {},
   "source": [
    "## 3. System Architecture Overview\n",
    "\n",
    "Our system follows a layered architecture with distinct components handling specific responsibilities:\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────┐\n",
    "│          Client Layer           │\n",
    "│ (Notebook, run_analysis.py)     │\n",
    "└────────────────┬────────────────┘\n",
    "                 │\n",
    "┌────────────────▼────────────────┐\n",
    "│        API Service Layer        │\n",
    "│ (main_security_agent_server.py) │\n",
    "└────────────────┬────────────────┘\n",
    "                 │\n",
    "┌────────────────▼────────────────┐\n",
    "│        Agent Layer             │\n",
    "│ (LangChain, LangGraph, ReAct)   │\n",
    "└────────────────┬────────────────┘\n",
    "                 │\n",
    "┌────────────────▼────────────────┐\n",
    "│       Tools Layer               │\n",
    "│ (MCP Server, mcp_cve_server.py) │\n",
    "└────────────────┬────────────────┘\n",
    "                 │\n",
    "┌────────────────▼────────────────┐\n",
    "│       Storage Layer             │\n",
    "│ (FAISS, Redis, SQLite)          │\n",
    "└─────────────────────────────────┘\n",
    "```\n",
    "\n",
    "**Why this architecture:** \n",
    "\n",
    "1. **Separation of Concerns**: Each layer has a distinct responsibility\n",
    "2. **Scalability**: Components can be scaled independently\n",
    "3. **Resilience**: Failures in one layer don't cascade to others\n",
    "4. **Maintainability**: Easier to update or replace individual components\n",
    "5. **Testing**: Components can be tested in isolation\n",
    "\n",
    "### Key Project Files and Their Roles\n",
    "\n",
    "```\n",
    ".\n",
    "├── main_security_agent_server.py  # FastAPI server coordinating analysis\n",
    "├── mcp_cve_server.py              # Tool server providing CVE search capabilities\n",
    "├── run_analysis.py                # CLI script for batch processing\n",
    "├── data/                          # Data storage\n",
    "│   ├── incidents.json             # Input security incidents\n",
    "│   ├── kev.json                   # Known Exploited Vulnerabilities\n",
    "│   ├── nvd_subset.json            # National Vulnerability Database subset\n",
    "│   └── vectorstore/               # FAISS vector indexes\n",
    "├── setup/                         # Setup scripts\n",
    "│   ├── download_cve_data.py       # Downloads CVE data\n",
    "│   └── build_faiss_indexes.py     # Builds vector indexes\n",
    "└── utils/                         # Utility functions\n",
    "    ├── retrieval_utils.py         # Vector search functions\n",
    "    ├── flatteners.py              # Text preprocessing for embeddings\n",
    "    ├── prompt_utils.py            # Prompt generation\n",
    "    ├── datastore_utils.py         # Database operations\n",
    "    └── decorators.py              # Logging and caching\n",
    "``` \n",
    "# 4. Data Ingestion & Preprocessing\n",
    "\n",
    "In this section, we'll explore the input data and prepare it for analysis. We need to:\n",
    "\n",
    "1. Load and inspect incident data\n",
    "2. Retrieve and prepare CVE data (KEV and NVD)\n",
    "3. Create flattened text representations for vector embedding\n",
    "\n",
    "**Why we do this:** Proper data preparation is critical for effective semantic search. By flattening complex JSON structures into searchable text, we enable the embedding model to capture semantic relationships between incidents and vulnerabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abec274",
   "metadata": {},
   "source": [
    "### Set Up Data Sources\n",
    "\n",
    "In addition to the `incidents.json` file, we will have some main data sources to draw upon for the application.\n",
    "\n",
    "The first two will be CVE data downloaded from the [Known Exploided Vulnerabilities Catalog](https://www.cisa.gov/known-exploited-vulnerabilities-catalog) (KEV).\n",
    "- This contains data on vulnerabilities that have _actually been used in attacks_.\n",
    "\n",
    "The second primary source is the government's [National Vulnerability Databaasse](https://nvd.nist.gov/vuln) (NVD).\n",
    "- This contains data on known vulnerabilities, often given by companies (SAP, Microsoft, etc.) and is brought to the attention of developers to ensure safe handling of vulnerabilities as they are discovered.\n",
    "\n",
    "#### Pulling the Data\n",
    "\n",
    "NIST has a reputation for a rate limit that we would far exceed for our purposes here (5 request perr 30 seconds) if used on individual CVEs.  Additionally, **the input data (incidents.json) does not specify any CVE IDs**, so the general spirit of our first step is to acquire the data we can in order to use NLP to map incident data with CVEs for better risk assessment.\n",
    "\n",
    "We begin by pulling the entire current KEV dataset JSON and the NVD.zip for a given year (here we used 2025):\n",
    "```python\n",
    "# NVD feed URL for 2025\n",
    "NVD_FEED_URL = (\n",
    "    \"https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-2025.json.zip\"\n",
    ")\n",
    "# CISA KEV (Known-Exploited Vulnerabilities) JSON feed\n",
    "KEV_FEED_URL = (\n",
    "    \"https://www.cisa.gov/sites/default/files/feeds/known_exploited_vulnerabilities.json\"\n",
    ")\n",
    "```\n",
    "\n",
    "The KEV JSON is manageable (around 1.2 MB), but the unzipped NVD JSON for 2025 (this far) is almosst 30 MB.  That isn't impossibly large, but when we are looking at bringing tokens, latency, and most importantly, accuracy, into the equation, we will want to isolate entries where possible for the \"canonical dataset\" that we are working with.\n",
    "\n",
    "We instrument a simple but effective method by extracting software names from the `incidents.json`'s `affected_assets.installed_software` field.  We then use that as a filter to create a second JSON (`nvd_subset.json`) of entries from the larger NVD dataset, but now only pertaining to affected software in our incidents.\n",
    "\n",
    "```python\n",
    "# setup/download_cve_data.py\n",
    "def extract_vendor_filters() -> list[str]:\n",
    "    \"\"\"Read all installed_software names from incidents and return lowercase tokens.\"\"\"\n",
    "    incidents = json.loads(INCIDENTS_PATH.read_text())\n",
    "    filters = set()\n",
    "    for inc in incidents:\n",
    "        for asset in inc.get(\"affected_assets\", []):\n",
    "            for sw in asset.get(\"installed_software\", []):\n",
    "                # Option 1: take the full name\n",
    "                filters.add(sw[\"name\"].lower())\n",
    "                # Option 2: split vendor and product\n",
    "                vendor = sw[\"name\"].split()[0].lower()\n",
    "                filters.add(vendor)\n",
    "    return sorted(filters)\n",
    "```\n",
    "\n",
    "This brings down the size to about 8.5 MB which is much more reasonable and gives our agent-to-be a more narrowed scope to search upon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b16e4a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total incidents: 39\n",
      "Fields in first incident: ['incident_id', 'timestamp', 'title', 'description', 'affected_assets', 'observed_ttps', 'indicators_of_compromise', 'initial_findings']\n",
      "\n",
      "Sample incident details:\n",
      "{\n",
      "  \"incident_id\": \"INC-2023-08-01-001\",\n",
      "  \"timestamp\": \"2023-08-01T09:15:00Z\",\n",
      "  \"title\": \"Unauthorized Access Attempt on VPN Gateway\",\n",
      "  \"description\": \"Multiple failed login attempts followed by a successful connection from an unusual geographic location on the main VPN gateway.\",\n",
      "  \"affected_assets\": [\n",
      "    {\n",
      "      \"hostname\": \"vpn-gateway-01\",\n",
      "      \"ip_address\": \"203.0.113.1\",\n",
      "      \"os\": \"Cisco IOS XE\",\n",
      "      \"installed_software\": [\n",
      "        {\n",
      "          \"name\": \"Cisco IOS XE\",\n",
      "          \"version\": \"17.3.4a\"\n",
      "        }\n",
      "      ],\n",
      "      \"role\": \"VPN Gateway\"\n",
      "    }\n",
      "  ],\n",
      "  \"observed_ttps\": [\n",
      "    {\n",
      "      \"framework\": \"MITRE ATT&CK\",\n",
      "      \"id\": \"T1110\",\n",
      "      \"name\": \"Brute Force\"\n",
      "    },\n",
      "    {\n",
      "      \"framework\": \"MITRE ATT&CK\",\n",
      "      \"id\": \"T1078\",\n",
      "      \"name\": \"Valid Accounts\"\n",
      "    }\n",
      "  ],\n",
      "  \"indicators_of_compromise\": [\n",
      "    {\n",
      "      \"type\": \"ip_address\",\n",
      "      \"value\": \"172.91.8.123\",\n",
      "      \"context\": \"Source IP of successful login\"\n",
      "    },\n",
      "    {\n",
      "      \"type\": \"username\",\n",
      "      \"value\": \"admin\",\n",
      "      \"context\": \"Account used for successful login\"\n",
      "    }\n",
      "  ],\n",
      "  \"initial_findings\": \"Credential stuffing or brute force attack successful against VPN.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Ensure we have an OpenAI API key\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    raise ValueError(\n",
    "        \"Please set the OPENAI_API_KEY environment variable in your .env file\"\n",
    "    )\n",
    "\n",
    "# Load incidents data\n",
    "data_dir = Path('data')\n",
    "with open(data_dir / 'incidents.json') as f:\n",
    "    incidents = json.load(f)\n",
    "\n",
    "# Display overview statistics\n",
    "print(f\"Total incidents: {len(incidents)}\")\n",
    "print(\"Fields in first incident:\", list(incidents[0].keys()))\n",
    "\n",
    "# Display first incident in pretty format\n",
    "print(\"\\nSample incident details:\")\n",
    "print(json.dumps(incidents[0], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "981cd927",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEV entries: 1351\n",
      "\n",
      "Sample KEV entry:\n",
      "{\n",
      "  \"cveID\": \"CVE-2023-38950\",\n",
      "  \"vendorProject\": \"ZKTeco\",\n",
      "  \"product\": \"BioTime\",\n",
      "  \"vulnerabilityName\": \"ZKTeco BioTime Path Traversal Vulnerability\",\n",
      "  \"dateAdded\": \"2025-05-19\",\n",
      "  \"shortDescription\": \"ZKTeco BioTime contains a path traversal vulnerability in the iclock API that allows an unauthenticated attacker to read arbitrary files via supplying a crafted payload.\",\n",
      "  \"requiredAction\": \"Apply mitigations per vendor instructions, follow applicable BOD 22-01 guidance for cloud services, or discontinue use of the product if mitigations are unavailable.\",\n",
      "  \"dueDate\": \"2025-06-09\",\n",
      "  \"knownRansomwareCampaignUse\": \"Unknown\",\n",
      "  \"notes\": \"https://www.zkteco.com/en/Security_Bulletinsibs ; https://nvd.nist.gov/vuln/detail/CVE-2023-38950\",\n",
      "  \"cwes\": [\n",
      "    \"CWE-22\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Load KEV (Known Exploited Vulnerabilities) data\n",
    "with open(data_dir / 'kev.json') as f:\n",
    "    kev_data = json.load(f)\n",
    "    \n",
    "print(f\"KEV entries: {len(kev_data.get('vulnerabilities', []))}\")\n",
    "print(\"\\nSample KEV entry:\")\n",
    "print(json.dumps(kev_data.get('vulnerabilities', [])[0], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b5095bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVD entries: 3062\n",
      "\n",
      "Sample NVD entry (CVE-2025-0020):\n",
      "Violation of Secure Design Principles, Hidden Functionality, Incorrect Provision of Specified Functionality vulnerability in ArcGIS (Authentication) allows Privilege Abuse, Manipulating Hidden Fields, Configuration/Environment Manipulation.\n",
      "\n",
      "The ArcGIS client_credentials OAuth 2.0 API implementation does not adhere to the RFC/standards; This hidden (known and by-design, but undocumented) functionality enables a requestor (Referred to as client in RFC 6749) to request an, undocumented, custom token expiration from ArcGIS (Referred to as authorization server in RFC 6749).\n"
     ]
    }
   ],
   "source": [
    "# Load NVD (National Vulnerability Database) data\n",
    "with open(data_dir / 'nvd_subset.json') as f:\n",
    "    nvd_data = json.load(f)\n",
    "    \n",
    "print(f\"NVD entries: {len(nvd_data)}\")\n",
    "\n",
    "# Display a sample NVD entry\n",
    "sample_cve_id = list(nvd_data.keys())[0]\n",
    "print(f\"\\nSample NVD entry ({sample_cve_id}):\")\n",
    "# Just show the description part to keep it manageable\n",
    "desc = nvd_data[sample_cve_id].get(\"cve\", {}).get(\"description\", {}).get(\"description_data\", [{}])[0].get(\"value\", \"\")\n",
    "print(desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bf576c",
   "metadata": {},
   "source": [
    "## 5. Text Flattening for Vector Embedding\n",
    "\n",
    "Before building our vector indexes, we need to convert the structured data into a flattened text format suitable for embedding. Let's examine our flattening strategies:\n",
    "\n",
    "**Why we do this:** Embeddings work on raw text, but our data is in complex JSON structures. Flattening transforms these structures into searchable text while preserving the semantic meaning of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f81694",
   "metadata": {},
   "source": [
    "Let's examine the flattening functions to understand how they work.  Here is an example implementation followed by an example of using it in the wild."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "780f43fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils/flatteners.py example implementation\n",
    "def flatten_incident(incident: dict) -> str:\n",
    "    \"\"\"\n",
    "    Flatten an incident into a text representation for embedding.\n",
    "    \n",
    "    Args:\n",
    "        incident: The incident dict to flatten\n",
    "        \n",
    "    Returns:\n",
    "        A string representation of the incident\n",
    "    \"\"\"\n",
    "    # Start with the title and description\n",
    "    text = f\"{incident.get('title', '')}\\n{incident.get('description', '')}\\n\"\n",
    "    \n",
    "    # Add initial findings\n",
    "    text += f\"{incident.get('initial_findings', '')}\\n\"\n",
    "    \n",
    "    # Add affected assets\n",
    "    for asset in incident.get(\"affected_assets\", []):\n",
    "        text += f\"Asset: {asset.get('hostname', '')} ({asset.get('ip_address', '')})\\n\"\n",
    "        text += f\"OS: {asset.get('os', '')}\\n\"\n",
    "        text += f\"Role: {asset.get('role', '')}\\n\"\n",
    "        \n",
    "        # Add installed software\n",
    "        for sw in asset.get(\"installed_software\", []):\n",
    "            text += f\"Software: {sw.get('name', '')} {sw.get('version', '')}\\n\"\n",
    "    \n",
    "    # Add TTPs (Tactics, Techniques, and Procedures)\n",
    "    for ttp in incident.get(\"observed_ttps\", []):\n",
    "        text += f\"TTP: {ttp.get('name', '')} ({ttp.get('id', '')})\\n\"\n",
    "    \n",
    "    # Add indicators of compromise\n",
    "    for ioc in incident.get(\"indicators_of_compromise\", []):\n",
    "        text += f\"IoC: {ioc.get('type', '')}: {ioc.get('value', '')} - {ioc.get('context', '')}\\n\"\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a22f4ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08:09:02 DEBUG    [root] Debug logging initialized\n",
      "08:09:02 INFO     [root] Info logging initialized\n",
      "08:09:02 WARNING  [root] Warning logging initialized\n",
      "08:09:02 ERROR    [root] Error logging initialized\n",
      "Flattened KEV document:\n",
      "CVE CVE-2023-38950\n",
      "ZKTeco\n",
      "BioTime\n",
      "ZKTeco BioTime Path Traversal Vulnerability\n",
      "ZKTeco BioTime contains a path traversal vulnerability in the iclock API that allows an unauthenticated attacker to read a ...\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Flattened NVD document:\n",
      "CVE CVE-2025-0020\n",
      "Violation of Secure Design Principles, Hidden Functionality, Incorrect Provision of Specified Functionality vulnerability in ArcGIS (Authentication) allows Privilege Abuse, Manipulat ...\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Flattened Incident document:\n",
      "Unauthorized Access Attempt on VPN Gateway\n",
      "Multiple failed login attempts followed by a successful connection from an unusual geographic location on the main VPN gateway.\n",
      "Credential stuffing or brute  ...\n"
     ]
    }
   ],
   "source": [
    "from utils.flatteners import flatten_kev, flatten_nvd, flatten_incident\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "# Example KEV entry flattening\n",
    "sample_kev = kev_data.get('vulnerabilities', [])[0]\n",
    "doc_kev = flatten_kev(sample_kev)\n",
    "print(\"Flattened KEV document:\")\n",
    "print(doc_kev.page_content[:200], \"...\")\n",
    "print(f\"\\n{'-'*50}\\n\")\n",
    "\n",
    "# Example NVD entry flattening\n",
    "sample_nvd = list(nvd_data.values())[0]\n",
    "doc_nvd = flatten_nvd(sample_nvd)\n",
    "print(\"\\nFlattened NVD document:\")\n",
    "print(doc_nvd.page_content[:200], \"...\")\n",
    "print(f\"\\n{'-'*50}\\n\")\n",
    "\n",
    "# Example Incident flattening\n",
    "doc_inc = Document(\n",
    "    page_content=flatten_incident(incidents[0]), \n",
    "    metadata={\"incident_id\": incidents[0][\"incident_id\"]}\n",
    ")\n",
    "print(\"\\nFlattened Incident document:\")\n",
    "print(doc_inc.page_content[:200], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a1359b",
   "metadata": {},
   "source": [
    "## 6. Building Vector Indexes\n",
    "\n",
    "Now we build FAISS vector indexes for efficient semantic search across our data sources. This process involves:\n",
    "\n",
    "1. Initializing the OpenAI embeddings model\n",
    "2. Creating FAISS indexes for KEV, NVD, and historical incident data\n",
    "3. Setting up utilities for semantic search\n",
    "\n",
    "**Why we do this:** Vector indexes enable fast similarity search over large datasets. By precomputing embeddings and storing them in FAISS indexes, we can perform semantic searches in milliseconds rather than having to recompute embeddings for each query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f50d2e1",
   "metadata": {},
   "source": [
    "### Indexing\n",
    "\n",
    "Initially, we simply gave the agent tools to search the JSON files, but this of course was not as efficient or ideal long-term as some other options that gave it more intelligence and accuracy.\n",
    "\n",
    "The static and reasonable structure and size of entries in both the KVD and NVD made them natural candidates for chunking, embedding, and indexing, each into their own FAISS index.\n",
    "\n",
    "We used `langchain_openai`'s `OpenAIEmbeddings` with the default model of `text-embedding-3-small` for both performance and low cost given the context of this project.\n",
    "\n",
    "We wrote the setup scripts to primarily run via CLI arguments since these are not intended to be called frequently, if at all, by our application later.  They are instead intended to be one-time runs to \"initialize\" a system that mimics a setup that a production environment would likely already have (Weaviate/Pinecone, Historical Data, Known Incindents, etc.).\n",
    "\n",
    "```python\n",
    "# setup/build_faiss_KEV_and_NVD_indexes.py\n",
    "# ---------- CLI --------------\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--model\", default=\"text-embedding-3-small\")\n",
    "parser.add_argument(\"--topk-test\", type=int, default=3)\n",
    "args = parser.parse_args()\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=args.model, show_progress_bar=True)\n",
    "```\n",
    "\n",
    "We use our embedddings naturally to create our indexes:\n",
    "\n",
    "```python\n",
    "def build_kev_index():\n",
    "    # ---------- Build / Skip KEV index ----------\n",
    "    if index_is_fresh(kev_json, kev_out):\n",
    "        print(\"✅ KEV index up-to-date – skipping build\")\n",
    "    else:\n",
    "        print(\"🔄 Building KEV index …\")\n",
    "        kev_raw  = json.load(kev_json.open())\n",
    "        kev_docs = [flatten_kev(e) for e in kev_raw[\"vulnerabilities\"]]\n",
    "        FAISS.from_documents(kev_docs, embeddings).save_local(kev_out)\n",
    "        print(\"✅ Saved KEV index to data/vectorstore/kev\\n\")\n",
    "\n",
    "def build_nvd_index():\n",
    "    # ---------- Build / Skip NVD index ----------\n",
    "    nvd_json = DATA_DIR / \"nvd_subset.json\"\n",
    "    nvd_out  = OUT_DIR / \"nvd\"\n",
    "\n",
    "    if index_is_fresh(nvd_json, nvd_out):\n",
    "        print(\"✅ NVD index up-to-date – skipping build\")\n",
    "    else:\n",
    "        print(\"🔄 Building NVD index …\")\n",
    "        nvd_raw  = json.load(nvd_json.open())\n",
    "        nvd_docs = [flatten_nvd(item) for item in nvd_raw.values()]\n",
    "        FAISS.from_documents(nvd_docs, embeddings).save_local(nvd_out)\n",
    "        print(\"✅ Saved NVD index to data/vectorstore/nvd\\n\")\n",
    "```\n",
    "\n",
    "**Note:** You'll see there is a check on whether the index is \"fresh\".  We instill a check ensures it ONLY builds the index if the lastModifiedDate of the corresponding JSON is more recent than the current index (and of course, we build the index if no index yet exists).\n",
    "\n",
    "Here is an example of initializing out embeddings and indexes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "faf8d52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "from utils.retrieval_utils import DATA_DIR\n",
    "\n",
    "def initialize_openai_embeddings():\n",
    "    \"\"\"\n",
    "    Initialize the global OpenAI embeddings object for vector representations.\n",
    "    \"\"\"\n",
    "    global embeddings\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "\n",
    "def initialize_faiss_indexes():\n",
    "    \"\"\"\n",
    "    Initialize global FAISS vector indexes for different vulnerability databases.\n",
    "    \"\"\"\n",
    "    global KEV_FAISS, NVD_FAISS, INCIDENT_HISTORY_FAISS\n",
    "    if embeddings is None:\n",
    "        initialize_openai_embeddings()\n",
    "\n",
    "    # Load pre-built FAISS indexes\n",
    "    KEV_FAISS = FAISS.load_local(DATA_DIR / \"kev\", embeddings,\n",
    "                              allow_dangerous_deserialization=True)\n",
    "    \n",
    "    NVD_FAISS = FAISS.load_local(DATA_DIR / \"nvd\", embeddings,\n",
    "                              allow_dangerous_deserialization=True)\n",
    "    \n",
    "    INCIDENT_HISTORY_FAISS = FAISS.load_local(DATA_DIR / \"incident_analysis_history\", \n",
    "                               embeddings, allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bb2523",
   "metadata": {},
   "source": [
    "Implementation details from `utils/retrieval_utils.py`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc6bc4bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08:09:04 INFO     [root] Initializing OpenAI embeddings...\n",
      "08:09:05 INFO     [root] OpenAI embeddings initialized!\n",
      "08:09:05 INFO     [root] initialize_openai_embeddings completed in 0.93s\n",
      "08:09:05 INFO     [root] Loading KEV FAISS index...\n",
      "08:09:05 DEBUG    [faiss.loader] Environment variable FAISS_OPT_LEVEL is not set, so let's pick the instruction set according to the current CPU\n",
      "08:09:05 INFO     [faiss.loader] Loading faiss with AVX2 support.\n",
      "08:09:05 INFO     [faiss.loader] Successfully loaded faiss with AVX2 support.\n",
      "08:09:05 INFO     [faiss] Failed to load GPU Faiss: name 'GpuIndexIVFFlat' is not defined. Will not load constructor refs for GPU indexes. This is only an error if you're trying to use GPU Faiss.\n",
      "08:09:05 INFO     [root] KEV FAISS index loaded!\n",
      "08:09:05 INFO     [root] Loading NVD FAISS index...\n",
      "08:09:05 INFO     [root] NVD FAISS index loaded!\n",
      "08:09:05 INFO     [root] Loading Incident Analysis History FAISS index...\n",
      "08:09:05 INFO     [root] Incident Analysis History FAISS index loaded!\n",
      "08:09:05 INFO     [root] initialize_faiss_indexes completed in 0.10s\n",
      "Embeddings and FAISS indexes initialized successfully.\n"
     ]
    }
   ],
   "source": [
    "from utils.retrieval_utils import initialize_openai_embeddings, initialize_faiss_indexes\n",
    "\n",
    "# Initialize OpenAI embeddings and FAISS indexes\n",
    "initialize_openai_embeddings()\n",
    "initialize_faiss_indexes()\n",
    "print(\"Embeddings and FAISS indexes initialized successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46137df",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Normalizing\n",
    "\n",
    "Originally, when we began and gave the agent tools to perform semantic searches, general JSON queries, etc. to map incidents to CVEs andd perform an intelligent reasoning for their ranking, we explored ways to normalize the results.\n",
    "\n",
    "A common practice would be to have pre-defined tiers to map onto to give \"weights\" that would sum to a final risk score for an incident, but we found that:\n",
    "1. These often bloated more than ideal and led to larger risk scores than felt accurate (Ex: a 0.65 on a user marking an email as phishing but without clicking on it)\n",
    "2. These neglected to take more advantage of the model's latent intelligence.\n",
    "\n",
    "So we deferred to the model to reason itself on how to rank the score.\n",
    "\n",
    "**However...**\n",
    "\n",
    "For develoment, we have typically given the pipeline about 5 incident_ids per call to the agent to balance speed and accuracy.  The model might rank otherwise similar incidents at different levels when comparing against the other incidents in its current batch, so **we needed a way to normalize the risk scores of similar incidentts**.\n",
    "\n",
    "This is when we decided to create **dummy data** mimicking the concept and JSON structure of the incidents.json file to build up some historical data:\n",
    "```json\n",
    "    {\n",
    "        \"incident_id\": \"INC-2024-02-22-002\", \n",
    "        \"title\": \"Cloud Infrastructure Unauthorized Access\",\n",
    "        \"description\": \"Detected unauthorized access and potential data exfiltration from AWS cloud environment.\",\n",
    "        \"initial_findings\": \"Multiple API calls from unusual geographic locations detected outside of normal business hours.\",\n",
    "        \"timestamp\": \"2024-02-22T14:45:22Z\",\n",
    "        \"affected_assets\": [\n",
    "            {\n",
    "                \"hostname\": \"AWS-PROD-APP-01\",\n",
    "                \"ip_address\": \"10.0.1.45\",\n",
    "                \"os\": \"Amazon Linux 2\",\n",
    "                \"installed_software\": [\n",
    "                    {\"name\": \"Docker\", \"version\": \"20.10.23\"},\n",
    "                    {\"name\": \"Kubernetes\", \"version\": \"1.27.3\"}\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        \"observed_ttps\": [\n",
    "            {\n",
    "                \"id\": \"T1078.004\",\n",
    "                \"name\": \"Valid Accounts: Cloud Accounts\",\n",
    "                \"description\": \"Unauthorized access to cloud infrastructure\"\n",
    "            },\n",
    "            {\n",
    "                \"id\": \"T1530\",\n",
    "                \"name\": \"Data Transfer\",\n",
    "                \"description\": \"Potential unauthorized data movement\"\n",
    "            }\n",
    "        ],\n",
    "        \"indicators_of_compromise\": [\n",
    "            {\n",
    "                \"type\": \"container_id\",\n",
    "                \"value\": \"a1b2c3d4e5f6\",\n",
    "                \"description\": \"Suspicious container with unexpected network activity\"\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"library_name\",\n",
    "                \"value\": \"boto3-exfil-mod\",\n",
    "                \"description\": \"Potential custom exfiltration library\"\n",
    "            }\n",
    "        ]\n",
    "    }, ...\n",
    "```\n",
    "\n",
    "We simply gave examples of incidents to an LLM manually and asked for roughly 40 examples of such similar entries.  We then had it do the same but for its analysis of those entries, using the format we would later enforce in agent responses via a parser.\n",
    "\n",
    "That gave us a mimic of what would be, in a production setting, historical data and analyses.\n",
    "\n",
    "That meant we had once more FAISS index to build, using a standardized flattener on the incidents to ensure quality retrieval later, into a third FAISS index, under `data/vectorstore/incident_analysis_history` using effectively the same method as that of the KEV and NVD indexes.\n",
    "\n",
    "Below, we will briefly go through how we added flattening for vectors to have standardized embeddings, then we will discuss the actual implmenetation of the indexes.\n",
    "\n",
    "With that, we now had the initial setup process that, with the right tools, would allow our agent to perform its task:\n",
    "1. Consistently (accounting for similar incidents in the past, mapping those to assigned risk values in prior analyses)\n",
    "2. Quickly (removing latency of API calling on-demand, reducing file size for more specific scope where applicable)\n",
    "3. Accurately (using semantic search comined with general JSON traversal, informed with the relevant data at hand, and with the tools we will see later)\n",
    "\n",
    "The setup steps are now in a simple shell script `setup_initial_CVE_data_and_FAISS_indexes.sh` that you can see and run here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80238b47",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Downloading CVE data...\"\n",
      "Using vendor filters: ['.net', '.net framework', 'adobe', 'adobe acrobat reader', 'apache', 'apache http server', 'apache struts', 'apache tomcat', 'axios', 'cisco', 'cisco ios xe', 'custom', 'custom c application', 'custom c++ application', 'custom java application', 'django', 'docker', 'ffmpeg', 'google', 'google chrome', 'java', 'java jre', 'jinja2', 'kubernetes', 'libxml2', 'microsoft', 'microsoft office', 'microsoft powershell', 'microsoft remote desktop services', 'microsoft windows', 'microsoft windows server', 'mozilla', 'mozilla firefox', 'mysql', 'mysql connector/j', 'nginx', 'nmap', 'node.js', 'openldap', 'openssh', 'openssl', 'php', 'php-fpm', 'pillow', 'postgresql', 'python', 'reportlab', 'sqlalchemy', 'wordpress']\n",
      "NVD ZIP already present.\n",
      "Raw NVD JSON already present.\n",
      "Loading raw NVD JSON...\n",
      "Total CVE items: 12137\n",
      "Filtered down to 3062 CVEs. Saving subset...\n",
      "Wrote subset to data\\nvd_subset.json\n",
      "Downloading KEV feed...\n",
      "Wrote KEV data to data\\kev.json\n",
      "Prep complete. You can now import search_cves_by_software, lookup_cve, enrich_with_kev.\n",
      "\"Building FAISS indexes for KEV and NVD...\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Building historical incident analysis index...\"\n",
      "08:09:33 DEBUG    [root] Debug logging initialized\n",
      "08:09:33 INFO     [root] Info logging initialized\n",
      "08:09:33 WARNING  [root] Warning logging initialized\n",
      "08:09:33 ERROR    [root] Error logging initialized\n",
      "08:09:34 DEBUG    [faiss.loader] Environment variable FAISS_OPT_LEVEL is not set, so let's pick the instruction set according to the current CPU\n",
      "08:09:34 INFO     [faiss.loader] Loading faiss with AVX2 support.\n",
      "08:09:34 INFO     [faiss.loader] Successfully loaded faiss with AVX2 support.\n",
      "08:09:34 INFO     [faiss] Failed to load GPU Faiss: name 'GpuIndexIVFFlat' is not defined. Will not load constructor refs for GPU indexes. This is only an error if you're trying to use GPU Faiss.\n",
      "âœ… Historical Incident Analyses index up-to-date â€“ skipping build\n",
      "\n",
      "ðŸ”Ž top-3 Historical Incident Analyses matches for 'ransomware'\n",
      "08:09:34 DEBUG    [openai._base_client] Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'idempotency_key': 'stainless-python-retry-e2dea71b-c545-4cb6-b843-e9966067e3d3', 'post_parser': <function Embeddings.create.<locals>.parser at 0x0000028E5E147CE0>, 'json_data': {'input': [[34489, 316, 1698]], 'model': 'text-embedding-3-small', 'encoding_format': 'base64'}}\n",
      "08:09:34 DEBUG    [openai._base_client] Sending HTTP Request: POST https://api.openai.com/v1/embeddings\n",
      "08:09:36 DEBUG    [openai._base_client] HTTP Response: POST https://api.openai.com/v1/embeddings \"200 OK\" Headers([('date', 'Tue, 20 May 2025 12:09:37 GMT'), ('content-type', 'application/json'), ('transfer-encoding', 'chunked'), ('connection', 'keep-alive'), ('access-control-allow-origin', '*'), ('access-control-expose-headers', 'X-Request-ID'), ('openai-model', 'text-embedding-3-small'), ('openai-organization', 'user-7cmzkemi0vsac94gcovz3pry'), ('openai-processing-ms', '83'), ('openai-version', '2020-10-01'), ('strict-transport-security', 'max-age=31536000; includeSubDomains; preload'), ('via', 'envoy-router-759dd9478d-pnb2k'), ('x-envoy-upstream-service-time', '87'), ('x-ratelimit-limit-requests', '5000'), ('x-ratelimit-limit-tokens', '5000000'), ('x-ratelimit-remaining-requests', '4999'), ('x-ratelimit-remaining-tokens', '4999997'), ('x-ratelimit-reset-requests', '12ms'), ('x-ratelimit-reset-tokens', '0s'), ('x-request-id', 'req_574bdfc5088b3e65748028be8f445eaa'), ('cf-cache-status', 'DYNAMIC'), ('set-cookie', '__cf_bm=vvG0cVTLtxy1hZr6OfhDx6JHIbcEIQNhRVz8nen99Nc-1747742977-1.0.1.1-4Jvl9fJcx_XYu7_zjowknT4IwCLrogCOrqebwgkDSBvcdqf6yiZkHELBRqiizV8Z8FQZFbmgbW5WPjhj0XoztePjgdP_d2ywW7F5SH1jcmE; path=/; expires=Tue, 20-May-25 12:39:37 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('x-content-type-options', 'nosniff'), ('set-cookie', '_cfuvid=wtIZZzQr5b0uPW2ApZo_ksGIAVmZ6BSuC5vQriPIXGk-1747742977117-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('server', 'cloudflare'), ('cf-ray', '942bb9d92e3612a4-CLT'), ('content-encoding', 'gzip'), ('alt-svc', 'h3=\":443\"; ma=86400')])\n",
      "08:09:36 DEBUG    [openai._base_client] request_id: req_574bdfc5088b3e65748028be8f445eaa\n",
      "â€¢ Incident ID: INC-2024-05-12-005 | score=0.9302\n",
      "  Ransomware Attack on Healthcare Network\n",
      "Critical ransomware infection spreading across hospital network, encrypting patient records and medical systems.\n",
      "Multiple Windows servers showing signs of encryption, with ransom note detected on primary file servers.\n",
      "Software: Epic EMR 2023.1, Microsoft SQL Server 2019\n",
      "Software: Active Directory 2019, SCCM 2103\n",
      "TTPs: Data Encrypted for Impact, Remote Services\n",
      "TTP IDs: T1486, T1021\n",
      "OS: Windows Server 2019, Windows Server 2019\n",
      "IOC: C:\\RANSOM_NOTE.txt\n",
      "IOC: .lockedâ€¦\n",
      "\n",
      "â€¢ Incident ID: INC-2024-07-02-008 | score=1.1663\n",
      "  Insider Threat Data Exfiltration\n",
      "Detected unauthorized data access and exfiltration by privileged user.\n",
      "Unusual data access patterns and large file transfers to external storage.\n",
      "Software: Microsoft Office 2021, 7-Zip 23.01\n",
      "TTPs: Valid Accounts, Exfiltration Over Alternative Protocol\n",
      "TTP IDs: T1078, T1048\n",
      "OS: Windows 11 Pro\n",
      "IOC: C:\\Users\\admin\\Downloads\\sensitive_data.zipâ€¦\n",
      "\n",
      "â€¢ Incident ID: INC-2024-07-04-010 | score=1.1714\n",
      "  Cryptocurrency Mining Malware\n",
      "Unauthorized cryptocurrency mining software detected on multiple servers.\n",
      "High CPU usage and suspicious network connections to mining pools.\n",
      "Software: Apache 2.4.46, MySQL 8.0.26\n",
      "TTPs: Resource Hijacking\n",
      "TTP IDs: T1496\n",
      "OS: CentOS 8\n",
      "IOC: xmrigâ€¦\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:02<00:00,  2.35s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:02<00:00,  2.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Done!\"\n"
     ]
    }
   ],
   "source": [
    "%echo \"Downloading CVE data...\"\n",
    "!python ./setup/download_cve_data.py\n",
    "\n",
    "%echo \"Building FAISS indexes for KEV and NVD...\"\n",
    "!python ./setup/build_faiss_KEV_and_NVD_indexes.py\n",
    "\n",
    "%echo \"Building historical incident analysis index...\"\n",
    "!python ./setup/build_historical_incident_analyses_index.py\n",
    "\n",
    "%echo \"Done!\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ffd4cfe",
   "metadata": {},
   "source": [
    "## 7. Testing Semantic Search\n",
    "\n",
    "Let's test our vector indexes by performing semantic searches over the different data sources:\n",
    "\n",
    "**Why we do this:** Verifying semantic search capabilities ensures that our system can effectively identify relevant CVEs and historical incidents. This helps validate our data preparation and embedding strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d418b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search query: Unauthorized Access Attempt on VPN Gateway\n",
      "08:09:38 INFO     [root] _search completed in 1.09s\n",
      "\n",
      "Top 3 KEV matches:\n",
      "1. CVE-2015-1187 (variance: 1.897)\n",
      "   CVE CVE-2015-1187 D-Link and TRENDnet Multiple Devices D-Link and TRENDnet Multiple Devices Remote C...\n",
      "2. CVE-2006-2492 (variance: 1.930)\n",
      "   CVE CVE-2006-2492 Microsoft Word Microsoft Word Malformed Object Pointer Vulnerability Microsoft Wor...\n",
      "3. CVE-2020-3452 (variance: 1.905)\n",
      "   CVE CVE-2020-3452 Cisco Adaptive Security Appliance (ASA) and Firepower Threat Defense (FTD) Cisco A...\n",
      "08:09:38 INFO     [root] _search completed in 0.96s\n",
      "\n",
      "Top 3 NVD matches:\n",
      "1. CVE-2025-25724 (variance: 1.903)\n",
      "   CVE CVE-2025-25724 list_item_verbose in tar/util.c in libarchive through 3.7.7 does not check an str...\n",
      "2. CVE-2025-20153 (variance: 1.930)\n",
      "   CVE CVE-2025-20153 A vulnerability in the email filtering mechanism of Cisco Secure Email Gateway co...\n",
      "3. CVE-2025-30065 (variance: 1.938)\n",
      "   CVE CVE-2025-30065 Schema parsing in the parquet-avro module of Apache Parquet 1.15.0 and previous v...\n"
     ]
    }
   ],
   "source": [
    "from utils.retrieval_utils import _search, KEV_FAISS, NVD_FAISS\n",
    "\n",
    "# Perform a semantic search using an incident title\n",
    "query_text = incidents[0]['title']\n",
    "print(f\"Search query: {query_text}\")\n",
    "\n",
    "# Search KEV database\n",
    "kev_results = _search(KEV_FAISS, query_text, k=3)\n",
    "print(\"\\nTop 3 KEV matches:\")\n",
    "for i, r in enumerate(kev_results, 1):\n",
    "    print(f\"{i}. {r['cve_id']} (variance: {r['variance']:.3f})\")\n",
    "    print(f\"   {r.get('preview', '')[:100]}...\")\n",
    "\n",
    "# Search NVD database\n",
    "nvd_results = _search(NVD_FAISS, query_text, k=3)\n",
    "print(\"\\nTop 3 NVD matches:\")\n",
    "for i, r in enumerate(nvd_results, 1):\n",
    "    print(f\"{i}. {r['cve_id']} (variance: {r['variance']:.3f})\")\n",
    "    print(f\"   {r.get('preview', '')[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a5f927",
   "metadata": {},
   "source": [
    "Let's also examine the core search function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "adbec86a",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "\n",
    "def _search(\n",
    "    store: FAISS,\n",
    "    query: str,\n",
    "    k: int = 5,\n",
    "    use_mmr: bool = True,\n",
    "    lambda_mult: float = 0.7,\n",
    "    fetch_k: int = None\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Perform a semantic search on a given FAISS vector store.\n",
    "    \n",
    "    Args:\n",
    "        store: The FAISS vector store to search\n",
    "        query: The search query string\n",
    "        k: Number of top results to return\n",
    "        use_mmr: Use Maximal Marginal Relevance for diverse results\n",
    "        lambda_mult: Diversity control for MMR search\n",
    "        fetch_k: Number of documents to fetch before filtering for MMR\n",
    "        \n",
    "    Returns:\n",
    "        List of search results with metadata and scores\n",
    "    \"\"\"\n",
    "    if use_mmr:\n",
    "        # embed the query once\n",
    "        vec = embeddings.embed_query(query)\n",
    "        # if fetch_k not provided, default to 2*k\n",
    "        fk = fetch_k or (2 * k)\n",
    "        # call the vector-based MMR-with-scores method\n",
    "        pairs = store.max_marginal_relevance_search_with_score_by_vector(\n",
    "            vec, k=k, fetch_k=fk, lambda_mult=lambda_mult,\n",
    "        )\n",
    "    else:\n",
    "        # direct text-based similarity search (score included)\n",
    "        pairs = store.similarity_search_with_score(query, k=k)\n",
    "\n",
    "    # Format results\n",
    "    out = []\n",
    "    for doc, score in pairs:\n",
    "        meta = doc.metadata.copy()\n",
    "        meta[\"variance\"] = float(score)\n",
    "        meta[\"preview\"] = ' '.join(doc.page_content.replace('\\n', ' ').split())[:120]\n",
    "        out.append(meta)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69dde69c",
   "metadata": {},
   "source": [
    "# 8. Agent Architecture and MCP Tools\n",
    "\n",
    "In this section, we'll explore the core agent architecture and the MCP (Model Context Protocol) tools it uses to analyze security incidents and identify relevant CVEs.\n",
    "\n",
    "## 8.1 MCP Server: Tool Definitions\n",
    "\n",
    "Our agent uses a toolkit of specialized functions for incident analysis. These tools are defined in `mcp_cve_server.py` and exposed via the .\n",
    "\n",
    "**Why we do this:** \n",
    "- MCP provides a standardized way for LLMs to interact with external tools\n",
    "- Tools are defined with rich metadata (annotations) to guide the LLM\n",
    "- The server handles caching, error handling, and logging consistently\n",
    "- Tool definitions are separate from agent logic, enabling reuse\n",
    "\n",
    "The tools in the project itself empower the agent to semantic search at will either with structured or unstructured input, query the JSON files for details as needed, return static strings showing the schema of each JSON to assist in such, and more.  All of them are centralized in `mcp_cve_server.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8bcb23bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08:09:39 INFO     [root] Initializing OpenAI embeddings...\n",
      "08:09:39 INFO     [root] OpenAI embeddings initialized!\n",
      "08:09:39 INFO     [root] initialize_openai_embeddings completed in 0.48s\n",
      "08:09:39 INFO     [root] Loading KEV FAISS index...\n",
      "08:09:39 INFO     [root] KEV FAISS index loaded!\n",
      "08:09:39 INFO     [root] Loading NVD FAISS index...\n",
      "08:09:39 INFO     [root] NVD FAISS index loaded!\n",
      "08:09:39 INFO     [root] Loading Incident Analysis History FAISS index...\n",
      "08:09:39 INFO     [root] Incident Analysis History FAISS index loaded!\n",
      "08:09:39 INFO     [root] initialize_faiss_indexes completed in 0.06s\n",
      "08:09:39 DEBUG    [mcp.server.lowlevel.server] Initializing server 'cve'\n",
      "08:09:39 DEBUG    [mcp.server.lowlevel.server] Registering handler for ListToolsRequest\n",
      "08:09:39 DEBUG    [mcp.server.lowlevel.server] Registering handler for CallToolRequest\n",
      "08:09:39 DEBUG    [mcp.server.lowlevel.server] Registering handler for ListResourcesRequest\n",
      "08:09:39 DEBUG    [mcp.server.lowlevel.server] Registering handler for ReadResourceRequest\n",
      "08:09:39 DEBUG    [mcp.server.lowlevel.server] Registering handler for PromptListRequest\n",
      "08:09:39 DEBUG    [mcp.server.lowlevel.server] Registering handler for GetPromptRequest\n",
      "08:09:39 DEBUG    [mcp.server.lowlevel.server] Registering handler for ListResourceTemplatesRequest\n",
      "08:09:40 DEBUG    [mcp.server.lowlevel.server] Initializing server 'cve'\n",
      "08:09:40 DEBUG    [mcp.server.lowlevel.server] Registering handler for ListToolsRequest\n",
      "08:09:40 DEBUG    [mcp.server.lowlevel.server] Registering handler for CallToolRequest\n",
      "08:09:40 DEBUG    [mcp.server.lowlevel.server] Registering handler for ListResourcesRequest\n",
      "08:09:40 DEBUG    [mcp.server.lowlevel.server] Registering handler for ReadResourceRequest\n",
      "08:09:40 DEBUG    [mcp.server.lowlevel.server] Registering handler for PromptListRequest\n",
      "08:09:40 DEBUG    [mcp.server.lowlevel.server] Registering handler for GetPromptRequest\n",
      "08:09:40 DEBUG    [mcp.server.lowlevel.server] Registering handler for ListResourceTemplatesRequest\n"
     ]
    }
   ],
   "source": [
    "# Core tool definitions from mcp_cve_server.py\n",
    "from typing import Any, Dict, List\n",
    "from fastmcp import FastMCP\n",
    "from mcp_cve_server import NVD_INDEX\n",
    "from utils.decorators import timing_metric, cache_result\n",
    "from utils.retrieval_utils import match_incident_to_cves, semantic_search_cves\n",
    "\n",
    "mcp = FastMCP(\"cve\")\n",
    "\n",
    "@mcp.tool(annotations={\n",
    "    \"title\": \"Match Incident to CVEs using semantic search\",\n",
    "    \"readOnlyHint\": True,\n",
    "    \"destructiveHint\": False,\n",
    "    \"idempotentHint\": False,\n",
    "    \"openWorldHint\": False\n",
    "})\n",
    "\n",
    "@cache_result(ttl_seconds=30)  # cache identical incident queries for 30s\n",
    "def match_incident_to_cves_tool(incident_id: str, k: int = 5, use_mmr: bool = True) -> dict:\n",
    "    \"\"\"\n",
    "    Match an incident to potentially relevant CVEs using semantic search.\n",
    "    \n",
    "    Args:\n",
    "        incident_id: The ID of the incident to match\n",
    "        k: Maximum number of matches to return\n",
    "        use_mmr: Whether to use MMR for diversity\n",
    "        \n",
    "    Returns:\n",
    "        Dict containing matching CVEs from KEV and NVD databases\n",
    "    \"\"\"\n",
    "    return match_incident_to_cves(incident_id, k, use_mmr)\n",
    "\n",
    "@mcp.tool(\n",
    "  annotations={\n",
    "    \"title\": \"Semantic Free-Form CVE Search\",\n",
    "    \"readOnlyHint\": True,\n",
    "    \"destructiveHint\": False,\n",
    "    \"idempotentHint\": False,\n",
    "    \"openWorldHint\": False\n",
    "  }\n",
    ")\n",
    "@cache_result(ttl_seconds=30)  # cache identical free-form queries\n",
    "def semantic_search_cves_tool(\n",
    "    query: str,\n",
    "    sources: List[str] = [\"kev\", \"nvd\", \"historical\"],\n",
    "    k: int = 5,\n",
    "    use_mmr: bool = False,\n",
    "    lambda_mult: float = 0.7\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Perform a semantic search for CVEs using a free-form query.\n",
    "    Contains ability to search multiple indexes in a single call to help with speed and token use, eliminating the need for the agent to perform\n",
    "    multiple tools calls to search the FAISS indexes available.\n",
    "    \n",
    "    Args:\n",
    "        query: Free-form search query\n",
    "        sources: Which databases to search (\"kev\", \"nvd\", \"historical\")\n",
    "        k: Maximum number of results per source\n",
    "        use_mmr: Whether to use MMR for diversity\n",
    "        lambda_mult: Diversity parameter for MMR\n",
    "        \n",
    "    Returns:\n",
    "        Dict containing search results from specified sources\n",
    "    \"\"\"\n",
    "    return semantic_search_cves(query, sources, k, use_mmr, lambda_mult)\n",
    "\n",
    "@mcp.tool(annotations={\n",
    "    \"title\": \"Search NVD Entries for a specific match for ALL words in the query\",\n",
    "    \"readOnlyHint\": True,\n",
    "    \"destructiveHint\": False,\n",
    "    \"idempotentHint\": False,\n",
    "    \"openWorldHint\": False\n",
    "})\n",
    "@cache_result(ttl_seconds=30)  # cache identical free-form queries\n",
    "def search_nvd(query: str, limit: int = 10) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Return up to `limit` full CVE records whose fields match ALL words in `query`.\n",
    "    Case-insensitive substring match over CVE ID, description, and any reference URLs.\n",
    "    \"\"\"\n",
    "    qwords = query.lower().split()\n",
    "    matches = []\n",
    "    for cve_id, rec in NVD_INDEX.items():\n",
    "        # flatten searchable text\n",
    "        desc = rec.get(\"cve\", {}) \\\n",
    "                  .get(\"description\", {}) \\\n",
    "                  .get(\"description_data\", [{}])[0] \\\n",
    "                  .get(\"value\", \"\")\n",
    "        refs = \" \".join([r.get(\"url\",\"\") for r in rec.get(\"cve\",{}) \\\n",
    "                                          .get(\"references\",{}) \\\n",
    "                                          .get(\"reference_data\",[])])\n",
    "        text = f\"{cve_id} {desc} {refs}\".lower()\n",
    "        if all(w in text for w in qwords):\n",
    "            # return the full record so the agent can inspect any fields\n",
    "            matches.append(rec)\n",
    "            if len(matches) >= limit:\n",
    "                break\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54eefc0e",
   "metadata": {},
   "source": [
    "## 8.2 Prompt Engineering\n",
    "\n",
    "Templated prompts are key for detecting performance over time and ensuring standardization.  They are particularly well-suited for instances like this project where the input data sources are well formatted and consistent in theme.\n",
    "\n",
    "**Why we do this:** Well-crafted prompts are critical for LLM performance. Our prompts are designed to:\n",
    "- Provide clear instructions and context\n",
    "- Include example formats for outputs\n",
    "- Guide the agent to use appropriate tools at the right time\n",
    "- Support structured JSON output via Pydantic models\n",
    "\n",
    "We originally had the agent simply using the tools to perform semantic search of the given incident, after being given a list of `incident_id` values, but upon inspection and consideration, we realized that the steps were ALWAYS taking place of:\n",
    "\n",
    "1. Use the `get_incident_tool` function to retrieve the incident's full JSON object\n",
    "2. Use the `semantic_match_incident_tool` function to retrieve KVE and (potentially) NVD results\n",
    "3. Use the more agnostic `semantic_search_cves_tool` function to search against similar incidents from the historical index.\n",
    "\n",
    "We realized we could skip the latency and token usage by simply \"preprocessing\" that for the LLM and injecting it into a standard prompt template.\n",
    "\n",
    "We break up the template into a System and Human message, both of which are combined, before sending it to the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c31b052-33bf-4145-9147-4de1fa598ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt template from utils/prompt_utils.py\n",
    "SYSTEM_TMPL = \"\"\"\n",
    "You are a CVE‐analysis assistant. Analyze the following incidents and provide structured analysis.\n",
    "\n",
    "Incident Details:\n",
    "{incident_details}\n",
    "\n",
    "Batch FAISS matches (KEV/NVD):\n",
    "{batch_faiss_results}\n",
    "\n",
    "Historical FAISS‐anchoring context:\n",
    "{historical_faiss_results}\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "Now, when I ask you to analyze incidents, use the KEV/NVD context to inform your severity rankings and the historical context to normalize your severity rankings.\n",
    "\"\"\"\n",
    "\n",
    "# Human query example\n",
    "query = \"\"\"\n",
    "I need you to help me analyze some security incidents and rank their actual severity, using identify potential CVE connections and details.\n",
    "Let's start with a small sample to test the system:\n",
    "1. Note the incident IDs and summaries you have available to you already.\n",
    "2. For each incident:\n",
    "    a.  Understand Incident Context: Reason about the affected assets, observed TTPs, and initial findings.\n",
    "    b.  Identify Relevant CVEs: Determine which CVEs are potentially relevant based on the incident context and affected software/hardware, using LLM reasoning and potentially querying data sources.\n",
    "    c.  Prioritize CVEs: Assess the risk and impact of relevant CVEs in the context of the specific incident, going beyond standard scores like CVSS.\n",
    "    d.  Generate Analysis: Provide a brief, human-readable explanation of why certain CVEs are prioritized, linking them back to the incident details.\n",
    "3. Finally, and most importantly, provide an organized list of all analyzed incidents in the following format:\n",
    "{\n",
    "    \"incidents\": [\n",
    "        {\n",
    "            \"incident_id\": \"The ID of the incident that caused the error\",\n",
    "            \"incident_summary\": \"A brief summary of the incident\",\n",
    "            \"cve_ids\": [\n",
    "                {\n",
    "                    \"cve_id\": \"The CVE ID that is related to the incident\",\n",
    "                    \"cve_summary\": \"A brief summary of the CVE and its relation to the incident\",\n",
    "                    \"cve_relevance\": \"The estimated relevance level of the CVE match (0.0-1.0)\",\n",
    "                    \"cve_risk_level\": \"The risk level of the CVE on a scale of (0.0-1.0)\"\n",
    "                }\n",
    "            ],\n",
    "            \"incident_risk_level\": \"The risk level of the incident (0.0-1.0)\",\n",
    "            \"incident_risk_level_explanation\": \"An explanation of the rationale for the risk level assessment\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc84b08-f36c-45d2-a663-4126f511c75a",
   "metadata": {},
   "source": [
    "## 8.3 Retrieval-Augmented Prompt Enhancement\n",
    "\n",
    "Like we mentioned earlier in the notebook, we learned that we were better off injecting semantic search results to KEV, NVD, and historical incidents to the prompt itself.  This led to an effective RAG system as part of the agenic workflow.\n",
    "\n",
    "**Test it!**\n",
    "\n",
    "**Goal:** Demonstrate how `batch_get_historical_context` fetches structured past analyses for prompt injection.    \n",
    "**Why we do this:** Shows the exact payload the agent will receive for historical context, ensuring transparency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "42a6caee-8b87-43a7-9e1a-937c37e339a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08:09:40 INFO     [root] Searching for similar incidents with k=3, MMR=True\n",
      "08:09:40 INFO     [root] Found 3 similar incidents\n",
      "08:09:40 INFO     [root] search_similar_incidents completed in 0.46s\n",
      "08:09:40 INFO     [root] Retrieving analyses from database for 3 incidents...\n",
      "08:09:40 INFO     [root] Retrieving analyses for 3 incidents...\n",
      "08:09:40 INFO     [root] Successfully retrieved 0 analyses!\n",
      "08:09:40 DEBUG    [root] Closed database session\n",
      "08:09:40 INFO     [root] Retrieved 0 analyses\n",
      "08:09:40 INFO     [root] get_similar_incidents_with_analyses completed in 0.47s\n",
      "08:09:40 INFO     [root] Retrieving analyses for 3 incidents...\n",
      "08:09:40 INFO     [root] Successfully retrieved 0 analyses!\n",
      "08:09:40 DEBUG    [root] Closed database session\n",
      "08:09:40 INFO     [root] Retrieved 0 analyses\n",
      "08:09:40 INFO     [root] Searching for similar incidents with k=3, MMR=True\n",
      "08:09:40 INFO     [root] Found 3 similar incidents\n",
      "08:09:40 INFO     [root] search_similar_incidents completed in 0.33s\n",
      "08:09:40 INFO     [root] Retrieving analyses from database for 3 incidents...\n",
      "08:09:40 INFO     [root] Retrieving analyses for 3 incidents...\n",
      "08:09:40 INFO     [root] Successfully retrieved 0 analyses!\n",
      "08:09:40 DEBUG    [root] Closed database session\n",
      "08:09:40 INFO     [root] Retrieved 0 analyses\n",
      "08:09:40 INFO     [root] get_similar_incidents_with_analyses completed in 0.33s\n",
      "08:09:40 INFO     [root] Retrieving analyses for 3 incidents...\n",
      "08:09:40 INFO     [root] Successfully retrieved 0 analyses!\n",
      "08:09:40 DEBUG    [root] Closed database session\n",
      "08:09:40 INFO     [root] Retrieved 0 analyses\n",
      "08:09:40 INFO     [root] batch_get_historical_context completed in 0.82s\n",
      "Historical context for batch:\n",
      "{\n",
      "  \"success\": true,\n",
      "  \"results\": [\n",
      "    {\n",
      "      \"incident_id\": \"INC-2023-08-01-001\",\n",
      "      \"historical_context\": {\n",
      "        \"similar_incidents\": [\n",
      "          {\n",
      "            \"incident_id\": \"INC-2024-07-14-020\"\n",
      "          },\n",
      "          {\n",
      "            \"incident_id\": \"INC-2024-07-21-027\"\n",
      "          },\n",
      "          {\n",
      "            \"incident_id\": \"INC-2024-07-06-012\"\n",
      "          }\n",
      "        ],\n",
      "        \"analyses\": {},\n",
      "        \"metadata\": {\n",
      "          \"num_incidents_found\": 3,\n",
      "          \"num_analyses_retrieved\": 0,\n",
      "          \"search_params\": {\n",
      "            \"k\": 3,\n",
      "            \"use_mmr\": true,\n",
      "            \"lambda_mult\": 0.7\n",
      "          },\n",
      "          \"fields_retrieved\": {\n",
      "            \"incident_fields\": [\n",
      "              \"incident_id\",\n",
      "              \"similarity\"\n",
      "            ],\n",
      "            \"analysis_fields\": [\n",
      "              \"incident_risk_level\",\n",
      "              \"incident_summary\",\n",
      "              \"cve_ids\",\n",
      "              \"incident_risk_level_explanation\"\n",
      "            ]\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"incident_id\": \"INC-2023-08-01-002\",\n",
      "      \"historical_context\": {\n",
      "        \"similar_incidents\": [\n",
      "          {\n",
      "            \"incident_id\": \"INC-2024-07-14-020\"\n",
      "          },\n",
      "          {\n",
      "            \"incident_id\": \"INC-2024-07-06-012\"\n",
      "          },\n",
      "          {\n",
      "            \"incident_id\": \"INC-2024-07-13-019\"\n",
      "          }\n",
      "        ],\n",
      "        \"analyses\": {},\n",
      "        \"metadata\": {\n",
      "          \"num_incidents_found\": 3,\n",
      "          \"num_analyses_retrieved\": 0,\n",
      "          \"search_params\": {\n",
      "            \"k\": 3,\n",
      "            \"use_mmr\": true,\n",
      "            \"lambda_mult\": 0.7\n",
      "          },\n",
      "          \"fields_retrieved\": {\n",
      "            \"incident_fields\": [\n",
      "              \"incident_id\",\n",
      "              \"similarity\"\n",
      "            ],\n",
      "            \"analysis_fields\": [\n",
      "              \"incident_risk_level\",\n",
      "              \"incident_summary\",\n",
      "              \"cve_ids\",\n",
      "              \"incident_risk_level_explanation\"\n",
      "            ]\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"metadata\": {\n",
      "    \"incidents_processed\": 2,\n",
      "    \"top_k\": 3\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from utils.retrieval_utils import batch_get_historical_context\n",
    "\n",
    "batch_ids = [incidents[0]['incident_id'], incidents[1]['incident_id']]\n",
    "historical_context = batch_get_historical_context(batch_ids)\n",
    "\n",
    "print(\"Historical context for batch:\")\n",
    "import json\n",
    "print(json.dumps(historical_context, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb52a301",
   "metadata": {},
   "source": [
    "## 8.3 Pydantic Output Parsing\n",
    "\n",
    "We use Pydantic models to define the structure of the agent's output:\n",
    "\n",
    "**Why we do this:** Structured outputs ensure:\n",
    "- Consistency in the format of analyses\n",
    "- Validation of required fields\n",
    "- Clear typing for downstream processing\n",
    "- Enforced schema compliance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c671f317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pydantic models from utils/prompt_utils.py\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "\n",
    "class CVEInfo(BaseModel):\n",
    "    \"\"\"\n",
    "    A Pydantic model for CVE information.\n",
    "    This model defines the structure of the output from the CVE analysis.\n",
    "    It includes fields for the CVE ID, summary, relevance, and risk level.\n",
    "    \"\"\"\n",
    "    cve_id: str = Field(description=\"The CVE ID that is related to the incident\")\n",
    "    cve_summary: str = Field(description=\"A brief summary of the CVE and its relation to the incident\")\n",
    "    cve_relevance: float = Field(description=\"The estimated relevance level of the CVE match (0.0-1.0)\")\n",
    "    cve_risk_level: float = Field(description=\"The risk level of the CVE on a scale of (0.0-1.0)\")\n",
    "\n",
    "class IncidentAnalysis(BaseModel):\n",
    "    \"\"\"\n",
    "    A Pydantic model for incident analysis.\n",
    "    This model defines the structure of the output from the incident analysis.\n",
    "    It includes fields for the incident ID, summary, list of related CVEs, and the risk level of the incident.\n",
    "    \"\"\"\n",
    "    incident_id: str = Field(description=\"The ID of the incident that caused the error\")\n",
    "    incident_summary: str = Field(description=\"A brief summary of the incident\")\n",
    "    cve_ids: list[CVEInfo] = Field(description=\"List of related CVEs and their details\")\n",
    "    incident_risk_level: float = Field(description=\"The risk level of the incident (0.0-1.0)\")\n",
    "    incident_risk_level_explanation: str = Field(description=\"An explanation of the rationale for the risk level assessment\")\n",
    "\n",
    "class IncidentAnalysisList(BaseModel):\n",
    "    incidents: list[IncidentAnalysis] = Field(description=\"List of incident analyses\")\n",
    "\n",
    "# Initialize the parser\n",
    "parser = PydanticOutputParser(pydantic_object=IncidentAnalysisList)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f827639-cb21-453d-8519-a6e09eab81c3",
   "metadata": {},
   "source": [
    "**Test it!**\n",
    "\n",
    "This leads us to a generalized `generate_prompt` function that alows for growth and modularity over time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "233acfeb-15ec-4fac-9a81-4fca413f0bf4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08:09:40 INFO     [root] Matching incident against KEV/NVD databases. KEV k=3, NVD k=3\n",
      "08:09:40 DEBUG    [root] Incident details: {'incident_id': 'INC-2023-08-13-039', 'timestamp': '2023-08-13T14:30:00Z', 'title': 'Open Redirect Vulnerability Exploited', 'description': 'Security logs show users being redirected to external, potentially malicious websites via a trusted internal URL.', 'affected_assets': [{'hostname': 'url-shortener-service', 'ip_address': '192.168.1.160', 'os': 'Ubuntu 20.04 LTS', 'installed_software': [{'name': 'Node.js', 'version': '14.17.0'}], 'role': 'Internal Service'}], 'observed_ttps': [{'framework': 'MITRE ATT&CK', 'id': 'T0864', 'name': 'CLIENT-SIDE ATTACK: Open Redirect'}], 'indicators_of_compromise': [{'type': 'http_request_parameter', 'value': 'redirect_url=http://malicious.site', 'context': 'Observed request parameter'}, {'type': 'user_behavior', 'value': 'clicked link leading to external site', 'context': 'Observed user action'}], 'initial_findings': 'Open redirect vulnerability exploited in URL shortener service.'}\n",
      "08:09:40 DEBUG    [root] Flattening incident text for KEV search\n",
      "08:09:40 DEBUG    [root] Searching KEV index with k=3, MMR=True\n",
      "08:09:41 INFO     [root] _search completed in 0.48s\n",
      "08:09:41 INFO     [root] Found 3 KEV matches\n",
      "08:09:41 INFO     [root] semantic_match_incident_kev completed in 0.49s\n",
      "08:09:41 INFO     [root] Lowest KEV variance: 1.909\n",
      "08:09:41 INFO     [root] KEV score above threshold, searching NVD database\n",
      "08:09:41 DEBUG    [root] Flattening incident text for NVD search\n",
      "08:09:41 DEBUG    [root] Searching NVD index with k=3, MMR=True\n",
      "08:09:41 INFO     [root] _search completed in 0.30s\n",
      "08:09:41 INFO     [root] Found 3 NVD matches\n",
      "08:09:41 INFO     [root] semantic_match_incident_nvd completed in 0.31s\n",
      "08:09:41 INFO     [root] semantic_match_incident completed in 0.80s\n",
      "08:09:41 INFO     [root] match_incident_to_cves completed in 0.80s\n",
      "08:09:41 INFO     [root] Matching incident against KEV/NVD databases. KEV k=3, NVD k=3\n",
      "08:09:41 DEBUG    [root] Incident details: {'incident_id': 'INC-2023-08-13-038', 'timestamp': '2023-08-13T11:00:00Z', 'title': 'Subdomain Takeover Attempt', 'description': 'Threat intelligence alert indicates a dangling DNS record pointing to a service that is no longer active, potentially allowing subdomain takeover.', 'affected_assets': [{'hostname': 'old-blog.example.com', 'ip_address': 'N/A', 'os': 'N/A', 'installed_software': [], 'role': 'Legacy DNS Entry'}], 'observed_ttps': [{'framework': 'MITRE ATT&CK', 'id': 'T1584', 'name': 'Compromise Infrastructure'}, {'framework': 'MITRE ATT&CK', 'id': 'T1584.001', 'name': 'Compromise Infrastructure: DNS'}], 'indicators_of_compromise': [{'type': 'dns_record', 'value': 'CNAME old-blog.example.com -> inactive-service.cloudprovider.com', 'context': 'Observed DNS record'}, {'type': 'threat_intel_alert', 'value': 'Dangling DNS record detected', 'context': 'Threat intel alert'}], 'initial_findings': 'Potential subdomain takeover risk due to dangling DNS record.'}\n",
      "08:09:41 DEBUG    [root] Flattening incident text for KEV search\n",
      "08:09:41 DEBUG    [root] Searching KEV index with k=3, MMR=True\n",
      "08:09:42 INFO     [root] _search completed in 0.45s\n",
      "08:09:42 INFO     [root] Found 3 KEV matches\n",
      "08:09:42 INFO     [root] semantic_match_incident_kev completed in 0.45s\n",
      "08:09:42 INFO     [root] Lowest KEV variance: 1.930\n",
      "08:09:42 INFO     [root] KEV score above threshold, searching NVD database\n",
      "08:09:42 DEBUG    [root] Flattening incident text for NVD search\n",
      "08:09:42 DEBUG    [root] Searching NVD index with k=3, MMR=True\n",
      "08:09:42 INFO     [root] _search completed in 0.77s\n",
      "08:09:42 INFO     [root] Found 3 NVD matches\n",
      "08:09:42 INFO     [root] semantic_match_incident_nvd completed in 0.77s\n",
      "08:09:42 INFO     [root] semantic_match_incident completed in 1.23s\n",
      "08:09:42 INFO     [root] match_incident_to_cves completed in 1.23s\n",
      "08:09:42 INFO     [root] batch_match_incident_to_cves completed in 2.03s\n",
      "08:09:42 INFO     [root] Searching for similar incidents with k=2, MMR=True\n",
      "08:09:43 INFO     [root] Found 2 similar incidents\n",
      "08:09:43 INFO     [root] search_similar_incidents completed in 0.95s\n",
      "08:09:43 INFO     [root] Retrieving analyses from database for 2 incidents...\n",
      "08:09:43 INFO     [root] Retrieving analyses for 2 incidents...\n",
      "08:09:43 INFO     [root] Successfully retrieved 0 analyses!\n",
      "08:09:43 DEBUG    [root] Closed database session\n",
      "08:09:43 INFO     [root] Retrieved 0 analyses\n",
      "08:09:43 INFO     [root] get_similar_incidents_with_analyses completed in 0.95s\n",
      "08:09:43 INFO     [root] Retrieving analyses for 2 incidents...\n",
      "08:09:43 INFO     [root] Successfully retrieved 0 analyses!\n",
      "08:09:43 DEBUG    [root] Closed database session\n",
      "08:09:43 INFO     [root] Retrieved 0 analyses\n",
      "08:09:43 INFO     [root] Searching for similar incidents with k=2, MMR=True\n",
      "08:09:44 INFO     [root] Found 2 similar incidents\n",
      "08:09:44 INFO     [root] search_similar_incidents completed in 0.16s\n",
      "08:09:44 INFO     [root] Retrieving analyses from database for 2 incidents...\n",
      "08:09:44 INFO     [root] Retrieving analyses for 2 incidents...\n",
      "08:09:44 INFO     [root] Successfully retrieved 0 analyses!\n",
      "08:09:44 DEBUG    [root] Closed database session\n",
      "08:09:44 INFO     [root] Retrieved 0 analyses\n",
      "08:09:44 INFO     [root] get_similar_incidents_with_analyses completed in 0.16s\n",
      "08:09:44 INFO     [root] Retrieving analyses for 2 incidents...\n",
      "08:09:44 INFO     [root] Successfully retrieved 0 analyses!\n",
      "08:09:44 DEBUG    [root] Closed database session\n",
      "08:09:44 INFO     [root] Retrieved 0 analyses\n",
      "08:09:44 INFO     [root] batch_get_historical_context completed in 1.13s\n",
      "System message (click the output to scroll through the results):\n",
      "\n",
      "You are a CVE‐analysis assistant. Analyze the following incidents and provide structured analysis.\n",
      "\n",
      "Incident Details:\n",
      "[\n",
      "  {\n",
      "    \"incident_id\": \"INC-2023-08-13-039\",\n",
      "    \"timestamp\": \"2023-08-13T14:30:00Z\",\n",
      "    \"title\": \"Open Redirect Vulnerability Exploited\",\n",
      "    \"description\": \"Security logs show users being redirected to external, potentially malicious websites via a trusted internal URL.\",\n",
      "    \"affected_assets\": [\n",
      "      {\n",
      "        \"hostname\": \"url-shortener-service\",\n",
      "        \"ip_address\": \"19\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Human message preview (this will match the prompt sent just above this in the generate_prompt call):\n",
      "Analyze these incidents and output JSON per Pydantic schema.\n"
     ]
    }
   ],
   "source": [
    "from utils.prompt_utils import generate_prompt, parser\n",
    "from utils.retrieval_utils import batch_match_incident_to_cves, batch_get_historical_context\n",
    "\n",
    "# Prepare batch FAISS and historical results for a sample\n",
    "batch_results = batch_match_incident_to_cves(start_index=0, batch_size=2, top_k=3)\n",
    "historical_results = batch_get_historical_context(incident_ids=[res['incident_id'] for res in batch_results['results']], top_k=2)\n",
    "\n",
    "# Generate the prompt messages\n",
    "prompt_messages = generate_prompt(\n",
    "    query=\"Analyze these incidents and output JSON per Pydantic schema.\",\n",
    "    batch_faiss_results=batch_results,\n",
    "    historical_faiss_results=historical_results\n",
    ")\n",
    "\n",
    "# Show the system and human messages\n",
    "print(\"System message (click the output to scroll through the results):\")\n",
    "print(prompt_messages[0].content[:500])\n",
    "\n",
    "print(f\"\\n{'-'*50}\\n\")\n",
    "\n",
    "print(\"Human message preview (this will match the prompt sent just above this in the generate_prompt call):\")\n",
    "print(prompt_messages[1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f967e89",
   "metadata": {},
   "source": [
    "## 8.4 LangChain ReAct Agent\n",
    "\n",
    "We use LangChain's ReAct agent pattern to orchestrate the analysis process:\n",
    "\n",
    "**Why we do this:** The ReAct agent pattern combines:\n",
    "- **Re**asoning: Understanding the task and formulating a plan\n",
    "- **Act**ion: Using tools to gather information\n",
    "- Observation: Processing the results of tool calls\n",
    "- Generation: Producing a final analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3b2bd05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent setup from main_security_agent_server.py\n",
    "import asyncio\n",
    "from langchain_mcp_adapters.tools import load_mcp_tools\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from mcp import ClientSession, StdioServerParameters, stdio_client\n",
    "\n",
    "from utils.prompt_utils import generate_prompt\n",
    "from utils.retrieval_utils import batch_get_historical_context, batch_match_incident_to_cves\n",
    "\n",
    "# Setup server parameters and model\n",
    "server_parameters = StdioServerParameters(\n",
    "    command=\"python\",\n",
    "    args=[\"mcp_cve_server.py\"],\n",
    ")\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\", openai_api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "async def run_agent(query, start_index, batch_size):\n",
    "    async with stdio_client(server_parameters) as (read, write):\n",
    "        # Initialize client session\n",
    "        async with ClientSession(read, write) as session:\n",
    "            await session.initialize()\n",
    "            \n",
    "            # Load MCP tools and create ReAct agent\n",
    "            tools = await load_mcp_tools(session)\n",
    "            agent = create_react_agent(model, tools, name=\"CVE_Agent\")\n",
    "            \n",
    "            # Prepare incident batch and historical context\n",
    "            batch_faiss_results = batch_match_incident_to_cves(\n",
    "                batch_size=batch_size,\n",
    "                start_index=start_index,\n",
    "                top_k=3\n",
    "            )\n",
    "            \n",
    "            historical_results = batch_get_historical_context(\n",
    "                incident_ids=[r[\"incident_id\"] for r in batch_faiss_results[\"results\"]],\n",
    "                top_k=2\n",
    "            )\n",
    "            \n",
    "            # Generate prompt with all context\n",
    "            prompt_messages = generate_prompt(\n",
    "                query=query,\n",
    "                batch_faiss_results=batch_faiss_results,\n",
    "                historical_faiss_results=historical_results\n",
    "            )\n",
    "            \n",
    "            # Execute agent\n",
    "            final_msg, full_response = await agent.ainvoke({\"messages\": prompt_messages})\n",
    "            \n",
    "            # Parse and validate results\n",
    "            analysis = parser.parse(final_msg.content)\n",
    "            \n",
    "            return analysis, full_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487f3172",
   "metadata": {},
   "source": [
    "## 8.5 Running the Agent\n",
    "\n",
    "Let's see the basic logic of how the agent is called.\n",
    "\n",
    "**Note:** The asynchronous nature of the functions do not \"play nicely\" with Jupyter, so ensure you follow the README guide on running the main server and then `run_analysis.py`, but below you can see a general outline of the structure expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0e5dbb68",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception running in Jupyter Notebook\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import asyncio\n",
    "from utils.prompt_utils import AnalysisRequest\n",
    "\n",
    "# Create a request to analyze a batch of incidents\n",
    "async def analyze_incidents():\n",
    "    request = AnalysisRequest(\n",
    "        start_index=0,\n",
    "        batch_size=2,\n",
    "        request_id=\"demo-123\",\n",
    "        openai_api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "        model_name=\"gpt-4o-mini\"\n",
    "    )\n",
    "    \n",
    "    # Run the analysis\n",
    "    analysis, response = await run_agent(\n",
    "        query=query,\n",
    "        start_index=request.start_index,\n",
    "        batch_size=request.batch_size\n",
    "    )\n",
    "    \n",
    "    # Display the results\n",
    "    print(\"Analysis Results:\")\n",
    "    for incident in analysis.incidents:\n",
    "        print(f\"\\nIncident: {incident.incident_id}\")\n",
    "        print(f\"Summary: {incident.incident_summary}\")\n",
    "        print(f\"Risk Level: {incident.incident_risk_level}\")\n",
    "        print(f\"Explanation: {incident.incident_risk_level_explanation}\")\n",
    "        print(\"\\nRelevant CVEs:\")\n",
    "        for cve in incident.cve_ids:\n",
    "            print(f\"  - {cve.cve_id} (Relevance: {cve.cve_relevance}, Risk: {cve.cve_risk_level})\")\n",
    "            print(f\"    {cve.cve_summary}\")\n",
    "    \n",
    "    # Display usage metrics\n",
    "    print(\"\\nUsage Metrics:\")\n",
    "    print(f\"Input tokens: {response['usage_metadata']['input_tokens']}\")\n",
    "    print(f\"Output tokens: {response['usage_metadata']['output_tokens']}\")\n",
    "    print(f\"Total tokens: {response['usage_metadata']['total_tokens']}\")\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "# Run the analysis (This will fail in Jupyter due to its own event loop, but you ccan use this in python directly if you want to explore)\n",
    "try: \n",
    "    analysis = await asyncio.run(await analyze_incidents()) \n",
    "except Exception: \n",
    "    print(\"Exception running in Jupyter Notebook\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58feeda3",
   "metadata": {},
   "source": [
    "# 9. Persistence and Data Management\n",
    "\n",
    "This section covers how the system persists analysis results and manages data for continuous learning and reference.\n",
    "\n",
    "## 9.1 SQLite Database\n",
    "\n",
    "Our system uses SQLite for structured persistence of incident analyses. This provides a lightweight, file-based database that requires no external server.\n",
    "\n",
    "**Why we do this:** Persistent storage enables:\n",
    "- Historical reference of past analyses\n",
    "- Audit trails for security review\n",
    "- Query capabilities for reporting and dashboards\n",
    "- Cross-referencing between incidents\n",
    "- Continuous learning for the system (analyses are queried and retrieved for similar incidents as part of the prompt injection)\n",
    "- \"Fire-and-forget\" functionality to execute batches asychronously without waiting for a response\n",
    "\n",
    "Below you can see the basic way we create two tables:\n",
    "1. To keep track of incidents and resulting analyses\n",
    "2. To keep track of run metrics (tokens, latency, tool usage, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f2ea7dd1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dan Guilliams\\AppData\\Local\\Temp\\ipykernel_48604\\2751999015.py:15: MovedIn20Warning: The ``declarative_base()`` function is now available as sqlalchemy.orm.declarative_base(). (deprecated since: 2.0) (Background on SQLAlchemy 2.0 at: https://sqlalche.me/e/b8d9)\n",
      "  Base = declarative_base()\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import Integer, create_engine, Column, String, Float, Text, DateTime, Index\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from datetime import datetime, UTC\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Swap with postgres:// URI as needed \n",
    "# --- Configuration ---\n",
    "DATABASE_URL = os.getenv(\"DATABASE_URL\", \"sqlite:///data/incident_analysis.db\") \n",
    "\n",
    "# --- Setup SQLAlchemy ---\n",
    "engine = create_engine(DATABASE_URL)\n",
    "SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)\n",
    "Base = declarative_base()\n",
    "\n",
    "# --- Model Definition ---\n",
    "class IncidentRecord(Base):\n",
    "    __tablename__ = \"incident_analysis\"\n",
    "\n",
    "    id = Column(Integer, primary_key=True, autoincrement=True)\n",
    "    request_id = Column(String, index=True)  # Uniquely identifies this *analysis*, not just incident\n",
    "    incident_id = Column(String, index=True)       # Allows filtering or joining across repeated incidents\n",
    "    created_at = Column(DateTime, default=lambda: datetime.now(UTC))\n",
    "    incident_raw_json = Column(Text)               # Original incident details\n",
    "    llm_analysis_json = Column(Text)               # Final analysis from the LLM\n",
    "    llm_risk_score = Column(Float, nullable=True)  # For quick filtering/analytics\n",
    "    model_name = Column(String)\n",
    "\n",
    "    __table_args__ = (\n",
    "        Index(\"ix_incident_id_request\", \"incident_id\", \"request_id\"),  # Fast filtering if needed\n",
    "    )\n",
    "\n",
    "# --- Create Tables If Not Exists ---\n",
    "def init_db():\n",
    "    Base.metadata.create_all(bind=engine)\n",
    "\n",
    "class RunMetadata(Base):\n",
    "    __tablename__ = \"run_metadata\"\n",
    "    id               = Column(Integer, primary_key=True, autoincrement=True)\n",
    "    request_id       = Column(String, index=True, nullable=False)\n",
    "    start_index      = Column(Integer, nullable=False)\n",
    "    batch_size       = Column(Integer, nullable=False)\n",
    "    input_tokens     = Column(Integer, nullable=True)\n",
    "    output_tokens    = Column(Integer, nullable=True)\n",
    "    total_tokens     = Column(Integer, nullable=True)\n",
    "    tools_called     = Column(Text, nullable=True)    # JSON-encoded list of tool names\n",
    "    duration_seconds = Column(Float, nullable=True)\n",
    "    error_count      = Column(Integer, default=0)\n",
    "    created_at       = Column(DateTime, default=lambda: datetime.now(UTC))\n",
    "\n",
    "def init_db():\n",
    "    Base.metadata.create_all(bind=engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1f8adb-95fa-47a9-9107-f90b594cbc61",
   "metadata": {},
   "source": [
    "Using the `dev/incident_dashboard.py` for a lightweight Streamlit UI gives us a better idea of what's being captured.\n",
    "\n",
    "Here is a glimpse of the more straight-forward columns in our `inident_analysis` table:\n",
    "\n",
    "![Run Incident Analysis Summary Example](Documentation/images/incident_analysis_streamlit_preview_screenshot.png)\n",
    "\n",
    "Note that they are by batches, these capture the asynchronous calls with variable number of `incident_id` values to request per batch, associated with their unique `request_id`\n",
    "\n",
    "The dashboard gives a clean look at the more detailed JSON entries for the incident and resulting agent analysis as well:\n",
    "\n",
    "![Run Incident Analysis Detail Example](Documentation/images/incident_analysis_streamlit_details_screenshot.png)\n",
    "\n",
    "We can access this by running\n",
    "```bash\n",
    "streamlit run dev/incident_dashboard.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8875a2d9",
   "metadata": {},
   "source": [
    "\n",
    "## 9.2 Saving Analysis Results\n",
    "\n",
    "When the agent completes an analysis, we save the results in both SQLite and as JSON backups:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "43f7e06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils/datastore_utils.py\n",
    "def save_incident_and_analysis_to_sqlite_db(\n",
    "    request_id: str,\n",
    "    incident_id: str,\n",
    "    model_name: str,\n",
    "    incident: dict,\n",
    "    analysis: dict\n",
    "):\n",
    "    \"\"\"\n",
    "    Save both the incident and its analysis to the SQLite database.\n",
    "    This is the primary storage location for incident analyses.\n",
    "    \n",
    "    Args:\n",
    "        request_id (str): Unique identifier for this analysis request\n",
    "        incident_id (str): ID of the incident being analyzed\n",
    "        model_name (str): Name of the LLM model used for analysis\n",
    "        incident (dict): The original incident data\n",
    "        analysis (dict): The LLM's analysis of the incident\n",
    "    \"\"\"\n",
    "    session = SessionLocal()\n",
    "    try:\n",
    "        record = IncidentRecord(\n",
    "            request_id=request_id,\n",
    "            incident_id=incident_id,\n",
    "            incident_raw_json=json.dumps(incident),\n",
    "            llm_analysis_json=json.dumps(analysis),\n",
    "            llm_risk_score=analysis.get(\"incident_risk_level\", None),\n",
    "            model_name=model_name\n",
    "        )\n",
    "        session.add(record)\n",
    "        session.commit()\n",
    "    except Exception as e:\n",
    "        session.rollback()\n",
    "        raise e\n",
    "    finally:\n",
    "        session.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1456b0",
   "metadata": {},
   "source": [
    "In addition to SQLite, we also save JSON backups locally (these are very small and lightweight files, but under large volume this can and would be abandoned or toggled if need-be):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "09dd0402",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_incident_analysis_backup_json(incident_id, analysis_data):\n",
    "    \"\"\"Save a backup of analysis data as JSON.\"\"\"\n",
    "    backup_dir = DATA_DIR / \"backups\"\n",
    "    backup_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    backup_path = backup_dir / f\"analysis_{incident_id}_{timestamp}.json\"\n",
    "    \n",
    "    with open(backup_path, 'w') as f:\n",
    "        json.dump(analysis_data, f, indent=2)\n",
    "    \n",
    "    return backup_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5f4d57",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 9.3 FAISS Vector Index Updates\n",
    "\n",
    "To support continuous learning, we update the INCIDENT_HISTORY_FAISS index with new incidents that have been analyzed:\n",
    "\n",
    "**Why we do this:** Updating vector indexes enables:\n",
    "- The system to learn from new analyses (pulled from the DB for similar incidents during prompt injection)\n",
    "- Improved results over time as more examples are added\n",
    "- Reference to previous analyses when encountering similar incidents\n",
    "- Consistency in risk evaluation by referring to precedents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dec36aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from utils.flatteners import flatten_incident\n",
    "from utils.retrieval_utils import get_incident\n",
    "\n",
    "def add_incident_to_faiss_history_index(incident_id, analysis):\n",
    "    \"\"\"\n",
    "    Add a completed incident analysis to the historical FAISS index.\n",
    "    \n",
    "    Args:\n",
    "        incident_id: The ID of the analyzed incident\n",
    "        analysis: The analysis object from the agent\n",
    "    \"\"\"\n",
    "    global INCIDENT_HISTORY_FAISS, embeddings\n",
    "    \n",
    "    if INCIDENT_HISTORY_FAISS is None or embeddings is None:\n",
    "        initialize_openai_embeddings()\n",
    "        initialize_faiss_indexes()\n",
    "    \n",
    "    # Create a document from the incident\n",
    "    flattened_text = flatten_incident(get_incident(incident_id))\n",
    "    doc = Document(\n",
    "        page_content=flattened_text,\n",
    "        metadata={\n",
    "            \"incident_id\": incident_id,\n",
    "            \"analysis_id\": analysis.get(\"analysis_id\", \"unknown\"),\n",
    "            \"created_at\": datetime.now().isoformat()\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Add to FAISS index\n",
    "    INCIDENT_HISTORY_FAISS.add_documents([doc])\n",
    "    \n",
    "    # Save updated index\n",
    "    index_path = DATA_DIR / \"vectorstore\" / \"incident_analysis_history\"\n",
    "    INCIDENT_HISTORY_FAISS.save_local(index_path)\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d4841a-b5f6-4a15-8868-1ce010f895e9",
   "metadata": {},
   "source": [
    "**Test It!**\n",
    "\n",
    "Below, we can test whether the setup is working by adding an incident and then searching for similar incidents to see if identical entries are returned as expected (given that we are entering these multiple times in the demo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e0a45731-4f5e-410a-a111-627d060ad659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding incident INC-9874-21-51-015f to historical index...\n",
      "08:09:46 INFO     [root] add_incident_to_faiss_history_index completed in 2.27s\n",
      "08:09:46 INFO     [root] Searching for similar incidents with k=5, MMR=True\n",
      "08:09:47 INFO     [root] Found 5 similar incidents\n",
      "08:09:47 INFO     [root] search_similar_incidents completed in 0.21s\n",
      "Same incident: True\n",
      "--------------------------------------------------\n",
      "Example Incident:\n",
      "\tincident_id: INC-9874-21-51-015f\n",
      "\tincident_summary: Subdomain Takeover Attempt\n",
      "\n",
      "Similar incidents: [\n",
      "  {\n",
      "    \"incident_id\": \"INC-9874-21-51-015f\",\n",
      "    \"analysis\": {\n",
      "      \"incident_id\": \"INC-9874-21-51-015f\",\n",
      "      \"incident_summary\": \"Subdomain Takeover Attempt\",\n",
      "      \"cve_ids\": [\n",
      "        {\n",
      "          \"cve_id\": \"CVE-2023-41265\",\n",
      "          \"cve_summary\": \"HTTP Tunneling Vulnerability in Qlik Sense which could be exploited if a subdomain is compromised.\",\n",
      "          \"cve_relevance\": 1.93,\n",
      "          \"cve_risk_level\": 0.8\n",
      "        }\n",
      "      ],\n",
      "      \"incident_risk_level\": 0.75,\n",
      "      \"incident_risk_level_explanation\": \"Dangling DNS records pose a critical risk for subdomain takeover. The related CVE suggests a known vulnerability in HTTP tunneling that could be leveraged in this context.\"\n",
      "    },\n",
      "    \"variance\": 0.2766869366168976\n",
      "  },\n",
      "  {\n",
      "    \"incident_id\": \"INC-2024-07-14-020\",\n",
      "    \"title\": \"Network Segmentation Bypass\",\n",
      "    \"timestamp\": \"2024-07-14T15:55:23Z\",\n",
      "    \"variance\": 1.894463300704956\n",
      "  },\n",
      "  {\n",
      "    \"incident_id\": \"INC-2024-01-15-001\",\n",
      "    \"title\": \"Phishing Campaign Targeting Financial Services\",\n",
      "    \"timestamp\": \"2024-01-15T09:23:41Z\",\n",
      "    \"variance\": 1.9693876504898071\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from time import sleep\n",
    "from utils.retrieval_utils import add_incident_to_faiss_history_index, INCIDENT_HISTORY_FAISS, search_similar_incidents\n",
    "\n",
    "# We will pick an example incident entry and example analysis (one actually generated by the agent in am previous run)\n",
    "# However, we will change the incident_id for each to ensure it's actually encoded into the index\n",
    "random_incident_id = f\"INC-{random.randint(1000,9999)}-{random.randint(10,99)}-{random.randint(10,99)}-0{random.randint(10,99)}f\"\n",
    "\n",
    "example_incident = {\n",
    "    \"incident_id\": f\"{random_incident_id}\",\n",
    "    \"timestamp\": \"2023-08-13T11:00:00Z\",\n",
    "    \"title\": \"Subdomain Takeover Attempt\",\n",
    "    \"description\": \"Threat intelligence alert indicates a dangling DNS record pointing to a service that is no longer active, potentially allowing subdomain takeover.\",\n",
    "    \"affected_assets\": [\n",
    "      {\n",
    "        \"hostname\": \"old-blog.example.com\",\n",
    "        \"ip_address\": \"N/A\",\n",
    "        \"os\": \"N/A\",\n",
    "        \"installed_software\": [],\n",
    "        \"role\": \"Legacy DNS Entry\"\n",
    "      }\n",
    "    ],\n",
    "    \"observed_ttps\": [\n",
    "      {\n",
    "        \"framework\": \"MITRE ATT&CK\",\n",
    "        \"id\": \"T1584\",\n",
    "        \"name\": \"Compromise Infrastructure\"\n",
    "      },\n",
    "      {\n",
    "        \"framework\": \"MITRE ATT&CK\",\n",
    "        \"id\": \"T1584.001\",\n",
    "        \"name\": \"Compromise Infrastructure: DNS\"\n",
    "      }\n",
    "    ],\n",
    "    \"indicators_of_compromise\": [\n",
    "      {\n",
    "        \"type\": \"dns_record\",\n",
    "        \"value\": \"CNAME old-blog.example.com -> inactive-service.cloudprovider.com\",\n",
    "        \"context\": \"Observed DNS record\"\n",
    "      },\n",
    "      {\n",
    "        \"type\": \"threat_intel_alert\",\n",
    "        \"value\": \"Dangling DNS record detected\",\n",
    "        \"context\": \"Threat intel alert\"\n",
    "      }\n",
    "    ],\n",
    "    \"initial_findings\": \"Potential subdomain takeover risk due to dangling DNS record.\"\n",
    "  }\n",
    "\n",
    "example_analysis = {\n",
    "    \"incident_id\": f\"{random_incident_id}\",\n",
    "    \"incident_summary\": \"Subdomain Takeover Attempt\",\n",
    "    \"cve_ids\": [\n",
    "      {\n",
    "        \"cve_id\": \"CVE-2023-41265\",\n",
    "        \"cve_summary\": \"HTTP Tunneling Vulnerability in Qlik Sense which could be exploited if a subdomain is compromised.\",\n",
    "        \"cve_relevance\": 1.93,\n",
    "        \"cve_risk_level\": 0.8\n",
    "      }\n",
    "    ],\n",
    "    \"incident_risk_level\": 0.75,\n",
    "    \"incident_risk_level_explanation\": \"Dangling DNS records pose a critical risk for subdomain takeover. The related CVE suggests a known vulnerability in HTTP tunneling that could be leveraged in this context.\"\n",
    "  }\n",
    "\n",
    "print(f\"Adding incident {example_incident['incident_id']} to historical index...\")\n",
    "await add_incident_to_faiss_history_index(example_incident, example_analysis)\n",
    "\n",
    "# We now expect to see entries for this same incident that we just stored to be returned in the search results\n",
    "similar_incidents = search_similar_incidents(example_incident)\n",
    "\n",
    "# The incident_ids will likely differ due to running this script, but we can check a very specific field such as the incident_risk_level_explanation to ensure valid results\n",
    "print(f\"Same incident: {example_analysis['incident_risk_level_explanation'] == similar_incidents[0]['analysis']['incident_risk_level_explanation']}\")\n",
    "print(f\"{'-'*50}\")\n",
    "# Print the results \n",
    "print(f\"Example Incident:\\n\\tincident_id: {example_incident['incident_id']}\\n\\tincident_summary: {example_analysis['incident_summary']}\\n\\nSimilar incidents: {json.dumps(similar_incidents[:3], indent=2)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16fdec0",
   "metadata": {},
   "source": [
    "## 9.4 Usage Metadata Tracking\n",
    "\n",
    "We track usage metadata to monitor performance and costs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "74472138",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_run_metadata(\n",
    "    request_id: str,\n",
    "    start_index: int,\n",
    "    batch_size: int,\n",
    "    usage_metrics: dict, # Extracted from the agent's response object\n",
    "    tools: list[str], # Extracted from the agent's response object\n",
    "    duration: float,\n",
    "    error_count: int = 0\n",
    "):\n",
    "    session = SessionLocal()\n",
    "    try:\n",
    "        rm = RunMetadata(\n",
    "            request_id=request_id, # Contains the batch, shows performance of a single call\n",
    "            start_index=start_index,\n",
    "            batch_size=batch_size,\n",
    "            input_tokens=usage_metrics.get(\"input_tokens\"),\n",
    "            output_tokens=usage_metrics.get(\"output_tokens\"),\n",
    "            total_tokens=usage_metrics.get(\"total_tokens\"),\n",
    "            tools_called=json.dumps(tools),\n",
    "            duration_seconds=duration,\n",
    "            error_count=error_count\n",
    "        )\n",
    "        session.add(rm)\n",
    "        session.commit()\n",
    "    finally:\n",
    "        session.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "55affcf9-01b9-4dfb-94e6-39ccf386ba23",
   "metadata": {},
   "source": [
    "Example metadata logs:\n",
    "![Run Metadata Example](Documentation/images/run_metadata_example_screenshot.png)\n",
    "\n",
    "Note that they are by batches, these capture the asynchronous calls with variable number of `incident_id` values to request per batch, associated with their unique `request_id`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6126ab7d",
   "metadata": {},
   "source": [
    "## 9.5 Caching Strategy\n",
    "\n",
    "To optimize performance and have a stand-in for later scaling to reduce costs, we implement a simple caching strategy:\n",
    "\n",
    "**Why we do this:** Effective caching:\n",
    "- Reduces redundant computation\n",
    "- Improves response times\n",
    "- Ensures consistent responses for identical queries\n",
    "- Optimizes resource usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c22f6e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "from typing import Callable\n",
    "\n",
    "# Simple caching decorator from utils/decorators.py\n",
    "def cache_result(ttl_seconds: int = 300) -> Callable:\n",
    "    \"\"\"Simple in-memory cache with TTL for function results.\n",
    "\n",
    "    Args:\n",
    "        ttl_seconds: Time to live for cached results in seconds (default: 300)\n",
    "\n",
    "    Returns:\n",
    "        Decorator function that implements caching\n",
    "    \"\"\"\n",
    "    cache: Dict[str, Dict[str, Any]] = {}\n",
    "\n",
    "    def decorator(func: Callable) -> Callable:\n",
    "        @functools.wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            # Create cache key from function name and arguments\n",
    "            key = f\"{func.__name__}:{str(args)}:{str(kwargs)}\"\n",
    "\n",
    "            # Check if cached and not expired\n",
    "            if key in cache:\n",
    "                result = cache[key]\n",
    "                if time.time() - result['timestamp'] < ttl_seconds:\n",
    "                    # logger.debug(f\"Cache hit for {func.__name__}\")\n",
    "                    return result['data']\n",
    "\n",
    "            # Execute function and cache result\n",
    "            result = func(*args, **kwargs)\n",
    "            cache[key] = {\n",
    "                'data': result,\n",
    "                'timestamp': time.time()\n",
    "            }\n",
    "            return result\n",
    "        return wrapper\n",
    "    return decorator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14cf0cb6-60f6-4e8c-8288-954df4ada5c2",
   "metadata": {},
   "source": [
    "## 9.6 Backup and Recovery\n",
    "\n",
    "To ensure data durability, we would propose implement backup and recovery procedures:\n",
    "\n",
    "- Daily database backups\n",
    "- Vector index snapshots\n",
    "- Redundant storage for JSON backups\n",
    "- Point-in-time recovery capability\n",
    "- Automated recovery testing (with allotted budgets per-agent/agent-class)\n",
    "\n",
    "By implementing comprehensive persistence strategies, our system ensures that valuable analysis results are preserved while ensuring performance, scalability, and observability.\n",
    "\n",
    "---\n",
    "\n",
    "I think we've now earned the right to look at our results more directly! Try running the actual streamlit app below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8467d22e-7860-4066-939a-70ca090662cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defaults to open on localhost:8501\n",
    "!streamlit run dev/incident_dashboard.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7501d18c-79a5-4ac9-abbd-69fd59c3b08a",
   "metadata": {},
   "source": [
    "# 10. Evaluation\n",
    "\n",
    "**How do we measure accuracy and drift?**\n",
    "\n",
    "We instrument our system to capture a variety of metrics—moving averages, variance, per-incident standard deviations, token usage, error counts, etc.—so that we can monitor performance over time. In the long run, these metrics should live behind a configurable layer, making it trivial to add, remove, or adjust what we log without touching the core code.\n",
    "\n",
    "Right now, a quick look at our test runs shows risk scores that are consistently higher than expected. That’s largely an artifact of using synthetic “dummy” data during our early normalization experiments, which tends to skew the model’s self-calibration.\n",
    "\n",
    "In a production rollout, we would:\n",
    "\n",
    "1. **Build a “golden dataset”** of expert-validated incidents and analyses to seed the historical index.\n",
    "2. **Audit metrics regularly**, especially early on, to verify that the model’s outputs align with real-world expectations.\n",
    "3. **Incorporate feedback loops**, tagging or flagging outlier runs with human-adjusted confidence or priority scores, and then retraining or re-indexing as needed.\n",
    "\n",
    "For this proof-of-concept, the underlying evaluation framework is sound—the key next step is to populate it with high-quality, representative data so that our statistics and drift detection become truly meaningful.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a113df3f-c219-46f8-a8e8-90c761c2a62a",
   "metadata": {},
   "source": [
    "# 11. Conclusion\n",
    "\n",
    "## 11.1 Summary\n",
    "\n",
    "In this notebook we’ve delivered a complete pipeline for contextual CVE analysis:\n",
    "\n",
    "1. **Semantic Search**  \n",
    "   FAISS vector stores over KEV, NVD, and historical incident records power high-signal retrieval.  \n",
    "2. **LLM-Driven Reasoning**  \n",
    "   A GPT-4o-mini–based ReAct agent interprets incident context, invokes retrieval tools, and synthesizes risk assessments.  \n",
    "3. **Structured Tooling**  \n",
    "   An MCP server (`mcp_cve_server.py`) exposes typed, cache-enabled functions for CVE lookup, semantic search, and schema introspection.  \n",
    "4. **Validated Output**  \n",
    "   Pydantic models enforce a consistent JSON schema (`IncidentAnalysisList`), ensuring every analysis includes IDs, summaries, relevance scores, and risk explanations.  \n",
    "5. **Durable Persistence & Learning**  \n",
    "   Analyses and run metrics are stored in SQLite and vector indexes are incrementally updated—so every run enriches future context.\n",
    "\n",
    "Together, these components demonstrate how retrieval-augmented generation can automate and standardize the initial triage of security incidents, reducing analyst toil, improving consistency, and providing transparent justifications for each risk score.\n",
    "\n",
    "### 11.1.1 Platform-Level Best Practices\n",
    "\n",
    "Beyond the AI logic itself, we baked in core software-engineering principles to ensure this system could evolve into a robust, service-based architecture:\n",
    "\n",
    "1. **Idempotency**  \n",
    "   Every analysis request carries a unique `request_id`, and duplicate submissions are automatically rejected via FastAPI dependency injection:  \n",
    "   ```python\n",
    "   @app.post(\"/analyze_incidents\")\n",
    "   async def analyze_incidents(\n",
    "       request: AnalysisRequest,\n",
    "       _dedupe: None = Depends(claim_request_id),\n",
    "   ):\n",
    "       ...\n",
    "    ```\n",
    "\n",
    "2. **Separation of Concerns**\n",
    "\n",
    "   * **Tool Server** (`mcp_cve_server.py`): Hosts all MCP-decorated functions; thin wrappers around modular utilities (e.g. `retrieval_utils`).\n",
    "   * **Agent Core** (`main_security_agent_server.py`): Contains prompt orchestration and ReAct logic, isolated from CLI or notebook invocation.\n",
    "   * **Fire-and-Forget Persistence**: Writes to the database and FAISS indexes asynchronously, so failed I/O doesn’t derail the analysis loop. FAISS write steps utilize native library functionality to lock on write to ensure we don't have concurrent write attempts (for the historial incidents index).\n",
    "\n",
    "3. **Observability**\n",
    "\n",
    "   * **Metrics & Tracing**: Decorators capture timing, token use, cache hits/misses, tool-call counts, and error rates.\n",
    "   * **Structured Logging**: Contextual logs (with request IDs) surface failures and performance bottlenecks.\n",
    "   * **Pluggable Backends**: Our `datastore_utils` abstraction makes swapping SQLite for PostgreSQL—or even a hosted analytics datastore—a drop-in change.\n",
    "\n",
    "4. **Scalability & Resilience**\n",
    "\n",
    "   * **Async Processing**: Batched, asynchronous calls (via `asyncio`/FastAPI) let us process hundreds of incidents in parallel without blocking.\n",
    "   * **Container-Ready**: Every component (API, MCP server, Redis) can be dockerized with resource limits and orchestrated via Kubernetes or Docker Compose.\n",
    "   * **Failure Isolation**: Individual tool calls and persistence tasks are wrapped in `try/except`, so transient errors don’t cascade.\n",
    "\n",
    "5. **Security & Configuration**\n",
    "\n",
    "   * **Secrets Management**: API keys and database URLs are injected via environment variables or a `.env` file—never hard-coded.\n",
    "   * **Input Validation**: Pydantic request models enforce schema compliance at the edge.\n",
    "   * **Least Privilege**: Redis and database connections use dedicated, minimally scoped credentials.\n",
    "   * **Config-Driven**: All key parameters—API URLs, batch sizes, concurrency levels, API keys, log file paths, etc.—are pulled from configuration (env vars or config files), with sensible defaults applied when no override is provided.\n",
    "\n",
    "6. **Testability & CI/CD**\n",
    "\n",
    "   * **Unit & Integration Tests**: Utilities (`utils/`) and MCP tools are covered by pytest with high extensibility.\n",
    "   * **Automated Builds**: A CI pipeline can lint, type-check, spin up a temporary Redis/SQLite instance, and run the full test suite on each PR.\n",
    "   * Containerization and deployment are things we could implment with likely a day (or less), moving us to aa more production-ready system.\n",
    "\n",
    "By layering these practices on top of our GenAI agent, we ensure that the prototype is not only powerful but also maintainable, observable, and ready to scale.\n",
    "\n",
    "## 11.2 Next Steps\n",
    "\n",
    "Building on this foundation, a few high-impact enhancements could include:\n",
    "\n",
    "1. **Broader Intelligence Feeds**  \n",
    "   Integrate additional threat feeds (e.g., MITRE ATT&CK, vendor advisories) to widen coverage.  \n",
    "2. **Remediation Guidance**  \n",
    "   Extend the agent to suggest concrete mitigation steps alongside risk assessments.  \n",
    "3. **Model Ensembles**  \n",
    "   Layer specialized LLMs for discrete tasks (e.g., one for summarization, another for scoring) to optimize cost and performance.  \n",
    "4. **Interactive Dashboards**  \n",
    "   Enable analysts to drill down, flag or correct an analysis, and trigger on-demand re-runs.  \n",
    "5. **Temporal and Trend Analysis**  \n",
    "   Add time-series views over incidents and CVE metrics to surface emerging patterns.  \n",
    "6. **Active Learning Loop**  \n",
    "   Incorporate direct analyst feedback into the training and indexing processes to continuously refine accuracy.\n",
    "\n",
    "## 11.3 Key Takeaways\n",
    "\n",
    "1. **Architecture & Workflow**  \n",
    "   A FastAPI front-end spawns a LangChain/LangGraph ReAct agent that orchestrates prompt generation, tool calls, and output parsing.  \n",
    "2. **Prompting Strategy**  \n",
    "   Pre-injected FAISS results and Pydantic instructions give the LLM focused, structured context and a clear output schema.  \n",
    "3. **Tool Interaction**  \n",
    "   Caching, timing metrics, and annotations in `mcp_cve_server.py` make tool calls reliable and efficient.  \n",
    "4. **Context Management**  \n",
    "   Batching, flattening, and targeted filtering keep the prompt concise while preserving relevance.  \n",
    "5. **Explainability & Traceability**  \n",
    "   Every risk score is backed by a Pydantic-validated explanation and a log of tool invocations.  \n",
    "\n",
    "## 11.4 Final Thoughts\n",
    "\n",
    "This project illustrates a practical, end-to-end GenAI solution for a real-world security challenge. By combining vector retrieval, declarative tool interfaces, and rigorous output validation, we’ve created an automated triage assistant that:\n",
    "\n",
    "- **Scales** to hundreds of incidents without manual review  \n",
    "- **Adapts** as new data feeds and analyses are added  \n",
    "- **Supports** human oversight with transparent reasoning trails  \n",
    "\n",
    "As cyber-threat volumes continue to grow, such AI-augmented workflows will become essential for security teams striving to stay ahead of emerging vulnerabilities. This notebook provides a solid, extensible blueprint for that future.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce24787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thank you for reviewing this notebook!\n",
    "print(\"Analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abe038f-eb2a-488c-b3d8-1ec81a18a530",
   "metadata": {},
   "source": [
    "Please reach out with any questions, feel free to request a code walkthrough and/or demonstration, and anything else by reaching out to me at dan.guilliams@danguilliams.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6297037c-de11-439c-bde1-67d2afc99f2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d02b65c-4125-44db-ac2d-acaae3657615",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
