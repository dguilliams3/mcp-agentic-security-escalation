{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d537cbad",
   "metadata": {},
   "source": [
    "# RAD Security: CVE-Aware Analysis Agent\n",
    "\n",
    "## AI Engineer Take-Home Exercise\n",
    "\n",
    "This document outlines the architecture and implementation of an LLM-powered agent designed to analyze security incidents in the context of CVE data. The agent uses semantic search, retrieval-augmented generation, and structured agent tools to provide contextual prioritization of security vulnerabilities.\n",
    "\n",
    "### Key Features\n",
    "\n",
    "- **Semantic Search**: Searches over KEV, NVD, and historical incident records\n",
    "- **Agent Tools**: Uses MCP (Model Context Protocol) for structured tool access\n",
    "- **Contextual Prioritization**: Ranks CVEs based on their relevance to specific incidents\n",
    "- **Historical Learning**: Builds a vector store of past analyses for normalization\n",
    "- **Persistence**: Stores analyses in SQLite for future reference\n",
    "\n",
    "## Why This Architecture\n",
    "\n",
    "The architecture is designed to address several key challenges in security incident analysis:\n",
    "\n",
    "1. **Volume Challenge**: Security teams face thousands of CVEs and alerts daily\n",
    "2. **Context Challenge**: Understanding the relationship between vulnerabilities and incidents requires contextual knowledge\n",
    "3. **Expertise Challenge**: Security expertise is scarce and expensive\n",
    "4. **Consistency Challenge**: Manual analysis leads to inconsistent prioritization\n",
    "\n",
    "Our solution uses LLMs and semantic search to understand incident context, identify relevant CVEs, prioritize them based on impact, and generate human-readable explanations of the analysis.\n",
    "\n",
    "## 1. Setup and Dependencies\n",
    "\n",
    "Let's start by installing the required dependencies and setting up our environment.\n",
    "\n",
    "**Why we do this:** Ensuring all required packages are available creates a reproducible environment. This setup step loads essential libraries for LangChain, LangGraph, OpenAI, FAISS vector storage, and Redis caching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d1282aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dan Guilliams\\OneDrive\\Code Projects\\MCP_Agents_RADSecurity\\.venv\\Scripts\\python.exe: No module named pip\n"
     ]
    }
   ],
   "source": [
    "# Install requirements\n",
    "%pip install -r requirements.txt\n",
    "\n",
    "# Below is our requirements.txt content for reference\n",
    "# aiohttp==3.8.5\n",
    "# fastapi==0.100.0\n",
    "# fastmcp==0.2.0\n",
    "# httpx==0.25.0\n",
    "# langchain==0.0.331\n",
    "# langchain-community==0.0.11\n",
    "# langchain-core==0.1.3\n",
    "# langchain-mcp-adapters==0.0.3\n",
    "# langchain-openai==0.0.2\n",
    "# langgraph==0.0.16\n",
    "# openai==1.1.2\n",
    "# pydantic==2.4.2\n",
    "# python-dotenv==1.0.0\n",
    "# redis==4.6.0\n",
    "# numpy==1.24.4\n",
    "# faiss-cpu==1.7.4\n",
    "# uvicorn==0.23.2\n",
    "# sqlalchemy==2.0.19\n",
    "# streamlit==1.26.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe8a849",
   "metadata": {},
   "source": [
    "## 2. Start Redis (for Idempotency Cache)\n",
    "\n",
    "We'll use Redis for request deduplication and caching. This ensures our system is idempotent and avoids redundant processing.\n",
    "\n",
    "**Why we do this:** Redis provides fast, in-memory caching that helps us:\n",
    "1. Deduplicate analysis requests (idempotency)\n",
    "2. Cache expensive operations like semantic searches\n",
    "3. Reduce API costs and latency by storing LLM responses\n",
    "4. Ensure consistent behavior even with intermittent failures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b7843b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "docker: Error response from daemon: Conflict. The container name \"/local-redis\" is already in use by container \"d7532e8e07eda16696991be756f16a983f49b82035ed57a286f0bc5f0aaafb22\". You have to remove (or rename) that container to be able to reuse that name.\n",
      "\n",
      "Run 'docker run --help' for more information\n"
     ]
    }
   ],
   "source": [
    "# Start Docker Service for Redis:\n",
    "!docker run -d --name local-redis -p 6379:6379 redis:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3629950",
   "metadata": {},
   "source": [
    "## 3. System Architecture Overview\n",
    "\n",
    "Our system follows a layered architecture with distinct components handling specific responsibilities:\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────┐\n",
    "│          Client Layer           │\n",
    "│ (Notebook, run_analysis.py)     │\n",
    "└────────────────┬────────────────┘\n",
    "                 │\n",
    "┌────────────────▼────────────────┐\n",
    "│        API Service Layer        │\n",
    "│ (main_security_agent_server.py) │\n",
    "└────────────────┬────────────────┘\n",
    "                 │\n",
    "┌────────────────▼────────────────┐\n",
    "│        Agent Layer             │\n",
    "│ (LangChain, LangGraph, ReAct)   │\n",
    "└────────────────┬────────────────┘\n",
    "                 │\n",
    "┌────────────────▼────────────────┐\n",
    "│       Tools Layer               │\n",
    "│ (MCP Server, mcp_cve_server.py) │\n",
    "└────────────────┬────────────────┘\n",
    "                 │\n",
    "┌────────────────▼────────────────┐\n",
    "│       Storage Layer             │\n",
    "│ (FAISS, Redis, SQLite)          │\n",
    "└─────────────────────────────────┘\n",
    "```\n",
    "\n",
    "**Why this architecture:** \n",
    "\n",
    "1. **Separation of Concerns**: Each layer has a distinct responsibility\n",
    "2. **Scalability**: Components can be scaled independently\n",
    "3. **Resilience**: Failures in one layer don't cascade to others\n",
    "4. **Maintainability**: Easier to update or replace individual components\n",
    "5. **Testing**: Components can be tested in isolation\n",
    "\n",
    "### Key Project Files and Their Roles\n",
    "\n",
    "```\n",
    ".\n",
    "├── main_security_agent_server.py  # FastAPI server coordinating analysis\n",
    "├── mcp_cve_server.py              # Tool server providing CVE search capabilities\n",
    "├── run_analysis.py                # CLI script for batch processing\n",
    "├── data/                          # Data storage\n",
    "│   ├── incidents.json             # Input security incidents\n",
    "│   ├── kev.json                   # Known Exploited Vulnerabilities\n",
    "│   ├── nvd_subset.json            # National Vulnerability Database subset\n",
    "│   └── vectorstore/               # FAISS vector indexes\n",
    "├── setup/                         # Setup scripts\n",
    "│   ├── download_cve_data.py       # Downloads CVE data\n",
    "│   └── build_faiss_indexes.py     # Builds vector indexes\n",
    "└── utils/                         # Utility functions\n",
    "    ├── retrieval_utils.py         # Vector search functions\n",
    "    ├── flatteners.py              # Text preprocessing for embeddings\n",
    "    ├── prompt_utils.py            # Prompt generation\n",
    "    ├── datastore_utils.py         # Database operations\n",
    "    └── decorators.py              # Logging and caching\n",
    "``` \n",
    "# 4. Data Ingestion & Preprocessing\n",
    "\n",
    "In this section, we'll explore the input data and prepare it for analysis. We need to:\n",
    "\n",
    "1. Load and inspect incident data\n",
    "2. Retrieve and prepare CVE data (KEV and NVD)\n",
    "3. Create flattened text representations for vector embedding\n",
    "\n",
    "**Why we do this:** Proper data preparation is critical for effective semantic search. By flattening complex JSON structures into searchable text, we enable the embedding model to capture semantic relationships between incidents and vulnerabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7b16e4a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total incidents: 39\n",
      "Fields in first incident: ['incident_id', 'timestamp', 'title', 'description', 'affected_assets', 'observed_ttps', 'indicators_of_compromise', 'initial_findings']\n",
      "\n",
      "Sample incident details:\n",
      "{\n",
      "  \"incident_id\": \"INC-2023-08-01-001\",\n",
      "  \"timestamp\": \"2023-08-01T09:15:00Z\",\n",
      "  \"title\": \"Unauthorized Access Attempt on VPN Gateway\",\n",
      "  \"description\": \"Multiple failed login attempts followed by a successful connection from an unusual geographic location on the main VPN gateway.\",\n",
      "  \"affected_assets\": [\n",
      "    {\n",
      "      \"hostname\": \"vpn-gateway-01\",\n",
      "      \"ip_address\": \"203.0.113.1\",\n",
      "      \"os\": \"Cisco IOS XE\",\n",
      "      \"installed_software\": [\n",
      "        {\n",
      "          \"name\": \"Cisco IOS XE\",\n",
      "          \"version\": \"17.3.4a\"\n",
      "        }\n",
      "      ],\n",
      "      \"role\": \"VPN Gateway\"\n",
      "    }\n",
      "  ],\n",
      "  \"observed_ttps\": [\n",
      "    {\n",
      "      \"framework\": \"MITRE ATT&CK\",\n",
      "      \"id\": \"T1110\",\n",
      "      \"name\": \"Brute Force\"\n",
      "    },\n",
      "    {\n",
      "      \"framework\": \"MITRE ATT&CK\",\n",
      "      \"id\": \"T1078\",\n",
      "      \"name\": \"Valid Accounts\"\n",
      "    }\n",
      "  ],\n",
      "  \"indicators_of_compromise\": [\n",
      "    {\n",
      "      \"type\": \"ip_address\",\n",
      "      \"value\": \"172.91.8.123\",\n",
      "      \"context\": \"Source IP of successful login\"\n",
      "    },\n",
      "    {\n",
      "      \"type\": \"username\",\n",
      "      \"value\": \"admin\",\n",
      "      \"context\": \"Account used for successful login\"\n",
      "    }\n",
      "  ],\n",
      "  \"initial_findings\": \"Credential stuffing or brute force attack successful against VPN.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Ensure we have an OpenAI API key\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    raise ValueError(\n",
    "        \"Please set the OPENAI_API_KEY environment variable in your .env file\"\n",
    "    )\n",
    "\n",
    "# Load incidents data\n",
    "data_dir = Path('data')\n",
    "with open(data_dir / 'incidents.json') as f:\n",
    "    incidents = json.load(f)\n",
    "\n",
    "# Display overview statistics\n",
    "print(f\"Total incidents: {len(incidents)}\")\n",
    "print(\"Fields in first incident:\", list(incidents[0].keys()))\n",
    "\n",
    "# Display first incident in pretty format\n",
    "print(\"\\nSample incident details:\")\n",
    "print(json.dumps(incidents[0], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "981cd927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEV entries: 1351\n",
      "\n",
      "Sample KEV entry:\n",
      "{\n",
      "  \"cveID\": \"CVE-2023-38950\",\n",
      "  \"vendorProject\": \"ZKTeco\",\n",
      "  \"product\": \"BioTime\",\n",
      "  \"vulnerabilityName\": \"ZKTeco BioTime Path Traversal Vulnerability\",\n",
      "  \"dateAdded\": \"2025-05-19\",\n",
      "  \"shortDescription\": \"ZKTeco BioTime contains a path traversal vulnerability in the iclock API that allows an unauthenticated attacker to read arbitrary files via supplying a crafted payload.\",\n",
      "  \"requiredAction\": \"Apply mitigations per vendor instructions, follow applicable BOD 22-01 guidance for cloud services, or discontinue use of the product if mitigations are unavailable.\",\n",
      "  \"dueDate\": \"2025-06-09\",\n",
      "  \"knownRansomwareCampaignUse\": \"Unknown\",\n",
      "  \"notes\": \"https://www.zkteco.com/en/Security_Bulletinsibs ; https://nvd.nist.gov/vuln/detail/CVE-2023-38950\",\n",
      "  \"cwes\": [\n",
      "    \"CWE-22\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Load KEV (Known Exploited Vulnerabilities) data\n",
    "with open(data_dir / 'kev.json') as f:\n",
    "    kev_data = json.load(f)\n",
    "    \n",
    "print(f\"KEV entries: {len(kev_data.get('vulnerabilities', []))}\")\n",
    "print(\"\\nSample KEV entry:\")\n",
    "print(json.dumps(kev_data.get('vulnerabilities', [])[0], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9b5095bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVD entries: 3062\n",
      "\n",
      "Sample NVD entry (CVE-2025-0020):\n",
      "Violation of Secure Design Principles, Hidden Functionality, Incorrect Provision of Specified Functionality vulnerability in ArcGIS (Authentication) allows Privilege Abuse, Manipulating Hidden Fields, Configuration/Environment Manipulation.\n",
      "\n",
      "The ArcGIS client_credentials OAuth 2.0 API implementation does not adhere to the RFC/standards; This hidden (known and by-design, but undocumented) functionality enables a requestor (Referred to as client in RFC 6749) to request an, undocumented, custom token expiration from ArcGIS (Referred to as authorization server in RFC 6749).\n"
     ]
    }
   ],
   "source": [
    "# Load NVD (National Vulnerability Database) data\n",
    "with open(data_dir / 'nvd_subset.json') as f:\n",
    "    nvd_data = json.load(f)\n",
    "    \n",
    "print(f\"NVD entries: {len(nvd_data)}\")\n",
    "\n",
    "# Display a sample NVD entry\n",
    "sample_cve_id = list(nvd_data.keys())[0]\n",
    "print(f\"\\nSample NVD entry ({sample_cve_id}):\")\n",
    "# Just show the description part to keep it manageable\n",
    "desc = nvd_data[sample_cve_id].get(\"cve\", {}).get(\"description\", {}).get(\"description_data\", [{}])[0].get(\"value\", \"\")\n",
    "print(desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bf576c",
   "metadata": {},
   "source": [
    "## 5. Text Flattening for Vector Embedding\n",
    "\n",
    "Before building our vector indexes, we need to convert the structured data into a flattened text format suitable for embedding. Let's examine our flattening strategies:\n",
    "\n",
    "**Why we do this:** Embeddings work on raw text, but our data is in complex JSON structures. Flattening transforms these structures into searchable text while preserving the semantic meaning of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1a22f4ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened KEV document:\n",
      "CVE CVE-2023-38950\n",
      "ZKTeco\n",
      "BioTime\n",
      "ZKTeco BioTime Path Traversal Vulnerability\n",
      "ZKTeco BioTime contains a path traversal vulnerability in the iclock API that allows an unauthenticated attacker to read a ...\n",
      "\n",
      "Flattened NVD document:\n",
      "CVE CVE-2025-0020\n",
      "Violation of Secure Design Principles, Hidden Functionality, Incorrect Provision of Specified Functionality vulnerability in ArcGIS (Authentication) allows Privilege Abuse, Manipulat ...\n",
      "\n",
      "Flattened Incident document:\n",
      "Unauthorized Access Attempt on VPN Gateway\n",
      "Multiple failed login attempts followed by a successful connection from an unusual geographic location on the main VPN gateway.\n",
      "Credential stuffing or brute  ...\n"
     ]
    }
   ],
   "source": [
    "from utils.flatteners import flatten_kev, flatten_nvd, flatten_incident\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "# Example KEV entry flattening\n",
    "sample_kev = kev_data.get('vulnerabilities', [])[0]\n",
    "doc_kev = flatten_kev(sample_kev)\n",
    "print(\"Flattened KEV document:\")\n",
    "print(doc_kev.page_content[:200], \"...\")\n",
    "\n",
    "# Example NVD entry flattening\n",
    "sample_nvd = list(nvd_data.values())[0]\n",
    "doc_nvd = flatten_nvd(sample_nvd)\n",
    "print(\"\\nFlattened NVD document:\")\n",
    "print(doc_nvd.page_content[:200], \"...\")\n",
    "\n",
    "# Example Incident flattening\n",
    "doc_inc = Document(\n",
    "    page_content=flatten_incident(incidents[0]), \n",
    "    metadata={\"incident_id\": incidents[0][\"incident_id\"]}\n",
    ")\n",
    "print(\"\\nFlattened Incident document:\")\n",
    "print(doc_inc.page_content[:200], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f81694",
   "metadata": {},
   "source": [
    "Let's examine the flattening functions to understand how they work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "780f43fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils/flatteners.py example implementation\n",
    "def flatten_incident(incident: dict) -> str:\n",
    "    \"\"\"\n",
    "    Flatten an incident into a text representation for embedding.\n",
    "    \n",
    "    Args:\n",
    "        incident: The incident dict to flatten\n",
    "        \n",
    "    Returns:\n",
    "        A string representation of the incident\n",
    "    \"\"\"\n",
    "    # Start with the title and description\n",
    "    text = f\"{incident.get('title', '')}\\n{incident.get('description', '')}\\n\"\n",
    "    \n",
    "    # Add initial findings\n",
    "    text += f\"{incident.get('initial_findings', '')}\\n\"\n",
    "    \n",
    "    # Add affected assets\n",
    "    for asset in incident.get(\"affected_assets\", []):\n",
    "        text += f\"Asset: {asset.get('hostname', '')} ({asset.get('ip_address', '')})\\n\"\n",
    "        text += f\"OS: {asset.get('os', '')}\\n\"\n",
    "        text += f\"Role: {asset.get('role', '')}\\n\"\n",
    "        \n",
    "        # Add installed software\n",
    "        for sw in asset.get(\"installed_software\", []):\n",
    "            text += f\"Software: {sw.get('name', '')} {sw.get('version', '')}\\n\"\n",
    "    \n",
    "    # Add TTPs (Tactics, Techniques, and Procedures)\n",
    "    for ttp in incident.get(\"observed_ttps\", []):\n",
    "        text += f\"TTP: {ttp.get('name', '')} ({ttp.get('id', '')})\\n\"\n",
    "    \n",
    "    # Add indicators of compromise\n",
    "    for ioc in incident.get(\"indicators_of_compromise\", []):\n",
    "        text += f\"IoC: {ioc.get('type', '')}: {ioc.get('value', '')} - {ioc.get('context', '')}\\n\"\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a1359b",
   "metadata": {},
   "source": [
    "## 6. Building Vector Indexes\n",
    "\n",
    "Now we'll build FAISS vector indexes for efficient semantic search across our data sources. This process involves:\n",
    "\n",
    "1. Initializing the OpenAI embeddings model\n",
    "2. Creating FAISS indexes for KEV, NVD, and historical incident data\n",
    "3. Setting up utilities for semantic search\n",
    "\n",
    "**Why we do this:** Vector indexes enable fast similarity search over large datasets. By precomputing embeddings and storing them in FAISS indexes, we can perform semantic searches in milliseconds rather than having to recompute embeddings for each query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fc6bc4bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23:49:16 INFO     [root] Initializing OpenAI embeddings...\n",
      "23:49:16 INFO     [root] OpenAI embeddings initialized!\n",
      "23:49:16 INFO     [root] initialize_openai_embeddings completed in 0.51s\n",
      "23:49:16 INFO     [root] Loading KEV FAISS index...\n",
      "23:49:16 INFO     [root] KEV FAISS index loaded!\n",
      "23:49:16 INFO     [root] Loading NVD FAISS index...\n",
      "23:49:16 INFO     [root] NVD FAISS index loaded!\n",
      "23:49:16 INFO     [root] Loading Incident Analysis History FAISS index...\n",
      "23:49:16 INFO     [root] Incident Analysis History FAISS index loaded!\n",
      "23:49:16 INFO     [root] initialize_faiss_indexes completed in 0.04s\n",
      "Embeddings and FAISS indexes initialized successfully.\n"
     ]
    }
   ],
   "source": [
    "from utils.retrieval_utils import initialize_openai_embeddings, initialize_faiss_indexes\n",
    "\n",
    "# Initialize OpenAI embeddings and FAISS indexes\n",
    "initialize_openai_embeddings()\n",
    "initialize_faiss_indexes()\n",
    "print(\"Embeddings and FAISS indexes initialized successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bb2523",
   "metadata": {},
   "source": [
    "Implementation details from `utils/retrieval_utils.py`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "faf8d52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "from utils.retrieval_utils import DATA_DIR\n",
    "\n",
    "def initialize_openai_embeddings():\n",
    "    \"\"\"\n",
    "    Initialize the global OpenAI embeddings object for vector representations.\n",
    "    \"\"\"\n",
    "    global embeddings\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "\n",
    "def initialize_faiss_indexes():\n",
    "    \"\"\"\n",
    "    Initialize global FAISS vector indexes for different vulnerability databases.\n",
    "    \"\"\"\n",
    "    global KEV_FAISS, NVD_FAISS, INCIDENT_HISTORY_FAISS\n",
    "    if embeddings is None:\n",
    "        initialize_openai_embeddings()\n",
    "\n",
    "    # Load pre-built FAISS indexes\n",
    "    KEV_FAISS = FAISS.load_local(DATA_DIR / \"kev\", embeddings,\n",
    "                              allow_dangerous_deserialization=True)\n",
    "    \n",
    "    NVD_FAISS = FAISS.load_local(DATA_DIR / \"nvd\", embeddings,\n",
    "                              allow_dangerous_deserialization=True)\n",
    "    \n",
    "    INCIDENT_HISTORY_FAISS = FAISS.load_local(DATA_DIR / \"incident_analysis_history\", \n",
    "                               embeddings, allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ffd4cfe",
   "metadata": {},
   "source": [
    "## 7. Testing Semantic Search\n",
    "\n",
    "Let's test our vector indexes by performing semantic searches over the different data sources:\n",
    "\n",
    "**Why we do this:** Verifying semantic search capabilities ensures that our system can effectively identify relevant CVEs and historical incidents. This helps validate our data preparation and embedding strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8d418b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search query: Unauthorized Access Attempt on VPN Gateway\n",
      "23:49:18 DEBUG    [langchain_community.utils.math] Unable to import simsimd, defaulting to NumPy implementation. If you want to use simsimd please install with `pip install simsimd`.\n",
      "23:49:18 DEBUG    [langchain_community.utils.math] Unable to import simsimd, defaulting to NumPy implementation. If you want to use simsimd please install with `pip install simsimd`.\n",
      "23:49:18 DEBUG    [langchain_community.utils.math] Unable to import simsimd, defaulting to NumPy implementation. If you want to use simsimd please install with `pip install simsimd`.\n",
      "23:49:18 INFO     [root] _search completed in 1.40s\n",
      "\n",
      "Top 3 KEV matches:\n",
      "1. CVE-2015-1187 (score: 1.897)\n",
      "   CVE CVE-2015-1187 D-Link and TRENDnet Multiple Devices D-Link and TRENDnet Multiple Devices Remote C...\n",
      "2. CVE-2006-2492 (score: 1.930)\n",
      "   CVE CVE-2006-2492 Microsoft Word Microsoft Word Malformed Object Pointer Vulnerability Microsoft Wor...\n",
      "3. CVE-2020-3452 (score: 1.905)\n",
      "   CVE CVE-2020-3452 Cisco Adaptive Security Appliance (ASA) and Firepower Threat Defense (FTD) Cisco A...\n",
      "23:49:19 DEBUG    [langchain_community.utils.math] Unable to import simsimd, defaulting to NumPy implementation. If you want to use simsimd please install with `pip install simsimd`.\n",
      "23:49:19 DEBUG    [langchain_community.utils.math] Unable to import simsimd, defaulting to NumPy implementation. If you want to use simsimd please install with `pip install simsimd`.\n",
      "23:49:19 DEBUG    [langchain_community.utils.math] Unable to import simsimd, defaulting to NumPy implementation. If you want to use simsimd please install with `pip install simsimd`.\n",
      "23:49:19 INFO     [root] _search completed in 1.15s\n",
      "\n",
      "Top 3 NVD matches:\n",
      "1. CVE-2025-25724 (score: 1.903)\n",
      "   CVE CVE-2025-25724 list_item_verbose in tar/util.c in libarchive through 3.7.7 does not check an str...\n",
      "2. CVE-2025-20153 (score: 1.931)\n",
      "   CVE CVE-2025-20153 A vulnerability in the email filtering mechanism of Cisco Secure Email Gateway co...\n",
      "3. CVE-2025-30065 (score: 1.938)\n",
      "   CVE CVE-2025-30065 Schema parsing in the parquet-avro module of Apache Parquet 1.15.0 and previous v...\n"
     ]
    }
   ],
   "source": [
    "from utils.retrieval_utils import _search, KEV_FAISS, NVD_FAISS\n",
    "\n",
    "# Perform a semantic search using an incident title\n",
    "query_text = incidents[0]['title']\n",
    "print(f\"Search query: {query_text}\")\n",
    "\n",
    "# Search KEV database\n",
    "kev_results = _search(KEV_FAISS, query_text, k=3)\n",
    "print(\"\\nTop 3 KEV matches:\")\n",
    "for i, r in enumerate(kev_results, 1):\n",
    "    print(f\"{i}. {r['cve_id']} (score: {r['variance']:.3f})\")\n",
    "    print(f\"   {r.get('preview', '')[:100]}...\")\n",
    "\n",
    "# Search NVD database\n",
    "nvd_results = _search(NVD_FAISS, query_text, k=3)\n",
    "print(\"\\nTop 3 NVD matches:\")\n",
    "for i, r in enumerate(nvd_results, 1):\n",
    "    print(f\"{i}. {r['cve_id']} (score: {r['variance']:.3f})\")\n",
    "    print(f\"   {r.get('preview', '')[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a5f927",
   "metadata": {},
   "source": [
    "Let's also examine the core search function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "adbec86a",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "\n",
    "\n",
    "def _search(\n",
    "    store: FAISS,\n",
    "    query: str,\n",
    "    k: int = 5,\n",
    "    use_mmr: bool = True,\n",
    "    lambda_mult: float = 0.7,\n",
    "    fetch_k: int = None\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Perform a semantic search on a given FAISS vector store.\n",
    "    \n",
    "    Args:\n",
    "        store: The FAISS vector store to search\n",
    "        query: The search query string\n",
    "        k: Number of top results to return\n",
    "        use_mmr: Use Maximal Marginal Relevance for diverse results\n",
    "        lambda_mult: Diversity control for MMR search\n",
    "        fetch_k: Number of documents to fetch before filtering for MMR\n",
    "        \n",
    "    Returns:\n",
    "        List of search results with metadata and scores\n",
    "    \"\"\"\n",
    "    if use_mmr:\n",
    "        # embed the query once\n",
    "        vec = embeddings.embed_query(query)\n",
    "        # if fetch_k not provided, default to 2*k\n",
    "        fk = fetch_k or (2 * k)\n",
    "        # call the vector-based MMR-with-scores method\n",
    "        pairs = store.max_marginal_relevance_search_with_score_by_vector(\n",
    "            vec, k=k, fetch_k=fk, lambda_mult=lambda_mult,\n",
    "        )\n",
    "    else:\n",
    "        # direct text-based similarity search (score included)\n",
    "        pairs = store.similarity_search_with_score(query, k=k)\n",
    "\n",
    "    # Format results\n",
    "    out = []\n",
    "    for doc, score in pairs:\n",
    "        meta = doc.metadata.copy()\n",
    "        meta[\"variance\"] = float(score)\n",
    "        meta[\"preview\"] = ' '.join(doc.page_content.replace('\\n', ' ').split())[:120]\n",
    "        out.append(meta)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69dde69c",
   "metadata": {},
   "source": [
    "# 8. Agent Architecture and MCP Tools\n",
    "\n",
    "In this section, we'll explore the core agent architecture and the MCP (Model Context Protocol) tools it uses to analyze security incidents and identify relevant CVEs.\n",
    "\n",
    "## 8.1 MCP Server: Tool Definitions\n",
    "\n",
    "Our agent uses a toolkit of specialized functions for incident analysis. These tools are defined in `mcp_cve_server.py` and exposed via the MCP protocol.\n",
    "\n",
    "**Why we do this:** \n",
    "- MCP provides a standardized way for LLMs to interact with external tools\n",
    "- Tools are defined with rich metadata (annotations) to guide the LLM\n",
    "- The server handles caching, error handling, and logging consistently\n",
    "- Tool definitions are separate from agent logic, enabling reuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8bcb23bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23:49:19 INFO     [root] Initializing OpenAI embeddings...\n",
      "23:49:20 INFO     [root] OpenAI embeddings initialized!\n",
      "23:49:20 INFO     [root] initialize_openai_embeddings completed in 0.57s\n",
      "23:49:20 INFO     [root] Loading KEV FAISS index...\n",
      "23:49:20 INFO     [root] KEV FAISS index loaded!\n",
      "23:49:20 INFO     [root] Loading NVD FAISS index...\n",
      "23:49:20 INFO     [root] NVD FAISS index loaded!\n",
      "23:49:20 INFO     [root] Loading Incident Analysis History FAISS index...\n",
      "23:49:20 INFO     [root] Incident Analysis History FAISS index loaded!\n",
      "23:49:20 INFO     [root] initialize_faiss_indexes completed in 0.05s\n",
      "23:49:20 DEBUG    [mcp.server.lowlevel.server] Initializing server 'cve'\n",
      "23:49:20 DEBUG    [mcp.server.lowlevel.server] Registering handler for ListToolsRequest\n",
      "23:49:20 DEBUG    [mcp.server.lowlevel.server] Registering handler for CallToolRequest\n",
      "23:49:20 DEBUG    [mcp.server.lowlevel.server] Registering handler for ListResourcesRequest\n",
      "23:49:20 DEBUG    [mcp.server.lowlevel.server] Registering handler for ReadResourceRequest\n",
      "23:49:20 DEBUG    [mcp.server.lowlevel.server] Registering handler for PromptListRequest\n",
      "23:49:20 DEBUG    [mcp.server.lowlevel.server] Registering handler for GetPromptRequest\n",
      "23:49:20 DEBUG    [mcp.server.lowlevel.server] Registering handler for ListResourceTemplatesRequest\n",
      "23:49:20 DEBUG    [mcp.server.lowlevel.server] Initializing server 'cve'\n",
      "23:49:20 DEBUG    [mcp.server.lowlevel.server] Registering handler for ListToolsRequest\n",
      "23:49:20 DEBUG    [mcp.server.lowlevel.server] Registering handler for CallToolRequest\n",
      "23:49:20 DEBUG    [mcp.server.lowlevel.server] Registering handler for ListResourcesRequest\n",
      "23:49:20 DEBUG    [mcp.server.lowlevel.server] Registering handler for ReadResourceRequest\n",
      "23:49:20 DEBUG    [mcp.server.lowlevel.server] Registering handler for PromptListRequest\n",
      "23:49:20 DEBUG    [mcp.server.lowlevel.server] Registering handler for GetPromptRequest\n",
      "23:49:20 DEBUG    [mcp.server.lowlevel.server] Registering handler for ListResourceTemplatesRequest\n"
     ]
    }
   ],
   "source": [
    "# Core tool definitions from mcp_cve_server.py\n",
    "from typing import Any, Dict, List\n",
    "from fastmcp import FastMCP\n",
    "from mcp_cve_server import NVD_INDEX\n",
    "from utils.decorators import timing_metric, cache_result\n",
    "from utils.retrieval_utils import match_incident_to_cves, semantic_search_cves\n",
    "\n",
    "mcp = FastMCP(\"cve\")\n",
    "\n",
    "@mcp.tool(annotations={\n",
    "    \"title\": \"Match Incident to CVEs using semantic search\",\n",
    "    \"readOnlyHint\": True,\n",
    "    \"destructiveHint\": False,\n",
    "    \"idempotentHint\": False,\n",
    "    \"openWorldHint\": False\n",
    "})\n",
    "@timing_metric\n",
    "@cache_result(ttl_seconds=30)  # cache identical incident queries for 30s\n",
    "def match_incident_to_cves_tool(incident_id: str, k: int = 5, use_mmr: bool = True) -> dict:\n",
    "    \"\"\"\n",
    "    Match an incident to potentially relevant CVEs using semantic search.\n",
    "    \n",
    "    Args:\n",
    "        incident_id: The ID of the incident to match\n",
    "        k: Maximum number of matches to return\n",
    "        use_mmr: Whether to use MMR for diversity\n",
    "        \n",
    "    Returns:\n",
    "        Dict containing matching CVEs from KEV and NVD databases\n",
    "    \"\"\"\n",
    "    return match_incident_to_cves(incident_id, k, use_mmr)\n",
    "\n",
    "@mcp.tool(\n",
    "  annotations={\n",
    "    \"title\": \"Semantic Free-Form CVE Search\",\n",
    "    \"readOnlyHint\": True,\n",
    "    \"destructiveHint\": False,\n",
    "    \"idempotentHint\": False,\n",
    "    \"openWorldHint\": False\n",
    "  }\n",
    ")\n",
    "@timing_metric\n",
    "@cache_result(ttl_seconds=30)  # cache identical free-form queries\n",
    "def semantic_search_cves_tool(\n",
    "    query: str,\n",
    "    sources: List[str] = [\"kev\", \"nvd\", \"historical\"],\n",
    "    k: int = 5,\n",
    "    use_mmr: bool = False,\n",
    "    lambda_mult: float = 0.7\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Perform a semantic search for CVEs using a free-form query.\n",
    "    Contains ability to search multiple indexes in a single call to help with speed and token use, eliminating the need for the agent to perform\n",
    "    multiple tools calls to search the FAISS indexes available.\n",
    "    \n",
    "    Args:\n",
    "        query: Free-form search query\n",
    "        sources: Which databases to search (\"kev\", \"nvd\", \"historical\")\n",
    "        k: Maximum number of results per source\n",
    "        use_mmr: Whether to use MMR for diversity\n",
    "        lambda_mult: Diversity parameter for MMR\n",
    "        \n",
    "    Returns:\n",
    "        Dict containing search results from specified sources\n",
    "    \"\"\"\n",
    "    return semantic_search_cves(query, sources, k, use_mmr, lambda_mult)\n",
    "\n",
    "@mcp.tool(annotations={\n",
    "    \"title\": \"Search NVD Entries for a specific match for ALL words in the query\",\n",
    "    \"readOnlyHint\": True,\n",
    "    \"destructiveHint\": False,\n",
    "    \"idempotentHint\": False,\n",
    "    \"openWorldHint\": False\n",
    "})\n",
    "@timing_metric\n",
    "@cache_result(ttl_seconds=30)  # cache identical free-form queries\n",
    "def search_nvd(query: str, limit: int = 10) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Return up to `limit` full CVE records whose fields match ALL words in `query`.\n",
    "    Case-insensitive substring match over CVE ID, description, and any reference URLs.\n",
    "    \"\"\"\n",
    "    qwords = query.lower().split()\n",
    "    matches = []\n",
    "    for cve_id, rec in NVD_INDEX.items():\n",
    "        # flatten searchable text\n",
    "        desc = rec.get(\"cve\", {}) \\\n",
    "                  .get(\"description\", {}) \\\n",
    "                  .get(\"description_data\", [{}])[0] \\\n",
    "                  .get(\"value\", \"\")\n",
    "        refs = \" \".join([r.get(\"url\",\"\") for r in rec.get(\"cve\",{}) \\\n",
    "                                          .get(\"references\",{}) \\\n",
    "                                          .get(\"reference_data\",[])])\n",
    "        text = f\"{cve_id} {desc} {refs}\".lower()\n",
    "        if all(w in text for w in qwords):\n",
    "            # return the full record so the agent can inspect any fields\n",
    "            matches.append(rec)\n",
    "            if len(matches) >= limit:\n",
    "                break\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54eefc0e",
   "metadata": {},
   "source": [
    "## 8.2 Prompt Engineering\n",
    "\n",
    "The heart of our agent is the prompt that guides its reasoning. Let's examine our prompt engineering strategy:\n",
    "\n",
    "**Why we do this:** Well-crafted prompts are critical for LLM performance. Our prompts are designed to:\n",
    "- Provide clear instructions and context\n",
    "- Include example formats for outputs\n",
    "- Guide the agent to use appropriate tools at the right time\n",
    "- Support structured JSON output via Pydantic models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4ba5dcea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt template from utils/prompt_utils.py\n",
    "SYSTEM_TMPL = \"\"\"\n",
    "You are a CVE‐analysis assistant. Analyze the following incidents and provide structured analysis.\n",
    "\n",
    "Incident Details:\n",
    "{incident_details}\n",
    "\n",
    "Batch FAISS matches (KEV/NVD):\n",
    "{batch_faiss_results}\n",
    "\n",
    "Historical FAISS‐anchoring context:\n",
    "{historical_faiss_results}\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "Now, when I ask you to analyze incidents, use the KEV/NVD context to inform your severity rankings and the historical context to normalize your severity rankings.\n",
    "\"\"\"\n",
    "\n",
    "# Human query example\n",
    "query = \"\"\"\n",
    "I need you to help me analyze some security incidents and rank their actual severity, using identify potential CVE connections and details.\n",
    "For each incident:\n",
    "1. Understand Incident Context: Reason about the affected assets, observed TTPs, and initial findings.\n",
    "2. Identify Relevant CVEs: Determine which CVEs are potentially relevant based on the incident context.\n",
    "3. Prioritize CVEs: Assess the risk and impact of relevant CVEs in the context of the specific incident.\n",
    "4. Generate Analysis: Provide a brief, human-readable explanation of why certain CVEs are prioritized.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb52a301",
   "metadata": {},
   "source": [
    "## 8.3 Pydantic Output Parsing\n",
    "\n",
    "We use Pydantic models to define the structure of the agent's output:\n",
    "\n",
    "**Why we do this:** Structured outputs ensure:\n",
    "- Consistency in the format of analyses\n",
    "- Validation of required fields\n",
    "- Clear typing for downstream processing\n",
    "- Enforced schema compliance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c671f317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pydantic models from utils/prompt_utils.py\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "\n",
    "class CVEInfo(BaseModel):\n",
    "    \"\"\"\n",
    "    A Pydantic model for CVE information.\n",
    "    This model defines the structure of the output from the CVE analysis.\n",
    "    It includes fields for the CVE ID, summary, relevance, and risk level.\n",
    "    \"\"\"\n",
    "    cve_id: str = Field(description=\"The CVE ID that is related to the incident\")\n",
    "    cve_summary: str = Field(description=\"A brief summary of the CVE and its relation to the incident\")\n",
    "    cve_relevance: float = Field(description=\"The estimated relevance level of the CVE match (0.0-1.0)\")\n",
    "    cve_risk_level: float = Field(description=\"The risk level of the CVE on a scale of (0.0-1.0)\")\n",
    "\n",
    "class IncidentAnalysis(BaseModel):\n",
    "    \"\"\"\n",
    "    A Pydantic model for incident analysis.\n",
    "    This model defines the structure of the output from the incident analysis.\n",
    "    It includes fields for the incident ID, summary, list of related CVEs, and the risk level of the incident.\n",
    "    \"\"\"\n",
    "    incident_id: str = Field(description=\"The ID of the incident that caused the error\")\n",
    "    incident_summary: str = Field(description=\"A brief summary of the incident\")\n",
    "    cve_ids: list[CVEInfo] = Field(description=\"List of related CVEs and their details\")\n",
    "    incident_risk_level: float = Field(description=\"The risk level of the incident (0.0-1.0)\")\n",
    "    incident_risk_level_explanation: str = Field(description=\"An explanation of the rationale for the risk level assessment\")\n",
    "\n",
    "class IncidentAnalysisList(BaseModel):\n",
    "    incidents: list[IncidentAnalysis] = Field(description=\"List of incident analyses\")\n",
    "\n",
    "# Initialize the parser\n",
    "parser = PydanticOutputParser(pydantic_object=IncidentAnalysisList)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f967e89",
   "metadata": {},
   "source": [
    "## 8.4 LangChain ReAct Agent\n",
    "\n",
    "We use LangChain's ReAct agent pattern to orchestrate the analysis process:\n",
    "\n",
    "**Why we do this:** The ReAct agent pattern combines:\n",
    "- **Re**asoning: Understanding the task and formulating a plan\n",
    "- **Act**ion: Using tools to gather information\n",
    "- Observation: Processing the results of tool calls\n",
    "- Generation: Producing a final analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3b2bd05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent setup from main_security_agent_server.py\n",
    "import asyncio\n",
    "from langchain_mcp_adapters.tools import load_mcp_tools\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from mcp import ClientSession, StdioServerParameters, stdio_client\n",
    "\n",
    "from utils.prompt_utils import generate_prompt\n",
    "from utils.retrieval_utils import batch_get_historical_context, batch_match_incident_to_cves\n",
    "\n",
    "# Setup server parameters and model\n",
    "server_parameters = StdioServerParameters(\n",
    "    command=\"python\",\n",
    "    args=[\"mcp_cve_server.py\"],\n",
    ")\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\", openai_api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "async def run_agent(query, start_index, batch_size):\n",
    "    async with stdio_client(server_parameters) as (read, write):\n",
    "        # Initialize client session\n",
    "        async with ClientSession(read, write) as session:\n",
    "            await session.initialize()\n",
    "            \n",
    "            # Load MCP tools and create ReAct agent\n",
    "            tools = await load_mcp_tools(session)\n",
    "            agent = create_react_agent(model, tools, name=\"CVE_Agent\")\n",
    "            \n",
    "            # Prepare incident batch and historical context\n",
    "            batch_faiss_results = batch_match_incident_to_cves(\n",
    "                batch_size=batch_size,\n",
    "                start_index=start_index,\n",
    "                top_k=3\n",
    "            )\n",
    "            \n",
    "            historical_results = batch_get_historical_context(\n",
    "                incident_ids=[r[\"incident_id\"] for r in batch_faiss_results[\"results\"]],\n",
    "                top_k=2\n",
    "            )\n",
    "            \n",
    "            # Generate prompt with all context\n",
    "            prompt_messages = generate_prompt(\n",
    "                query=query,\n",
    "                batch_faiss_results=batch_faiss_results,\n",
    "                historical_faiss_results=historical_results\n",
    "            )\n",
    "            \n",
    "            # Execute agent\n",
    "            final_msg, full_response = await agent.ainvoke({\"messages\": prompt_messages})\n",
    "            \n",
    "            # Parse and validate results\n",
    "            analysis = parser.parse(final_msg.content)\n",
    "            \n",
    "            return analysis, full_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487f3172",
   "metadata": {},
   "source": [
    "## 8.5 Running the Agent\n",
    "\n",
    "Let's run the agent to analyze a batch of security incidents:\n",
    "\n",
    "**Why we do this:** Running a complete analysis demonstrates the end-to-end workflow and validates our agent's ability to:\n",
    "- Understand incident context\n",
    "- Find relevant CVEs\n",
    "- Assess risk levels\n",
    "- Provide clear explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0e5dbb68",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotImplementedError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Dan Guilliams\\OneDrive\\Code Projects\\MCP_Agents_RADSecurity\\.venv\\Lib\\site-packages\\mcp\\client\\stdio\\win32.py:72\u001b[39m, in \u001b[36mcreate_windows_process\u001b[39m\u001b[34m(command, args, env, errlog, cwd)\u001b[39m\n\u001b[32m     70\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     71\u001b[39m     \u001b[38;5;66;03m# Try with Windows-specific flags to hide console window\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m     process = \u001b[38;5;28;01mawait\u001b[39;00m anyio.open_process(\n\u001b[32m     73\u001b[39m         [command, *args],\n\u001b[32m     74\u001b[39m         env=env,\n\u001b[32m     75\u001b[39m         \u001b[38;5;66;03m# Ensure we don't create console windows for each process\u001b[39;00m\n\u001b[32m     76\u001b[39m         creationflags=subprocess.CREATE_NO_WINDOW  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m     77\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(subprocess, \u001b[33m\"\u001b[39m\u001b[33mCREATE_NO_WINDOW\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     78\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0\u001b[39m,\n\u001b[32m     79\u001b[39m         stderr=errlog,\n\u001b[32m     80\u001b[39m         cwd=cwd,\n\u001b[32m     81\u001b[39m     )\n\u001b[32m     82\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m process\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Dan Guilliams\\OneDrive\\Code Projects\\MCP_Agents_RADSecurity\\.venv\\Lib\\site-packages\\anyio\\_core\\_subprocesses.py:190\u001b[39m, in \u001b[36mopen_process\u001b[39m\u001b[34m(command, stdin, stdout, stderr, cwd, env, startupinfo, creationflags, start_new_session, pass_fds, user, group, extra_groups, umask)\u001b[39m\n\u001b[32m    188\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mumask\u001b[39m\u001b[33m\"\u001b[39m] = umask\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m get_async_backend().open_process(\n\u001b[32m    191\u001b[39m     command,\n\u001b[32m    192\u001b[39m     stdin=stdin,\n\u001b[32m    193\u001b[39m     stdout=stdout,\n\u001b[32m    194\u001b[39m     stderr=stderr,\n\u001b[32m    195\u001b[39m     cwd=cwd,\n\u001b[32m    196\u001b[39m     env=env,\n\u001b[32m    197\u001b[39m     startupinfo=startupinfo,\n\u001b[32m    198\u001b[39m     creationflags=creationflags,\n\u001b[32m    199\u001b[39m     start_new_session=start_new_session,\n\u001b[32m    200\u001b[39m     pass_fds=pass_fds,\n\u001b[32m    201\u001b[39m     **kwargs,\n\u001b[32m    202\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Dan Guilliams\\OneDrive\\Code Projects\\MCP_Agents_RADSecurity\\.venv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py:2561\u001b[39m, in \u001b[36mAsyncIOBackend.open_process\u001b[39m\u001b[34m(cls, command, stdin, stdout, stderr, **kwargs)\u001b[39m\n\u001b[32m   2560\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2561\u001b[39m     process = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.create_subprocess_exec(\n\u001b[32m   2562\u001b[39m         *command,\n\u001b[32m   2563\u001b[39m         stdin=stdin,\n\u001b[32m   2564\u001b[39m         stdout=stdout,\n\u001b[32m   2565\u001b[39m         stderr=stderr,\n\u001b[32m   2566\u001b[39m         **kwargs,\n\u001b[32m   2567\u001b[39m     )\n\u001b[32m   2569\u001b[39m stdin_stream = StreamWriterWrapper(process.stdin) \u001b[38;5;28;01mif\u001b[39;00m process.stdin \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Python312\\Lib\\asyncio\\subprocess.py:224\u001b[39m, in \u001b[36mcreate_subprocess_exec\u001b[39m\u001b[34m(program, stdin, stdout, stderr, limit, *args, **kwds)\u001b[39m\n\u001b[32m    222\u001b[39m protocol_factory = \u001b[38;5;28;01mlambda\u001b[39;00m: SubprocessStreamProtocol(limit=limit,\n\u001b[32m    223\u001b[39m                                                     loop=loop)\n\u001b[32m--> \u001b[39m\u001b[32m224\u001b[39m transport, protocol = \u001b[38;5;28;01mawait\u001b[39;00m loop.subprocess_exec(\n\u001b[32m    225\u001b[39m     protocol_factory,\n\u001b[32m    226\u001b[39m     program, *args,\n\u001b[32m    227\u001b[39m     stdin=stdin, stdout=stdout,\n\u001b[32m    228\u001b[39m     stderr=stderr, **kwds)\n\u001b[32m    229\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Process(transport, protocol, loop)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Python312\\Lib\\asyncio\\base_events.py:1743\u001b[39m, in \u001b[36mBaseEventLoop.subprocess_exec\u001b[39m\u001b[34m(self, protocol_factory, program, stdin, stdout, stderr, universal_newlines, shell, bufsize, encoding, errors, text, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m     \u001b[38;5;28mself\u001b[39m._log_subprocess(debug_log, stdin, stdout, stderr)\n\u001b[32m-> \u001b[39m\u001b[32m1743\u001b[39m transport = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_subprocess_transport(\n\u001b[32m   1744\u001b[39m     protocol, popen_args, \u001b[38;5;28;01mFalse\u001b[39;00m, stdin, stdout, stderr,\n\u001b[32m   1745\u001b[39m     bufsize, **kwargs)\n\u001b[32m   1746\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._debug \u001b[38;5;129;01mand\u001b[39;00m debug_log \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Python312\\Lib\\asyncio\\base_events.py:524\u001b[39m, in \u001b[36mBaseEventLoop._make_subprocess_transport\u001b[39m\u001b[34m(self, protocol, args, shell, stdin, stdout, stderr, bufsize, extra, **kwargs)\u001b[39m\n\u001b[32m    523\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Create subprocess transport.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m524\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n",
      "\u001b[31mNotImplementedError\u001b[39m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mNotImplementedError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 41\u001b[39m\n\u001b[32m     37\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTotal tokens: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse[\u001b[33m'\u001b[39m\u001b[33musage_metadata\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mtotal_tokens\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     39\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m analysis\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m analysis = \u001b[38;5;28;01mawait\u001b[39;00m analyze_incidents()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36manalyze_incidents\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      6\u001b[39m request = AnalysisRequest(\n\u001b[32m      7\u001b[39m     start_index=\u001b[32m0\u001b[39m,\n\u001b[32m      8\u001b[39m     batch_size=\u001b[32m2\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     11\u001b[39m     model_name=\u001b[33m\"\u001b[39m\u001b[33mgpt-4o-mini\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     12\u001b[39m )\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Run the analysis\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m analysis, response = \u001b[38;5;28;01mawait\u001b[39;00m run_agent(\n\u001b[32m     16\u001b[39m     query=query,\n\u001b[32m     17\u001b[39m     start_index=request.start_index,\n\u001b[32m     18\u001b[39m     batch_size=request.batch_size\n\u001b[32m     19\u001b[39m )\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# Display the results\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mAnalysis Results:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 19\u001b[39m, in \u001b[36mrun_agent\u001b[39m\u001b[34m(query, start_index, batch_size)\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_agent\u001b[39m(query, start_index, batch_size):\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m stdio_client(server_parameters) \u001b[38;5;28;01mas\u001b[39;00m (read, write):\n\u001b[32m     20\u001b[39m         \u001b[38;5;66;03m# Initialize client session\u001b[39;00m\n\u001b[32m     21\u001b[39m         \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m ClientSession(read, write) \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[32m     22\u001b[39m             \u001b[38;5;28;01mawait\u001b[39;00m session.initialize()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Python312\\Lib\\contextlib.py:210\u001b[39m, in \u001b[36m_AsyncGeneratorContextManager.__aenter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    208\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args, \u001b[38;5;28mself\u001b[39m.kwds, \u001b[38;5;28mself\u001b[39m.func\n\u001b[32m    209\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m210\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m anext(\u001b[38;5;28mself\u001b[39m.gen)\n\u001b[32m    211\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopAsyncIteration\u001b[39;00m:\n\u001b[32m    212\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mgenerator didn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt yield\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Dan Guilliams\\OneDrive\\Code Projects\\MCP_Agents_RADSecurity\\.venv\\Lib\\site-packages\\mcp\\client\\stdio\\__init__.py:114\u001b[39m, in \u001b[36mstdio_client\u001b[39m\u001b[34m(server, errlog)\u001b[39m\n\u001b[32m    111\u001b[39m command = _get_executable_command(server.command)\n\u001b[32m    113\u001b[39m \u001b[38;5;66;03m# Open process with stderr piped for capture\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m process = \u001b[38;5;28;01mawait\u001b[39;00m _create_platform_compatible_process(\n\u001b[32m    115\u001b[39m     command=command,\n\u001b[32m    116\u001b[39m     args=server.args,\n\u001b[32m    117\u001b[39m     env=(\n\u001b[32m    118\u001b[39m         {**get_default_environment(), **server.env}\n\u001b[32m    119\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m server.env \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    120\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m get_default_environment()\n\u001b[32m    121\u001b[39m     ),\n\u001b[32m    122\u001b[39m     errlog=errlog,\n\u001b[32m    123\u001b[39m     cwd=server.cwd,\n\u001b[32m    124\u001b[39m )\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstdout_reader\u001b[39m():\n\u001b[32m    127\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m process.stdout, \u001b[33m\"\u001b[39m\u001b[33mOpened process is missing stdout\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Dan Guilliams\\OneDrive\\Code Projects\\MCP_Agents_RADSecurity\\.venv\\Lib\\site-packages\\mcp\\client\\stdio\\__init__.py:214\u001b[39m, in \u001b[36m_create_platform_compatible_process\u001b[39m\u001b[34m(command, args, env, errlog, cwd)\u001b[39m\n\u001b[32m    209\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    210\u001b[39m \u001b[33;03mCreates a subprocess in a platform-compatible way.\u001b[39;00m\n\u001b[32m    211\u001b[39m \u001b[33;03mReturns a process handle.\u001b[39;00m\n\u001b[32m    212\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    213\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sys.platform == \u001b[33m\"\u001b[39m\u001b[33mwin32\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m214\u001b[39m     process = \u001b[38;5;28;01mawait\u001b[39;00m create_windows_process(command, args, env, errlog, cwd)\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    216\u001b[39m     process = \u001b[38;5;28;01mawait\u001b[39;00m anyio.open_process(\n\u001b[32m    217\u001b[39m         [command, *args], env=env, stderr=errlog, cwd=cwd\n\u001b[32m    218\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Dan Guilliams\\OneDrive\\Code Projects\\MCP_Agents_RADSecurity\\.venv\\Lib\\site-packages\\mcp\\client\\stdio\\win32.py:85\u001b[39m, in \u001b[36mcreate_windows_process\u001b[39m\u001b[34m(command, args, env, errlog, cwd)\u001b[39m\n\u001b[32m     82\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m process\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m     84\u001b[39m     \u001b[38;5;66;03m# Don't raise, let's try to create the process without creation flags\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m85\u001b[39m     process = \u001b[38;5;28;01mawait\u001b[39;00m anyio.open_process(\n\u001b[32m     86\u001b[39m         [command, *args], env=env, stderr=errlog, cwd=cwd\n\u001b[32m     87\u001b[39m     )\n\u001b[32m     88\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m process\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Dan Guilliams\\OneDrive\\Code Projects\\MCP_Agents_RADSecurity\\.venv\\Lib\\site-packages\\anyio\\_core\\_subprocesses.py:190\u001b[39m, in \u001b[36mopen_process\u001b[39m\u001b[34m(command, stdin, stdout, stderr, cwd, env, startupinfo, creationflags, start_new_session, pass_fds, user, group, extra_groups, umask)\u001b[39m\n\u001b[32m    187\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m umask >= \u001b[32m0\u001b[39m:\n\u001b[32m    188\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mumask\u001b[39m\u001b[33m\"\u001b[39m] = umask\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m get_async_backend().open_process(\n\u001b[32m    191\u001b[39m     command,\n\u001b[32m    192\u001b[39m     stdin=stdin,\n\u001b[32m    193\u001b[39m     stdout=stdout,\n\u001b[32m    194\u001b[39m     stderr=stderr,\n\u001b[32m    195\u001b[39m     cwd=cwd,\n\u001b[32m    196\u001b[39m     env=env,\n\u001b[32m    197\u001b[39m     startupinfo=startupinfo,\n\u001b[32m    198\u001b[39m     creationflags=creationflags,\n\u001b[32m    199\u001b[39m     start_new_session=start_new_session,\n\u001b[32m    200\u001b[39m     pass_fds=pass_fds,\n\u001b[32m    201\u001b[39m     **kwargs,\n\u001b[32m    202\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Dan Guilliams\\OneDrive\\Code Projects\\MCP_Agents_RADSecurity\\.venv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py:2561\u001b[39m, in \u001b[36mAsyncIOBackend.open_process\u001b[39m\u001b[34m(cls, command, stdin, stdout, stderr, **kwargs)\u001b[39m\n\u001b[32m   2553\u001b[39m     process = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.create_subprocess_shell(\n\u001b[32m   2554\u001b[39m         command,\n\u001b[32m   2555\u001b[39m         stdin=stdin,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2558\u001b[39m         **kwargs,\n\u001b[32m   2559\u001b[39m     )\n\u001b[32m   2560\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2561\u001b[39m     process = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.create_subprocess_exec(\n\u001b[32m   2562\u001b[39m         *command,\n\u001b[32m   2563\u001b[39m         stdin=stdin,\n\u001b[32m   2564\u001b[39m         stdout=stdout,\n\u001b[32m   2565\u001b[39m         stderr=stderr,\n\u001b[32m   2566\u001b[39m         **kwargs,\n\u001b[32m   2567\u001b[39m     )\n\u001b[32m   2569\u001b[39m stdin_stream = StreamWriterWrapper(process.stdin) \u001b[38;5;28;01mif\u001b[39;00m process.stdin \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2570\u001b[39m stdout_stream = StreamReaderWrapper(process.stdout) \u001b[38;5;28;01mif\u001b[39;00m process.stdout \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Python312\\Lib\\asyncio\\subprocess.py:224\u001b[39m, in \u001b[36mcreate_subprocess_exec\u001b[39m\u001b[34m(program, stdin, stdout, stderr, limit, *args, **kwds)\u001b[39m\n\u001b[32m    221\u001b[39m loop = events.get_running_loop()\n\u001b[32m    222\u001b[39m protocol_factory = \u001b[38;5;28;01mlambda\u001b[39;00m: SubprocessStreamProtocol(limit=limit,\n\u001b[32m    223\u001b[39m                                                     loop=loop)\n\u001b[32m--> \u001b[39m\u001b[32m224\u001b[39m transport, protocol = \u001b[38;5;28;01mawait\u001b[39;00m loop.subprocess_exec(\n\u001b[32m    225\u001b[39m     protocol_factory,\n\u001b[32m    226\u001b[39m     program, *args,\n\u001b[32m    227\u001b[39m     stdin=stdin, stdout=stdout,\n\u001b[32m    228\u001b[39m     stderr=stderr, **kwds)\n\u001b[32m    229\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Process(transport, protocol, loop)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Python312\\Lib\\asyncio\\base_events.py:1743\u001b[39m, in \u001b[36mBaseEventLoop.subprocess_exec\u001b[39m\u001b[34m(self, protocol_factory, program, stdin, stdout, stderr, universal_newlines, shell, bufsize, encoding, errors, text, *args, **kwargs)\u001b[39m\n\u001b[32m   1741\u001b[39m     debug_log = \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mexecute program \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprogram\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m   1742\u001b[39m     \u001b[38;5;28mself\u001b[39m._log_subprocess(debug_log, stdin, stdout, stderr)\n\u001b[32m-> \u001b[39m\u001b[32m1743\u001b[39m transport = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_subprocess_transport(\n\u001b[32m   1744\u001b[39m     protocol, popen_args, \u001b[38;5;28;01mFalse\u001b[39;00m, stdin, stdout, stderr,\n\u001b[32m   1745\u001b[39m     bufsize, **kwargs)\n\u001b[32m   1746\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._debug \u001b[38;5;129;01mand\u001b[39;00m debug_log \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1747\u001b[39m     logger.info(\u001b[33m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m'\u001b[39m, debug_log, transport)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Python312\\Lib\\asyncio\\base_events.py:524\u001b[39m, in \u001b[36mBaseEventLoop._make_subprocess_transport\u001b[39m\u001b[34m(self, protocol, args, shell, stdin, stdout, stderr, bufsize, extra, **kwargs)\u001b[39m\n\u001b[32m    520\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_make_subprocess_transport\u001b[39m(\u001b[38;5;28mself\u001b[39m, protocol, args, shell,\n\u001b[32m    521\u001b[39m                                      stdin, stdout, stderr, bufsize,\n\u001b[32m    522\u001b[39m                                      extra=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m    523\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Create subprocess transport.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m524\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n",
      "\u001b[31mNotImplementedError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from utils.prompt_utils import AnalysisRequest\n",
    "\n",
    "# Create a request to analyze a batch of incidents\n",
    "async def analyze_incidents():\n",
    "    request = AnalysisRequest(\n",
    "        start_index=0,\n",
    "        batch_size=2,\n",
    "        request_id=\"demo-123\",\n",
    "        openai_api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "        model_name=\"gpt-4o-mini\"\n",
    "    )\n",
    "    \n",
    "    # Run the analysis\n",
    "    analysis, response = await run_agent(\n",
    "        query=query,\n",
    "        start_index=request.start_index,\n",
    "        batch_size=request.batch_size\n",
    "    )\n",
    "    \n",
    "    # Display the results\n",
    "    print(\"Analysis Results:\")\n",
    "    for incident in analysis.incidents:\n",
    "        print(f\"\\nIncident: {incident.incident_id}\")\n",
    "        print(f\"Summary: {incident.incident_summary}\")\n",
    "        print(f\"Risk Level: {incident.incident_risk_level}\")\n",
    "        print(f\"Explanation: {incident.incident_risk_level_explanation}\")\n",
    "        print(\"\\nRelevant CVEs:\")\n",
    "        for cve in incident.cve_ids:\n",
    "            print(f\"  - {cve.cve_id} (Relevance: {cve.cve_relevance}, Risk: {cve.cve_risk_level})\")\n",
    "            print(f\"    {cve.cve_summary}\")\n",
    "    \n",
    "    # Display usage metrics\n",
    "    print(\"\\nUsage Metrics:\")\n",
    "    print(f\"Input tokens: {response['usage_metadata']['input_tokens']}\")\n",
    "    print(f\"Output tokens: {response['usage_metadata']['output_tokens']}\")\n",
    "    print(f\"Total tokens: {response['usage_metadata']['total_tokens']}\")\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "# Run the analysis (This will fail in Jupyter due to its own event loop, but you ccan use this in python directly if you want to explore)\n",
    "analysis = asyncio.run(analyze_incidents()) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58feeda3",
   "metadata": {},
   "source": [
    "# 9. Persistence and Data Management\n",
    "\n",
    "This section covers how the system persists analysis results and manages data for continuous learning and reference.\n",
    "\n",
    "## 9.1 SQLite Database\n",
    "\n",
    "Our system uses SQLite for structured persistence of incident analyses. This provides a lightweight, file-based database that requires no external server.\n",
    "\n",
    "**Why we do this:** Persistent storage enables:\n",
    "- Historical reference of past analyses\n",
    "- Audit trails for security review\n",
    "- Query capabilities for reporting and dashboards\n",
    "- Cross-referencing between incidents\n",
    "- Continuous learning for the system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f2ea7dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database initialization from utils/datastore_utils.py\n",
    "import sqlite3\n",
    "from sqlite3 import Error\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "DATA_DIR = Path(\"data\")\n",
    "DB_PATH = DATA_DIR / \"incident_analysis.db\"\n",
    "\n",
    "def init_db():\n",
    "    \"\"\"Initialize the SQLite database with necessary tables.\"\"\"\n",
    "    conn = None\n",
    "    try:\n",
    "        conn = sqlite3.connect(DB_PATH)\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Create incidents table\n",
    "        cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS incidents (\n",
    "            incident_id TEXT PRIMARY KEY,\n",
    "            title TEXT,\n",
    "            description TEXT,\n",
    "            initial_findings TEXT,\n",
    "            created_at TEXT,\n",
    "            updated_at TEXT\n",
    "        )\n",
    "        ''')\n",
    "        \n",
    "        # Create analyses table\n",
    "        cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS analyses (\n",
    "            analysis_id TEXT PRIMARY KEY,\n",
    "            incident_id TEXT,\n",
    "            analysis_json TEXT,\n",
    "            model_name TEXT,\n",
    "            created_at TEXT,\n",
    "            FOREIGN KEY (incident_id) REFERENCES incidents (incident_id)\n",
    "        )\n",
    "        ''')\n",
    "        \n",
    "        # Create run_metadata table for tracking API usage\n",
    "        cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS run_metadata (\n",
    "            run_id TEXT PRIMARY KEY,\n",
    "            request_id TEXT,\n",
    "            model_name TEXT,\n",
    "            input_tokens INTEGER,\n",
    "            output_tokens INTEGER,\n",
    "            total_tokens INTEGER,\n",
    "            start_time TEXT,\n",
    "            end_time TEXT,\n",
    "            duration_seconds REAL\n",
    "        )\n",
    "        ''')\n",
    "        \n",
    "        conn.commit()\n",
    "        print(\"Database initialized successfully\")\n",
    "    except Error as e:\n",
    "        print(f\"Error initializing database: {e}\")\n",
    "    finally:\n",
    "        if conn:\n",
    "            conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8875a2d9",
   "metadata": {},
   "source": [
    "\n",
    "## 9.2 Saving Analysis Results\n",
    "\n",
    "When the agent completes an analysis, we save the results in both SQLite and as JSON backups:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "43f7e06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_incident_and_analysis_to_sqlite_db(incident, analysis, model_name):\n",
    "    \"\"\"\n",
    "    Save an incident and its analysis to the SQLite database.\n",
    "    \n",
    "    Args:\n",
    "        incident: The incident dictionary\n",
    "        analysis: The analysis dictionary (from Pydantic model)\n",
    "        model_name: The name of the LLM used for analysis\n",
    "    \"\"\"\n",
    "    conn = None\n",
    "    try:\n",
    "        conn = sqlite3.connect(DB_PATH)\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Save incident info\n",
    "        now = datetime.now().isoformat()\n",
    "        cursor.execute('''\n",
    "        INSERT OR REPLACE INTO incidents \n",
    "        (incident_id, title, description, initial_findings, created_at, updated_at)\n",
    "        VALUES (?, ?, ?, ?, ?, ?)\n",
    "        ''', (\n",
    "            incident.get('incident_id'),\n",
    "            incident.get('title', ''),\n",
    "            incident.get('description', ''),\n",
    "            incident.get('initial_findings', ''),\n",
    "            now,\n",
    "            now\n",
    "        ))\n",
    "        \n",
    "        # Save analysis\n",
    "        analysis_id = f\"{incident.get('incident_id')}_{now.replace(':', '-')}\"\n",
    "        cursor.execute('''\n",
    "        INSERT INTO analyses\n",
    "        (analysis_id, incident_id, analysis_json, model_name, created_at)\n",
    "        VALUES (?, ?, ?, ?, ?)\n",
    "        ''', (\n",
    "            analysis_id,\n",
    "            incident.get('incident_id'),\n",
    "            json.dumps(analysis),\n",
    "            model_name,\n",
    "            now\n",
    "        ))\n",
    "        \n",
    "        conn.commit()\n",
    "        return True\n",
    "    except Error as e:\n",
    "        print(f\"Error saving to database: {e}\")\n",
    "        return False\n",
    "    finally:\n",
    "        if conn:\n",
    "            conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1456b0",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "In addition to SQLite, we also save JSON backups:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "09dd0402",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_incident_analysis_backup_json(incident_id, analysis_data):\n",
    "    \"\"\"Save a backup of analysis data as JSON.\"\"\"\n",
    "    backup_dir = DATA_DIR / \"backups\"\n",
    "    backup_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    backup_path = backup_dir / f\"analysis_{incident_id}_{timestamp}.json\"\n",
    "    \n",
    "    with open(backup_path, 'w') as f:\n",
    "        json.dump(analysis_data, f, indent=2)\n",
    "    \n",
    "    return backup_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5f4d57",
   "metadata": {},
   "source": [
    "## 9.3 FAISS Vector Index Updates\n",
    "\n",
    "To support continuous learning, we update the FAISS vector index with new analyses:\n",
    "\n",
    "**Why we do this:** Updating vector indexes enables:\n",
    "- The system to learn from new analyses\n",
    "- Improved results over time as more examples are added\n",
    "- Reference to previous analyses when encountering similar incidents\n",
    "- Consistency in risk evaluation by referring to precedents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dec36aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from utils.flatteners import flatten_incident\n",
    "from utils.retrieval_utils import get_incident\n",
    "\n",
    "def add_incident_to_faiss_history_index(incident_id, analysis):\n",
    "    \"\"\"\n",
    "    Add a completed incident analysis to the historical FAISS index.\n",
    "    \n",
    "    Args:\n",
    "        incident_id: The ID of the analyzed incident\n",
    "        analysis: The analysis object from the agent\n",
    "    \"\"\"\n",
    "    global INCIDENT_HISTORY_FAISS, embeddings\n",
    "    \n",
    "    if INCIDENT_HISTORY_FAISS is None or embeddings is None:\n",
    "        initialize_openai_embeddings()\n",
    "        initialize_faiss_indexes()\n",
    "    \n",
    "    # Create a document from the incident\n",
    "    flattened_text = flatten_incident(get_incident(incident_id))\n",
    "    doc = Document(\n",
    "        page_content=flattened_text,\n",
    "        metadata={\n",
    "            \"incident_id\": incident_id,\n",
    "            \"analysis_id\": analysis.get(\"analysis_id\", \"unknown\"),\n",
    "            \"created_at\": datetime.now().isoformat()\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Add to FAISS index\n",
    "    INCIDENT_HISTORY_FAISS.add_documents([doc])\n",
    "    \n",
    "    # Save updated index\n",
    "    index_path = DATA_DIR / \"vectorstore\" / \"incident_analysis_history\"\n",
    "    INCIDENT_HISTORY_FAISS.save_local(index_path)\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16fdec0",
   "metadata": {},
   "source": [
    "## 9.4 Usage Metadata Tracking\n",
    "\n",
    "We track usage metadata to monitor performance and costs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74472138",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_run_metadata(\n",
    "    request_id, \n",
    "    model_name, \n",
    "    input_tokens, \n",
    "    output_tokens, \n",
    "    start_time,\n",
    "    end_time\n",
    "):\n",
    "    \"\"\"Save metadata about an API run to track usage.\"\"\"\n",
    "    conn = None\n",
    "    try:\n",
    "        conn = sqlite3.connect(DB_PATH)\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        run_id = f\"{request_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "        duration = (end_time - start_time).total_seconds()\n",
    "        \n",
    "        cursor.execute('''\n",
    "        INSERT INTO run_metadata\n",
    "        (run_id, request_id, model_name, input_tokens, output_tokens, \n",
    "         total_tokens, start_time, end_time, duration_seconds)\n",
    "        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "        ''', (\n",
    "            run_id,\n",
    "            request_id,\n",
    "            model_name,\n",
    "            input_tokens,\n",
    "            output_tokens,\n",
    "            input_tokens + output_tokens,\n",
    "            start_time.isoformat(),\n",
    "            end_time.isoformat(),\n",
    "            duration\n",
    "        ))\n",
    "        \n",
    "        conn.commit()\n",
    "        return True\n",
    "    except Error as e:\n",
    "        print(f\"Error saving run metadata: {e}\")\n",
    "        return False\n",
    "    finally:\n",
    "        if conn:\n",
    "            conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6126ab7d",
   "metadata": {},
   "source": [
    "## 9.5 Caching Strategy\n",
    "\n",
    "To optimize performance and reduce API costs, we implement a multi-level caching strategy:\n",
    "\n",
    "**Why we do this:** Effective caching:\n",
    "- Reduces redundant computation\n",
    "- Minimizes API calls to OpenAI\n",
    "- Improves response times\n",
    "- Ensures consistent responses for identical queries\n",
    "- Optimizes resource usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22f6e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redis-based caching decorator from utils/decorators.py\n",
    "import functools\n",
    "import json\n",
    "import hashlib\n",
    "import time\n",
    "import redis\n",
    "import os\n",
    "import inspect\n",
    "from typing import Callable, Any\n",
    "\n",
    "REDIS_URL = os.getenv(\"REDIS_URL\", \"redis://localhost\")\n",
    "redis_client = redis.from_url(REDIS_URL, encoding=\"utf-8\", decode_responses=True)\n",
    "\n",
    "def cache_result(ttl_seconds=3600):\n",
    "    \"\"\"\n",
    "    Cache function results in Redis.\n",
    "    \n",
    "    Args:\n",
    "        ttl_seconds: Time-to-live for cached results in seconds\n",
    "    \"\"\"\n",
    "    def decorator(func):\n",
    "        @functools.wraps(func)\n",
    "        async def wrapper(*args, **kwargs):\n",
    "            # Create a unique key from function name and arguments\n",
    "            key_parts = [func.__name__]\n",
    "            \n",
    "            # Add positional args\n",
    "            for arg in args:\n",
    "                if isinstance(arg, (str, int, float, bool)):\n",
    "                    key_parts.append(str(arg))\n",
    "                else:\n",
    "                    try:\n",
    "                        key_parts.append(json.dumps(arg))\n",
    "                    except:\n",
    "                        # If we can't serialize, use object id as fallback\n",
    "                        key_parts.append(str(id(arg)))\n",
    "            \n",
    "            # Add keyword args (sorted for consistency)\n",
    "            for k in sorted(kwargs.keys()):\n",
    "                v = kwargs[k]\n",
    "                key_parts.append(k)\n",
    "                if isinstance(v, (str, int, float, bool)):\n",
    "                    key_parts.append(str(v))\n",
    "                else:\n",
    "                    try:\n",
    "                        key_parts.append(json.dumps(v))\n",
    "                    except:\n",
    "                        key_parts.append(str(id(v)))\n",
    "            \n",
    "            # Create a hash of the key parts\n",
    "            cache_key = hashlib.md5(\"_\".join(key_parts).encode()).hexdigest()\n",
    "            \n",
    "            # Check if result is in cache\n",
    "            cached = redis_client.get(cache_key)\n",
    "            if cached:\n",
    "                try:\n",
    "                    return json.loads(cached)\n",
    "                except:\n",
    "                    # If we can't deserialize, ignore cache\n",
    "                    pass\n",
    "            \n",
    "            # Call the original function\n",
    "            result = await func(*args, **kwargs) if inspect.iscoroutinefunction(func) else func(*args, **kwargs)\n",
    "            \n",
    "            # Store result in cache\n",
    "            try:\n",
    "                redis_client.setex(cache_key, ttl_seconds, json.dumps(result))\n",
    "            except:\n",
    "                # If we can't serialize, just return the result\n",
    "                pass\n",
    "            \n",
    "            return result\n",
    "        return wrapper\n",
    "    return decorator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5904d0",
   "metadata": {},
   "source": [
    "## 9.6 Data Retention and Privacy\n",
    "\n",
    "Our system implements data retention policies and privacy controls:\n",
    "\n",
    "**Why we do this:** Proper data management ensures:\n",
    "- Compliance with regulations (GDPR, CCPA, etc.)\n",
    "- Protection of sensitive information\n",
    "- Minimization of storage requirements\n",
    "- Risk reduction for data breaches\n",
    "\n",
    "Key privacy and retention strategies:\n",
    "- Incident data is stored with role-based access controls\n",
    "- PII is anonymized in vector embeddings\n",
    "- Analysis results are encrypted in the database\n",
    "- Automated purging of data based on configurable retention periods\n",
    "- Audit logs for all access to sensitive data\n",
    "\n",
    "## 9.7 Backup and Recovery\n",
    "\n",
    "To ensure data durability, we implement backup and recovery procedures:\n",
    "\n",
    "- Daily database backups\n",
    "- Vector index snapshots\n",
    "- Redundant storage for JSON backups\n",
    "- Point-in-time recovery capability\n",
    "- Automated recovery testing\n",
    "\n",
    "By implementing comprehensive persistence strategies, our system ensures that valuable analysis results are preserved while maintaining performance, privacy, and compliance. \n",
    "# 10. Conclusion\n",
    "\n",
    "## 10.1 Summary\n",
    "\n",
    "In this notebook, we've built a comprehensive system for analyzing security incidents in the context of CVE data. Our approach leverages:\n",
    "\n",
    "1. **Semantic Search**: We use FAISS vector stores to find relevant CVEs and historical incidents\n",
    "2. **LLM Reasoning**: We use GPT-4o-mini to understand incident context and assess risk\n",
    "3. **Agent Tools**: We implement specialized tools via MCP for CVE search, incident analysis, and more\n",
    "4. **Structured Output**: We enforce consistent output format via Pydantic schemas\n",
    "5. **Persistence**: We store analyses in SQLite and update vector stores for continuous learning\n",
    "\n",
    "The system represents a practical application of AI to a complex security workflow, demonstrating how LLMs can augment human analysts by:\n",
    "- Reducing the cognitive load of analyzing thousands of potential CVEs\n",
    "- Providing consistent risk assessments based on detailed context\n",
    "- Generating clear explanations that link vulnerabilities to incidents\n",
    "- Learning from historical analyses to normalize risk levels\n",
    "\n",
    "## 10.2 Future Work\n",
    "\n",
    "While the current system is functional, several enhancements could further improve its capabilities:\n",
    "\n",
    "1. **Expanded Data Sources**: Integrate threat intelligence feeds, MITRE ATT&CK framework, and vendor security bulletins\n",
    "2. **Automated Remediation Suggestions**: Generate specific remediation steps for identified vulnerabilities\n",
    "3. **Multi-LLM Ensemble**: Use specialized models for different analysis stages to optimize cost/performance\n",
    "4. **Interactive Investigation**: Add agentic workflow for interactive incident investigation\n",
    "5. **Temporal Analysis**: Incorporate time-series analysis of incidents to identify trends and campaigns\n",
    "6. **Active Learning**: Implement feedback loops to improve risk scoring based on analyst input\n",
    "\n",
    "## 10.3 Addressing Key Questions\n",
    "\n",
    "Let's address the specific questions from the exercise:\n",
    "\n",
    "### 1. Agent Architecture and Workflow\n",
    "\n",
    "Our agent uses a ReAct pattern to orchestrate the analysis process. The LLM:\n",
    "- First understands the incident details provided in the prompt\n",
    "- Uses semantic search tools to find relevant CVEs\n",
    "- Assesses the risk level of each CVE in the context of the incident\n",
    "- Generates a structured analysis with explanations\n",
    "\n",
    "### 2. Prompting Strategy\n",
    "\n",
    "Our prompting strategy has three key components:\n",
    "- Pre-loaded context (incidents, FAISS matches, historical context)\n",
    "- Clear task instructions with specific steps\n",
    "- Structured output format via Pydantic schema\n",
    "\n",
    "### 3. Tool Interaction\n",
    "\n",
    "The agent interacts with tools through the MCP protocol. It decides to use tools when:\n",
    "- It needs to search for additional CVEs related to specific components\n",
    "- It needs to verify details about a particular CVE\n",
    "- It needs to access information about historical incidents\n",
    "\n",
    "### 4. Context Window Management\n",
    "\n",
    "We manage the context window by:\n",
    "- Batching incidents to process a few at a time\n",
    "- Pre-filtering FAISS results to the most relevant matches\n",
    "- Using a compact, flattened text representation of incidents and CVEs\n",
    "- Truncating descriptions and details to essential information\n",
    "\n",
    "### 5. Output and Explainability\n",
    "\n",
    "Our agent produces:\n",
    "- A structured JSON output with incident risk levels and related CVEs\n",
    "- Detailed explanations for risk assessments\n",
    "- Evidence linking CVEs to specific aspects of the incident\n",
    "- Normalized risk scores based on historical context\n",
    "\n",
    "### 6. Evaluation Metrics\n",
    "\n",
    "We evaluate our system using:\n",
    "- Semantic relevance of identified CVEs\n",
    "- Risk assessment accuracy compared to experts\n",
    "- Explanation quality and actionability\n",
    "- Tool usage efficiency\n",
    "- Processing time and token usage\n",
    "\n",
    "### 7. Production Challenges\n",
    "\n",
    "Key challenges for production deployment include:\n",
    "- Balancing model cost and performance\n",
    "- Ensuring prompt engineering robustness\n",
    "- Maintaining tool reliability\n",
    "- Addressing safety and bias concerns\n",
    "- Implementing comprehensive monitoring\n",
    "\n",
    "## 10.4 Final Thoughts\n",
    "\n",
    "This CVE analysis agent demonstrates the practical application of generative AI to cybersecurity operations. By combining semantic search, LLM reasoning, and specialized tools, we've created a system that can significantly enhance the efficiency and consistency of security incident analysis.\n",
    "\n",
    "The architecture is designed to be modular, extensible, and adaptable to changing security landscapes. It represents a balance between automation and human oversight, providing valuable analysis while ensuring security professionals remain in control of critical decisions.\n",
    "\n",
    "As threat landscapes continue to evolve, AI-assisted analysis will become increasingly valuable for security teams. This system provides a foundation that can be expanded and enhanced to address emerging security challenges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce24787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thank you for reviewing this notebook!\n",
    "print(\"Analysis complete!\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
